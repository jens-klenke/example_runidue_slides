---
title: "Induktive Statistik"
author: "Prof. Dr. Christoph Hanck"
date: "Sommersemester 2025"
output: 
  runidue::lectureslides:
    lang: "de"
    keep_tex: true
editor_options: 
  chunk_output_type: console
classoption: aspectratio=169
---

```{r, include=F}
knitr::opts_chunk$set(cache = T)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```


# Einleitung

## Motivation

- Ziel der \hil{deskriptiven} (beschreibenden) \hil{Statistik}: das Datenmaterial mit Hilfe tabellarischer und grafischer Repräsentationen sowie geeigneter Kenngrößen übersichtlich aufbereiten.
- Ziel der \hil{induktiven} (schließenden) \hil{Statistik}: Verifikation theoretischer Modelle anhand von Daten und Testen von Hypothesen über unbekannte Parameter auf Basis von Wahrscheinlichkeitsmodellen.




## Motivation
\framesubtitle{Beispiel: Lernerfolg}
 
```{r, echo = F}
#library(devtools)
#install_github("martinschmelzer/runidue")
library(runidue)
library(scales)
load("Resources/Data/JACKvsEXAM.RData")
model <- lm(Exam ~ JACK, data = JACKvsEXAM)

plot(x = jitter(JACKvsEXAM$JACK, factor = 0.33), y = JACKvsEXAM$Exam, col = alpha("steelblue", 0.5),
     pch = 19, xlab = "JACK Bonuspunkte", ylab = "Punkte in der Klausur")
abline(coef(model), col = "red2", lwd = 2)
```




## Motivation

Übergang von der deskriptiven zur induktiven Statistik:

- \hil{Induktives Schließen}, statistische Inferenz, Repräsentationsschluss: Schluss von einer Teilgesamtheit auf die Grundgesamtheit
- \hil{Induktive Statistik}: Methoden, die es erlauben von den Beobachtungen einer
Teilgesamtheit \linebreak (= *Stichprobe*) auf bestimmte Charakteristika
der dazugehörenden *Grundgesamtheit* zu
schließen.

## Motivation

- Beim Glücksspiel meinen Sie, dass die \glqq{}Gegenseite\grqq{} mit manipulierten Würfeln spielt, d.h. die Zahl 6 fällt häufiger als andere Zahlen. Wie können Sie Ihre Vermutung überprüfen?
- Vor der Bundestagswahl prognostiziert ein Wahlforschungsinstitut, dass 30% der Stimmen auf die CDU/CSU entfallen. Wie kann eine solche Prognose verlässlich durchgeführt werden?

Ziel: Schlussfolgerungen von einer Stichprobe auf die Grundgesamtheit


## Motivation
\framesubtitle{QuizAcademy}

\QuizAcademy{Schokoladenwürfel}\endQuizAcademy



## Motivation

\begin{center}
\includegraphics[height=0.6\textheight, keepaspectratio]{Resources/Grafiken/Schaetzen.jpg}
\end{center}
\source{\href{https://blog.ritter-sport.de/2014/08/05/schatzt-euch-bunt/}{https://blog.ritter-sport.de/2014/08/05/schatzt-euch-bunt/}}



## Motivation

Man beachte: Wahrscheinlichkeitsrechnung

- ist __mehr__ als Grundlage der schließenden Statistik
- hat enorme eigenständige ökonomische Bedeutung z.B. in
  - Mikroökonomik
  - Investition und Finanzierung
  - Portfoliotheorie




## Gliederung

1. Einleitung
2. Grundlagen
3. Eindimensionale Zufallsvariablen
4. Ausgewählte theoretische Verteilungen
5. Grundzüge der Stichprobentheorie
6. Statistische Schätzverfahren
7. Statistische Testverfahren
8. Zweidimensionale Zufallsvariablen



# Grundlagen


## Grundlegende Begriffe
\framesubtitle{Zufallsvorgang}


Zufallsvorgänge (bzw. stochastische Vorgänge) sind durch zwei wesentliche Eigenschaften gekennzeichnet:

1. Sie besitzen verschiedene, sich gegenseitig ausschließende Ausgänge, die bereits vor Beginn des Vorgangs bekannt sind;
2. es ist nicht vorhersehbar, welcher Ausgang tatsächlich eintreten wird.

\hil{Beispiele} für Zufallsvorgänge:

- Der Ausgang eines Fußballspiels, der Kurs einer Aktie am nächsten Tag oder die realisierte Augenzahl eines Würfelwurfes.


## Grundlegende Begriffe
\framesubtitle{Zufallsvorgang, Zufallsexperiment}
 
 
- Ist ein Zufallsvorgang unverändert beliebig oft wiederholbar, liegt ein \hil{Zufallsexperiment} vor. Die unveränderte Wiederholbarkeit beschreibt man auch als unter gleichen (Rand-)Bedingungen wiederholbar.
- D.h., dass die Randbedingungen wie bei naturwissenschaftlichen Experimenten kontrolliert werden können. Dies stellt sicher, dass die Bedingungen, unter denen das Experiment stattgefunden hat, auch bei weiteren Durchführungen hätten eingehalten werden können.
- Damit gehören alle Zufallsvorgänge, die fiktiv unter gleichen Bedingungen wiederholbar sind, zu den Zufallsexperimenten. Dies erlaubt es, auch Zufallsvorgänge als Zufallsexperimente aufzufassen, deren praktische Wiederholung unter gleichen Bedingungen schwierig wäre.
 


## Grundlegende Begriffe
\framesubtitle{Zufallsvorgang, Zufallsexperiment, Stichprobenraum}

- Alle Ausgänge $\omega_i$ eines Zufallsvorganges bzw. -experimentes fasst man zu einem \hil{Stichprobenraum} (Ergebnis- bzw. Ereignisraum) $\Omega$ zusammen. Der Stichprobenraum ist eine Menge, deren Elemente die Ausgänge sind.
- $\Omega$ kann endlich oder unendlich viele Ausgänge enthalten. Lassen sich die unendlich vielen Ausgänge mit den natürlichen Zahlen $\mathbb{N}$ abzählen, bezeichnet man $\Omega$ als \hil{abzählbar unendlich}. Gelingt dies nicht, heißt $\Omega$ \hil{überabzählbar unendlich}.

\[\Omega=\{\omega_1,\ldots,\omega_m,\ldots\}.\]


## Grundlegende Begriffe
\framesubtitle{Zufallsvorgang, Zufallsexperiment, Stichprobenraum}
\xmpl[Würfelwurf]
Der Zufallsvorgang *Werfen eines Würfels* hat sechs mögliche Ausgänge.

$\Omega$ ist endlich und lässt sich schreiben als:
   \[ \Omega=\{\omega_1,\omega_2,\ldots,\omega_6\}=\{\omega_i,i=1,\ldots,6\} =\{1,2,3,4,5,6\}. \]
\endxmpl



## Grundlegende Begriffe
\framesubtitle{Zufallsvorgang, Zufallsexperiment, Stichprobenraum}
\xmpl[Münzwurf]
Wirft man eine Münze mit Seiten *Zahl* $(Z)$ und *Kopf* $(K)$ bis zum ersten Mal $Z$ erscheint, lauten die möglichen Ausgänge:
\begin{alignat*}{2}
\omega_1&=Z&\qquad &\text{(zum ersten Mal \textit{Zahl} im ersten Wurf),}\\
\omega_2&=KZ&\qquad &\text{(zum ersten Mal \textit{Zahl} im zweiten Wurf),}\\
\vdots&&\qquad&\\
\omega_m&=\underset{m-1}{\underbrace{K\ldots K}}Z&\qquad &\text{(zum ersten Mal \textit{Zahl} im $m$--ten Wurf),}\\
\vdots&&\qquad&
\end{alignat*}
Der Stichprobenraum ist hier abzählbar unendlich:
\vspace{-0.75em}
\[\Omega=\{\omega_1,\ldots\omega_m,\ldots\}\,.\]
\endxmpl





## Grundlegende Begriffe
\framesubtitle{Zufallsvorgang, Zufallsexperiment, Stichprobenraum}
\xmpl[Zugverspätung]
Die Verspätung eines Zuges in Minuten sei ein Zufallsvorgang mit Ausgängen im Intervall $[0; 10]$. Bei unendlicher Messgenauigkeit sind überabzählbar viele Verspätungen möglich, da das Intervall $[0; 10]$ Teilmenge der reellen Zahlen $\mathbb{R}$ ist.

Die reellen Zahlen sind, wie auch jede ihrer Teilmengen, mächtiger als $\mathbb{N}$ und daher überabzählbar unendlich.
\endxmpl



## Grundlegende Begriffe
\framesubtitle{Ereignisse}

- Jede Teilmenge von $\Omega$ heißt \hil{(Zufalls--)Ereignis}.
- Da eine Menge auch Teilmenge von sich selbst und die leere Menge $\emptyset$ Teilmenge jeder Menge ist, sind $\Omega$ und $\emptyset$ selbst Ereignisse von $\Omega$.
- Ein Ereignis $A \subset \Omega$ tritt ein, wenn der Ausgang $\omega_i$ des Zufallsvorgangs Element von $A$ ist: $\omega_i\in A$.
- Da ein Zufallsvorgang immer in einem Ausgang $\omega_i\in\Omega$ mündet, ist $\Omega$ auch das \hil{sichere Ereignis}.
- Analog hierzu heißt die leere Menge $\emptyset$ das \hil{unmögliche Ereignis}, weil kein $\omega_i\in\Omega$ existiert, das Element der leeren Menge $\emptyset$ ist: $\omega_i\notin \emptyset, i=1,\ldots,m$.




## Grundlegende Begriffe
\framesubtitle{Ereignisse}

- Teilmengen $\{\omega_i\}$, deren einziges Element ein Ausgang $\omega_i\in\Omega$ ist, heißen \hil{Elementarereignisse}.
- Umfassen Teilmengen mehrere Ausgänge, nennt man sie \hil{zusammengesetzte Ereignisse}.
- Z.\nbs B. ist beim *\blue{Wurf eines Würfels}* der Ausgang: *\blue{Augenzahl 3 liegt oben}* ein Elementarereignis und wird geschrieben als $\{3\}$; das Ereignis $A$: *\blue{gerade Augenzahl liegt oben}* ist ein zusammengesetztes Ereignis, als Menge geschrieben: $\{2,4,6\}$.
- $A$ tritt ein, wenn der Würfelwurf die Augenzahl 2, 4 oder 6 ergibt.




## Grundlegende Begriffe
\framesubtitle{Ereignisse}

- Die insgesamt möglichen Ereignisse eines Zufallsvorgangs findet man, indem alle Teilmengen für $\Omega$ gebildet werden.
- Die Zusammenfassung dieser Teilmengen führt bei endlichem oder abzählbar unendlichem Stichprobenraum $\Omega$ zur \hil{Potenzmenge} $PM(\Omega)$.
- Die Anzahl der Ereignisse der Potenzmenge ist $2^m$, wobei m der Anzahl der Elemente von $\Omega$ entspricht.
- Siehe Buch, S.\nbs 11.




## Grundlegende Begriffe
\framesubtitle{Ereignisse}
\xmpl[Potenzmenge]\label{xmpl:potenz}
Ein Zufallsvorgang hat den Stichprobenraum $\Omega=\{1,2,3\}$; wegen $m=3$ beträgt die Anzahl der möglichen Ereignisse \[2^3=8.\] Diese Ereignisse lauten $A_1=\emptyset$, $A_2=\{1\}$,
$A_3=\{2\}$, $A_4=\{3\}$, $A_5=\{1,2\}$, $A_6=\{1,3\}$,
$A_7=\{2,3\}$, $A_8=\{1,2,3\}=\Omega$. Die Potenzmenge ist \[PM(\Omega)=\{A_1,\ldots,A_8\}.\]
\endxmpl


## Grundlegende Begriffe
\framesubtitle{Vereinigungsereignis}

- Zwischen den Ereignissen können bestimmte Beziehungen vorliegen. 
- \blue{Beispiel:} Das Ereignis $A_5 =\{1,2\}$ tritt dann ein, wenn entweder $A_2=\{1\}$ oder $A_3=\{2\}$ eintritt. $A_5$ heißt daher \hil{Vereinigungsereignis}, geschrieben als $A_5 = A_2\cup A_3$.
- Verallgemeinert erhält man Vereinigungsereignisse $V$ als $V=\bigcup^n_{j=1}A_j$.
- Für $n=2$ ist $V$ als rote Fläche im \hil{Venn--Diagramm} wiedergegeben, wobei das Rechteck den Stichprobenraum $\Omega$ festlegt.

\vspace{-0.5cm}

```{r Vereinigung, echo = F, fig.width = 0.6, fig.asp = 0.5}
library(venn)
#venn("11 + 10 + 01", snames = c("$A_1$", "$A_2$"), ellipse = F, zcolor = rep("tomato3", 3), opacity = 1, cexsn = 0.001)
#text(c("$A_1$", "$A_2$"), x = c(200, 850), y = 800, cex = 1.5)

venn("11 + 10 + 01", snames = c(" ", " "), ellipse = F, zcolor = rep("tomato3", 3), opacity = 1, cexsn = 0.001)
text(c("$A_1$", "$A_2$"), x = c(200, 850), y = 800, cex = 1.5)

```


## Grundlegende Begriffe
\framesubtitle{Durchschnittsereignis}

- Ist der Schnitt zweier Ereignisse $A_i$ und $A_j$ nicht leer, gilt $A_i\cap A_j=D\neq\emptyset$, so treten mit $D$ auch die Ereignisse $A_i$ und $A_j$ ein.
- $D$ heißt daher \hil{Durchschnittsereignis}, das allgemein definiert ist als $D=\bigcap_{j=1}^n A_j$. $D$ tritt ein, wenn alle $A_j$ eintreten.

\vspace{-0.5cm}

```{r Durchschnitt, echo = F, fig.width = 0.5}
#library(venn)
#venn("A B", 
#     snames = c("$A_1$", "$A_2$"), zcolor = "tomato3", # Labels und Farbe
#     opacity = 1, sncs = 1.3)                          # Farbstärke und Textgröße

library(venn)
venn("11", 
     snames = c(" ", " "), zcolor = "tomato3", # Labels und Farbe
     opacity = 1, sncs = 1.3)                          # Farbstärke und Textgröße
text(c("$A_1$", "$A_2$"), x = c(200, 850), y = 800, cex = 1.5)
```



## Grundlegende Begriffe
\framesubtitle{Komplementärereignis}

- Tritt $A_i$ genau dann ein, wenn $A_j$ nicht eintritt, so sind die beiden Ereignisse zueinander komplementär.
- $A_i$ heißt \hil{Komplementärereignis} oder kurz Komplement und lässt sich schreiben als $A_i=\bar{A}_j$. Natürlich ist auch $A_j$ Komplementärereignis zu $A_i$: $A_j = \bar{A}_i$.
- Im Beispiel \ref{xmpl:potenz} ist das Ereignis $A_2=\{1\}$ das Komplement zu $A_7 =\{2,3\}:A_2=\bar{A}_7.$ Umgekehrt gilt auch $A_7=\bar{A}_2$.
- Für $A$ und sein Komplement $\bar{A}$ gelten $A \cup \bar{A} = \Omega$ und $A \cap \bar{A} = \emptyset$.

<!-- \vspace{-0.5cm} -->

```{r Komplementaerereignis, echo = F, fig.width = 0.35}
library(venn)
venn("~A", snames = c("A"), ellipse = F, zcolor = "tomato3", cexil = 0.001, cexsn = 0.001, opacity=1, sncs=0.0001)
text("A", x = 500, y = 500, cex = 1.5)
text("$\\bar{A}$", x = 800, y = 800, cex = 1.5)
```



## Grundlegende Begriffe
\framesubtitle{Teilereignisse und disjunkte Ereignisse}

- $A_i$ und $A_j$ heißen \hil{disjunkt}, wenn ihr Schnitt leer ist: $A_i\cap A_j =\emptyset$.
- Komplementäre Ereignisse sind daher immer auch disjunkt, die Umkehrung gilt aber nicht.
- So sind im Beispiel \ref{xmpl:potenz} $A_2=\{1\}$ und $A_3 = \{2\}$ zwar disjunkt, aber nicht komplementär. Denn wenn $A_3$ nicht eintritt, folgt nicht notwendigerweise das Eintreten von $A_2$; es könnte auch $A_4=\{3\}$ eintreten.

```{r Disjunkt, echo = F, fig.width = 0.5}
library(grid)
plot(c(0, 3), c(0, 2), xaxt = "n", yaxt = "n", type = "n", xlab = "", ylab = "")
grid.circle(x = .4,  y = .5,  r = .2)
grid.circle(x = .42, y = .45, r = .1)
grid.circle(x = .75, y = .6,  r = .12)
text(x = .8,  y = 1.25, labels = "$A_6$", cex = 1.3)
text(x = 1,   y = .6,  labels = "$A_2$", cex = 1.3)
text(x = 2.3, y = 1.2, labels = "$A_3$", cex = 1.3)
```



## Grundlegende Begriffe
\framesubtitle{Teilereignisse und disjunkte Ereignisse}

- $A_i$ ist ein \hil{Teilereignis} von $A_j$, wenn jeder Ausgang eines Zufallsvorgangs, der zu $A_i$ gehört, auch in $A_j$ liegt, $A_j$ aber mindestens einen Ausgang $\omega_j$ enthält, der nicht auch in $A_i$ enthalten ist.
- $A_i$ ist eine \hil{echte Teilmenge} von $A_j:$ $A_i\subset A_j$.
- In der Abbildung repräsentieren die Kreise die Ereignisse $A_2$, $A_3$ und $A_6$ des Beispiels \ref{xmpl:potenz}.




## Grundlegende Begriffe
\framesubtitle{Differenzereignis}

- Das \hil{Differenzereignis} $A_i\setminus A_j$ tritt dann ein, wenn der Ausgang des Zufallsvorgangs in $A_i$, aber nicht in $A_j$ liegt.
- Man nennt $A_i\setminus A_j$ auch das \hil{relative Komplement} zu $A_j$ bezüglich $A_i$. Alternativ kann man schreiben $A_i\setminus A_j=A_i\cap\bar{A}_j$. Im Beispiel \ref{xmpl:potenz} folgt für $A_6 = \{1,3\}$ und $A_5 = \{1,2\}$ das Differenzereignis $A_6\setminus A_5$ als
    \[A_6\setminus A_5 = A_6\cap\bar{A}_5 = \{1,3\}\cap\{3\}=\{3\}.\]
    Nur $\omega_3=3$ führt dazu, dass $A_6\setminus A_5$ eintritt. Das Beispiel zeigt, dass im Allgemeinen $A_i\setminus A_j \neq A_j\setminus A_i$.

\vspace{-0.75em}
```{r Differenzmenge, echo = F, fig.width = 0.3}
library(venn)
venn("10", ellipse = F, zcolor = "tomato3", cexsn = 0.001, opacity=1, sncs=0.0001)
text(x = c(210, 850),y = c(750, 750), labels = c("$A_1\\quad$", "$\\;A_2$"))
```




## Grundlegende Begriffe
\framesubtitle{Ereignisse - QuizAcademy}

\QuizAcademy{Würfelwurf 1}\endQuizAcademy


## Grundlegende Begriffe
\framesubtitle{Mengen in R}

```{r Mengen, fontsize='\\scriptsize'}
x <- c(1,3,5)
y <- c(2,3,6)

union(x,y)

intersect(x,y)

setdiff(x,y)

setdiff(y,x)

setdiff(union(x,y),y) # Komplement von y, wenn Omega = union(x,y)
```

## Grundlegende Begriffe
\framesubtitle{Mengen in R}

\tiny

```{r Mengen2, results='hold', fontsize='\\scriptsize'}
rje::powerSet(x)
```

\normalsize


## Grundlegende Begriffe
\framesubtitle{Ereignisse}

\exe[]
Ein Würfel wird einmal gewürfelt. Es sind folgende Ereignisse definiert:
\begin{gather*}
A_1=\{1,2\},\; A_2=\{3,4\},\; A_3=\{2,4\},\; A_4=\{1,2,3,4\},\\ A_5=\{4\}, A_6=\{3,4,5,6\}, A_7=\{5,6\},\; A_8=\{1,2,4,5\},\\ A_9=\{2,3,6\},\; A_{10}=\{1,4,5\}.
\end{gather*}

Bestimmen Sie folgende Ereignisse und stellen Sie diese jeweils in einem Venn-Diagramm grafisch dar!

\begin{center}
\begin{tabular}{lll}
\blue{(a)} $A_1\cup A_2$ & \blue{(b)} $A_2\cap A_3$ & \blue{(c)} $\bar{A}_1$\\
\blue{(d)} $A_1\cap A_2$ & \blue{(e)} $A_8\cap\bar{A}_9=A_8\setminus A_9$ & \blue{(f)} $A_5\subset A_4$
\end{tabular}
\end{center}

\endexe





## Grundlegende Begriffe
\framesubtitle{Ereignisse}

- Jedes zusammengesetzte Ereignis $A$ kann in disjunkte Teilereignisse $A_j\neq\emptyset$ so zerlegt werden, dass gilt: $A=\bigcup^n_{j=1} A_j$. $A$ ergibt sich jetzt als disjunkte Vereinigung.
- In Beispiel \ref{xmpl:potenz} lässt sich $A_8$ in $A_2=\{1\}$ und $A_7=\{2,3\}$, aber auch in $A_2=\{1\}$, $A_3=\{2\}$ und $A_4=\{3\}$ zerlegen. Beim zweiten Fall wurde $A_8$ in Elementarereignisse zerlegt.
- Die Zerlegung von $A$ in Elementarereignisse heißt \hil{kanonische Darstellung}: Jedes Ereignis ergibt sich eindeutig als Vereinigungsereignis von Elementarereignissen: 

\[A=\bigcup^n_{j=1}\{\omega_j\}.\]



## Grundlegende Begriffe
\framesubtitle{Vollständiges Ereignissystem}
$\quad$

- Da auch $\Omega$ zu den zusammengesetzten Ereignissen gehört, kann es auch in (Teil-)Ereignisse $A_j$ zerlegt werden.
- Die Menge $\{A_1,\ldots,A_n\}$ bildet ein \hil{vollständiges
System von Ereignissen}, wenn für diese Zerlegung gilt:

\begin{align*}
(1) \qquad & \Omega=A_1\cup\ldots\cup A_n=\bigcup^n_{j=1}A_j,\\
(2) \qquad & A_i\cap A_j=\emptyset\quad\text{für  $i\neq j$},\\
(3) \qquad & A_j\neq\emptyset\quad\text{für $j=1,\ldots,n$},
\end{align*}

- Ein vollständiges System wird auch \hil{vollständiges Ereignissystem} bzw. \hil{vollständige Zerlegung} genannt.




## Grundlegende Begriffe
\framesubtitle{Vollständiges Ereignissystem}

```{r, echo = F, fig.width = 0.8}
par(mar = c(3.5, 3.5, 2, 2.3), bty = "o")
plot(c(0, 3), c(0, 2), xaxt = "n", yaxt = "n", type = "n", xlab = "", ylab = "", xaxs = "i", yaxs = "i")
segments(x0 = 0,   x1 = 1,   y0 = 0.3,  y1 = 2)
segments(x0 = .6,  x1 = 1.2, y0 = 1.32, y1 = 0)
segments(x0 = 1.5, x1 = 2,   y0 = 0,    y1 = 2)
segments(x0 = 1.8, x1 = 3,   y0 = 1.2,  y1 = 1.5)
segments(x0 = 2.2, x1 = 2.8, y0 = 1.3,  y1 = 0)

text(x = c(0.3, 0.6, 1.3, 2.4, 2.1, 2.7), y = c(1.6, .5, 1.1, 1.6, .5, 1), labels = paste0("$A_",1:6,"$"))
mtext("$\\Omega$", las = 1, side = 4, line = .5, cex = 1.2, at = 1.75)
```



## Grundlegende Begriffe
\framesubtitle{Vollständiges Ereignissystem}
\vspace{1cm}

\exe
Ein Würfel wird einmal gewürfelt. Es sind folgende Ereignisse
definiert:
\begin{gather*}
A_1=\{2,4,6\},\; A_2=\{1,3,5\},\; A_3=\{2\},\\
A_4=\{4,6\},\;A_5=\{1,3\}, A_6=\{5,6\}.
\end{gather*}
Welche der folgenden Mengen bildet ein vollständiges Ereignissystem?

(a) $F_1=\{A_1, A_2\}$
(b) $F_2=\{A_1, A_5, A_6\}$
(c) $F_3=\{A_3, A_4, A_5\}$
(d) $F_4=\{A_2, A_3, A_4\}$

\endexe



## Der Wahrscheinlichkeitsbegriff

- Zufallsvorgänge zeichnen sich dadurch aus, dass ungewiss ist, welches ihrer möglichen Ereignisse eintritt.
- Wahrscheinlichkeiten quantifizieren die Chance des Eintretens eines bestimmten Ereignisses und werden mit $P$ symbolisiert.
- Für ein Ereignis $A\subset\Omega$ gibt $P(A)$ jetzt die \hil{Wahrscheinlichkeit} für das Eintreten von $A$ an.
- $P$ ist also eine Funktion, die Ereignissen reelle Zahlen zuordnet, welche wir als Wahrscheinlichkeit bezeichnen.



## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Wahrscheinlichkeitfunktion und Kolmogoroff-Axiome}

- Die axiomatische Grundlage für Wahrscheinlichkeiten wurde von \hil{Kolmogoroff} entwickelt.
- Auf der Basis dieser Axiomatik lässt sich die Wahrscheinlichkeitsfunktion wie folgt definieren:


\block{Kolmogoroff-Axiome}
\begin{itemize}\tightlist
\leftskip=12pt
\item[(K1)] $P(A)\geq 0$ für alle $A$,
\item[(K2)] $P(\Omega)=1$,
\item[(K3)] \[P\left(\bigcup^\infty_{j=1} A_j\right) = P(A_1)+P(A_2)  +\ldots=\sum^\infty_{j=1} P(A_j)\] für alle $A_i$ und $A_j$, die paarweise disjunkt sind: $A_i\cap A_j=\emptyset,$ $i\neq j$.
\end{itemize}
    
\endblock



## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Wahrscheinlichkeitfunktion und Kolmogoroff-Axiome}

\block{Kolmogoroff-Axiome}
\begin{itemize}\tightlist
\leftskip=8pt
\item[(K1)] \hil{Nichtnegativität}: Die Wahrscheinlichkeit $P$ eines Ereignisses $A$ ist stets nichtnegativ.
\item[(K2)] \hil{Normierung} der Wahrscheinlichkeit.
\item[(K3)] \hil{Volladdidivität}: Die Wahrscheinlichkeit für die Vereinigung paarweise disjunkter Ereignisse gleicht der Summe der Einzelwahrscheinlichkeiten.
\end{itemize}
\endblock

Jedes Ereignis $A\subset\Omega$ wird somit durch $P$ in das geschlossene Intervall $[0,1]\subset\mathbb{R}$ abgebildet.
 



## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Rechenregeln}

Aus diesen Axiomen lassen sich folgende Rechenregeln ableiten:

\block{Rechenregeln}
 \begin{itemize}\tightlist\leftskip=12pt
\item[(R1)] $P(A)+P(\bar{A})=1,$
\item[(R2)] $P(A)\leq P(B)$ für $A\subset B$,
\item[(R3)] $P(A_1\cup A_2\cup\ldots\cup A_n)=\sum^n_{j=1}\limits P(A_j)$ (für paarweise disjunkte Ereignisse),
\item[(R4)] $P(A\cup B)=P(A)+P(B)-P(A\cap B)$ (für paarweise nicht disjunkte Ereignisse).
 \end{itemize}
\endblock
Ihre Herleitung findet sich im Buch auf S.\ 23.




## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Rechenregeln - Additionssatz}



\[P(A\cup B)=P(A)+P(B)-\textcolor[RGB]{205,79,57}{P(A\cap B)}\]
 
```{r, echo = F, fig.width = 0.4}
venn("11", zcolor = "tomato3", opacity=1, cexsn = 0.001, sncs=0.0001)
text(c("$A$", "$B$"), x=c(190, 850), y=770, cex=1.2)
mtext("$\\Omega$", las = 1, side = 4, line = -3, cex = 1.1, at = 750)
```
 
- $A$ und $B$ sind nicht disjunkt, da $A\cap B \not=\emptyset$. $A\cap B$ entspricht der roten Fläche.
- $(A\cup B)$ würde mit $P(A)+P(B)$ die Wahrscheinlichkeit für $A\cap B$ doppelt erfassen; folglich muss $P(A\cap B)$ subtrahiert werden.


## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Rechenregeln}

- Der \hil{Additionssatz für disjunkte Ereignisse}
\[P(A_1\cup A_2\cup\ldots\cup A_n)=\sum^n_{j=1}\limits P(A_j)\]
liefert eine Vorschrift für die Wahrscheinlichkeit eines Ereignisses $A=\{\omega_1,\omega_2,\ldots,\omega_n\}$ eines abzählbar unendlichen
   Stichprobenraumes $\Omega$.
- \hil{Elementarereignisse} $\{\omega_j\},$ $j=1,\ldots,n,$ sind paarweise disjunkt.





## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Rechenregeln}
\exe[]
Es sind folgende Wahrscheinlichkeiten gegeben: 
\[P(A)=0.4, P(B)=0.7, P(A\cap B)=0.25.\]
Bestimmen Sie die Wahrscheinlichkeiten
\[P(A\cup B),\;P(\bar{B}),\;P(A\cap \bar{B}),\;P(A\cup \bar{B})\]
\endexe
Wir verwenden einen Punkt anstelle eines Kommas als \hil{Dezimaltrennzeichen}, da dies der angloamerikanischen Schreibweise der Programmiersprache \R entspricht.

## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Rechenregeln}
\exe
Ein Würfel wird ein Mal gewürfelt. Wie groß ist die Wahrscheinlichkeit,

a) eine 2 oder eine 3 zu würfeln?
b) eine 2, eine 3 oder eine 4 zu würfeln?
c) eine ungerade Zahl oder eine gerade Zahl zu würfeln?
d) eine Augenzahl kleiner als 4 oder eine gerade Augenzahl zu würfeln?
e) eine Augenzahl kleiner als 4 und eine gerade Augenzahl zu würfeln?

\endexe



## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Rechenregeln}
\exe
In einer Urne befinden sich 20 rote und 30 grüne Kugeln. 5 rote und 10 grüne Kugeln sind mit einer 1 beschriftet.
Wie groß ist die Wahrscheinlichkeit, dass

a) eine gezogene Kugel rot oder mit einer 1 beschriftet ist?
b) eine gezogene Kugel nicht mit einer 1 beschriftet ist?
c) eine rote Kugel gezogen wird und diese nicht mit einer 1 beschriftet ist?
d) eine Kugel gezogen wird, die rot oder nicht mit einer 1 beschriftet ist?

\endexe


## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Wahrscheinlichkeitsinterpretation}

- Mit den \hil{Axiomen von Kolmogoroff} und den Regeln \blue{(R1)} bis \blue{(R4)} sind nur die allgemeinen Eigenschaften der Wahrscheinlichkeiten festgelegt, nicht jedoch welche Werte sie bei bestimmten Ereignissen annehmen.
- Hierzu muss eine dem Zufallsvorgang adäquate \hil{Wahrscheinlichkeitsinterpretation} vorliegen.


Mit dem subjektiven und statistisch/frequentistischen Wahrscheinlichkeitsbegriff befasst sich das Buch auf S.\ 25.



## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Objektive Wahrscheinlichkeit}

\begin{center}
\begin{tikzpicture}
\node[box] (a) {Objektive\\Wahrscheinlichkeitsinterpretation};
\node[box ,below of=a, xshift=-2cm, yshift=-1cm] (b) {a priori};
\node[box, below of=a, xshift=2cm, yshift=-0.5cm] (c) {statistisch\\frequentistisch};
\node[box, below of=b, xshift=-1.5cm, yshift=-0.25cm] (d) {klassisch\\(Laplace)};
\node[box, below of=b, xshift=1.5cm] (e) {geometrisch};
\draw[arrow] (a) to (b);
\draw[arrow] (a) to (c);
\draw[arrow] (b) to (d);
\draw[arrow] (b) to (e);
\end{tikzpicture}
\end{center}



## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Objektive Wahrscheinlichkeit}

\exe
Student Paul kommt jeden Morgen zwischen 8.00 und 8.30 Uhr an einer Bushaltestelle an, an der zwei Buslinien halten:
\begin{gather*}
\text{Linie A:}\quad 8.04, 8.14, 8.24 \\ 
\text{Linie B:}\quad 8.10, 8.20, 8.30.
\end{gather*}
Wie groß ist die Wahrscheinlichkeit, dass er Linie A nimmt?
\endexe



## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Objektive Wahrscheinlichkeit}

- \hil{a-priori}:
  - \hil{Laplace- (bzw. klassische) Wahrscheinlichkeit}: Anwendung bei Zufallsvorgängen mit endlichem $\Omega$ und deren Elementarereignisse die gleiche Eintrittswahrscheinlichkeit besitzen (\hil{Laplace-Experimente}).
 
  - Laplace-Experimente können als die zufällige Entnahme aus einer endlichen Menge von Objekten charakterisiert werden. Wird mehrfach gezogen, muss zwischen *Ziehen mit Zurücklegen* oder *Ziehen ohne Zurücklegen* unterschieden werden.




## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Laplace-Wahrscheinlichkeit}

\block{Laplace-Wahrscheinlichkeit}
  Die Wahrscheinlichkeit für ein Elementarereignis $\{\omega_i\},$ $i=1,\ldots,m$ beträgt dann $P(\{\omega_i\})=\frac{1}{m}$.\mps

$P(A)$ erhält man als
  \[
  P(A)=\dfrac{\text{Anzahl der für $A$ günstigen Ausgänge}}{\text{Anzahl der möglichen Ausgänge}}.
  \]
  Die Anzahl der Elemente einer Menge $M$ heißt \hil{Mächtigkeit} $|M|$. Die Laplace-Wahrscheinlichkeit ist dann
  \[P(A)=\frac{|A|}{|\Omega|}.\]
\endblock




## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Laplace-Wahrscheinlichkeit - QuizAcademy}

\QuizAcademy{Würfelwurf 2}\endQuizAcademy



## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Laplace-Wahrscheinlichkeit}
\exe[Laplace-Experimente]

(a) In einer Urne befinden sich 20 Kugeln, von denen 8 rot sind. Berechnen Sie die Wahrscheinlichkeit eine rote Kugel bei einer zufälligen Entnahme zu erhalten.
(b) Wie groß ist die Wahrscheinlichkeit, nach Ziehen einer roten Kugel ohne Zurücklegen im zweiten Zug erneut eine rote Kugel zu ziehen?
(c) Ein \hil{Laplace-Würfel} (idealer Würfel) wird geworfen. Berechnen Sie die Wahrscheinlichkeit, dass eine Augenzahl größer als 2 oben liegt.
 
\endexe



## Der Wahrscheinlichkeitsbegriff
\framesubtitle{Laplace-Wahrscheinlichkeit}
\exe[]
Ein Laplace-Würfel wird einmal geworfen. Wie groß ist die Wahrscheinlichkeit, dass eine Augenzahl kleiner 3 fällt?
\endexe
\exe[]
Eine Laplace-Münze und ein Laplace-Würfel werden gemeinsam geworfen. Wie groß ist die Wahrscheinlichkeit, dass Kopf und eine Augenzahl größer als 4 fällt?
\endexe




## Bedingte Wahrscheinlichkeit
\framesubtitle{Änderung des Stichprobenraums}
\vspace{1cm}

- Die Berechnung von Wahrscheinlichkeiten erfolgt bislang unter Bezug auf den ganzen Stichprobenraum $\Omega$.
- Es lassen sich aber auch dann Wahrscheinlichkeiten für $A$ berechnen, wenn nicht ganz $\Omega$, sondern nur noch ein Teil $B$ davon relevant ist. Die Abbildung verdeutlicht die Veränderung des Bezugssystems.
 
\vspace{-.5cm}

```{r BedingteWkeit, echo = F, fig.width=0.6}
library(plotrix)
par(mar = c(2, 2, 2, 3), xpd = F)
plot(c(-1, 2.5), c(-1.25, 1.25), xaxt = 'n', yaxt = 'n', type = 'n', xlab = '', ylab = '', asp = 1)
rect(xleft = 3, xright = -2, ybottom = -2, ytop = sin(pi - .4),col = "steelblue", border = "black")
draw.circle(x = 0, y = 0, radius = 1, col = "white")
abline(h = sin(pi - .4))
x <- cos(seq(pi - .4, 2 * pi + .4, .001))
y <- sin(seq(pi - .4, 2 * pi + .4, .001))
polygon(x = x, y = y, col = "tomato3")
mtext("$\\Omega$", las = 1, side = 4, line = .25, cex = 1.3, at = 1, col = "black")
mtext("$B$", las = 1, side = 4, line = 1, cex = 1.1, at = -0.5, col = "steelblue")
mtext("\\rotatebox{90}{\\makebox[14ex]{\\upbracefill}}", las = 1, side = 4, line = .5, cex = 1, at = -1.25)
text(x = -.85, y = 0.85, labels = "$A$", cex = 1.5)
```




## Bedingte Wahrscheinlichkeit
\framesubtitle{Änderung des Stichprobenraums}

- Der Kreis entspricht $A$, das untere Rechteck $B$ und das rote Kreissegment $A\cap B$.
- $P(A)$, $P(B)$ und $P(A\cap B)$ sind die Wahrscheinlichkeiten für $A$, $B$ und $A\cap B$, wenn $\Omega$ zugrunde liegt.
- Es kann aber auch die Wahrscheinlichkeit für $A$ unter der Bedingung berechnet werden, dass nur noch die Ausgänge in $B$ relevant sind.
- Diese \hil{bedingte Wahrscheinlichkeit} wird mit $P(A|B)$ bezeichnet.
- In der Abbildung ist $P(A)$ das Verhältnis der Kreisfläche zur Fläche des Rechtecks $\Omega$; die bedingte Wahrscheinlichkeit $P(A|B)$ jedoch das Verhältnis der roten Fläche zur Fläche des Rechtecks $B$.




## Bedingte Wahrscheinlichkeit
\framesubtitle{Formel}

\begin{block}{Bedingte Wahrscheinlichkeit}
 Es seien $A$, $B\subset\Omega$ Ereignisse
 und %$(\Omega, A, P)$ der Wahrscheinlichkeitsraum,
 $P(B) > 0$. Für die \hil{bedingte Wahrscheinlichkeit} $P(A|B)$ gilt \[ P(A|B) = \frac{P(A\cap B)}{P(B)}. \]
\end{block}





## Bedingte Wahrscheinlichkeit
\framesubtitle{Beispiele}

\xmpl[Idealer Würfel]
Ein idealer Würfel wird geworfen. Also sind $\Omega =\{1,2,\ldots,6\}$ und $P(\{\omega_i\}) = 1/6$ für $i=1,\ldots,6$.
\medskip

$A$ ist das Ereignis, eine *1* zu würfeln; $B$ das Ereignis einer ungeraden Augenzahl. $A\cap B$ tritt bei einer *1* ein. Die Wahrscheinlichkeiten betragen $P(A) = 1/6,$ $P(B) = 1/2$ und $P(A\cap B) = 1/6$.
\medskip

Was ist nun die Wahrscheinlichkeit einer *1* unter der Bedingung, dass eine ungerade Augenzahl eintritt? Der durch die Bedingung gegebene Stichprobenraum lautet $B = \{1,3,5\}$, also gilt $P(A|B)=1/3.$
\medskip

Denselben Wert erhält man nach der eingeführten Definition:
\[ P(A|B) = \frac{P(A\cap B)}{P(B)} = \frac{\frac{1}{6}}{\frac{1}{2}}=\frac{1}{3}.\]
\endxmpl




## Bedingte Wahrscheinlichkeit
\framesubtitle{Aufgaben}
\exe
Nach der achtmaligen Durchführung des Zufallsexperiments *zweimaliges Werfen einer Münze* erhält man folgendes Resultat:
\begin{center}
\begin{tabular}{l|cccccccc}
Versuch & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\ \hline
1. Wurf & K & Z & Z & Z & K & Z & Z & K\\
2. Wurf & Z & Z & K & K & K & Z & K & Z
\end{tabular}
\end{center}
Ereignis $A$: K beim 1. Wurf,\quad Ereignis $B$: K beim 2. Wurf

Berechnen Sie folgende Wahrscheinlichkeiten:

\begin{center}
\begin{tabular}{lll}
\blue{(a)} $P(A)$   & \blue{(b)} $P(B)$   & \blue{(c)} $P(A\cap B)$\\
\blue{(d)} $P(A|B)$ & \blue{(e)} $P(B|A)$ & \\
\end{tabular}
\end{center}
\endexe



## Bedingte Wahrscheinlichkeit
\framesubtitle{Aufgaben}
\exe
In einem Dorf leben 1000 Personen. Es ist bekannt, dass 600 Einwohner nach Mallorca in den Urlaub fahren und dass 500 Einwohner die Bild--Zeitung lesen.
Zusätzlich weiß man, dass von den 600 Mallorca--Urlaubern 400 die Bild lesen.
Wie groß ist die Wahrscheinlichkeit, dass

(a) eine Person Mallorca--Urlauber ist,
(b) eine Person Bild--Zeitung--Leser ist,
(c) eine Person Mallorca--Urlauber und Bild--Zeitung--Leser ist,
(d) wenn ein Einwohner Bild--Zeitung liest, dieser auch ein Mallorca--Urlauber ist,
(e) wenn ein Einwohner Mallorca--Urlauber ist, dieser auch Bild--Zeitung liest?

\endexe



## Bedingte Wahrscheinlichkeit
\framesubtitle{Paarweise stochastische Unabhängigkeit}

- $P(A|B)$ lässt sich als die Wahrscheinlichkeit von $A$ unter der zusätzlichen Information interpretieren, dass $B$ bereits eingetreten ist.
- Übt diese Information keinen Einfluss auf die Wahrscheinlichkeit von $A$ aus, gilt $P(A|B)=P(A)$.
- $A$ ist dann \hil{unabhängig} von $B$.
- Dann ist aber auch $B$ unabhängig von $A$, wie folgende Umformung zeigt:
\begin{align*}
  P(B|A) & = \frac{P(A\cap B)}{P(A)}=\frac{P(A|B)P(B)}{P(A)} \\
 & = \frac{P(A)P(B)}{P(A)}=P(B)\;\;\; (\text{wegen }P(A|B)=P(A)).
\end{align*}




## Bedingte Wahrscheinlichkeit
\framesubtitle{Paarweise stochastische Unabhängigkeit - QuizAcademy}
\QuizAcademy{Würfelwurf 3}\endQuizAcademy




## Bedingte Wahrscheinlichkeit
\framesubtitle{Paarweise stochastische Unabhängigkeit}

- Diese über die Wahrscheinlichkeiten festgelegte Eigenschaft zweier Ereignisse heißt \hil{paarweise stochastische Unabhängigkeit}.
- Dies liefert eine einfache Prüfung auf stochastische Unabhängigkeit:
\defn[Paarweise stochastische Unabhängigkeit]
Zwei Ereignisse $A$ und $B$ heißen \hil{paarweise stochastisch unabhängig}, wenn eine der drei folgenden Bedingungen
erfüllt ist:
\begin{eqnarray*}
 & & (1) \quad P(A\cap B)=P(A)P(B), \\
 & & (2) \quad P(A|B)=P(A) \quad\text{falls } P(B)>0, \\
 & & (3) \quad P(B|A)=P(B) \quad \text{falls } P(A)>0;
\end{eqnarray*}
andernfalls bezeichnet man sie als \hil{stochastisch abhängig}.
\enddefn




## Bedingte Wahrscheinlichkeit
\framesubtitle{Paarweise stochastische Unabhängigkeit}
\xmpl[Zweimaliges Würfeln]
Ein idealer Würfel wird zweimal geworfen. $A$ sei die *Augenzahl 1 im ersten Wurf*; $B$ sei die *Augenzahl 2 im zweiten Wurf*.

Der Stichprobenraum umfasst 36 geordnete Zahlenpaare, weil zu unterscheiden ist, wann welche Augenzahl eintritt:
\[ \Omega = \{(1,1),\ldots,(1,6),\ldots,(6,1),\ldots,(6,6)\}. \]
\endxmpl



## Bedingte Wahrscheinlichkeit
\vspace{0.8cm}
\framesubtitle{Paarweise stochastische Unabhängigkeit}
\xmpl[*]
Für die Ereignisse erhält man:
\begin{alignat*}{2}
& A=\{(1,1),\ldots,(1,6)\}& , & \;
 P(A)=\frac{|A|}{|\Omega|}=\frac{6}{36}=\frac{1}{6},\\
& B=\{(1,2),\ldots,(6,2)\}& , & \;
P(B)=\frac{|B|}{|\Omega|}=\frac{6}{36}=\frac{1}{6}, \\
& A \cap B=\{(1,2)\}& , & \; P(A\cap B)=\frac{|A\cap B|}{|\Omega|}=\frac{1}{36}.
\end{alignat*}
Wegen \[P(A\cap B)=\frac{1}{36}=P(A)P(B)=\frac{1}{6}\cdot \frac{1}{6}\] folgt paarweise stochastische Unabhängigkeit.
\endxmpl



## Bedingte Wahrscheinlichkeit
\vspace{1cm}
\framesubtitle{Paarweise stochastische Unabhängigkeit}
\exe
Aus einer Urne mit vier Kugeln mit den Ziffern 1,2,3,4 wird eine entnommen. Für welche Ereignisse liegt paarweise stochastische Unabhängigkeit vor?

(a) $A=\{1,2\}$, $B=\{1\}$
(b) $A=\{1,2,3\}$, $B=\{1,2,4\}$
(c) $A=\{1,2\}$, $B=\{1,3\}$

\endexe

- Das Buch auf S.\nbs 34 diskutiert die Verallgemeinerung der stochastischen Unabhängigkeit auf mehr als zwei Ereignisse.

- Sind Ereignisse gemeinsam stochastisch unabhängig, dann sind sie auch paarweise stochastisch unabhängig. Die Umkehrung gilt allerdings nicht!


## Bedingte Wahrscheinlichkeit
\framesubtitle{Multiplikationssätze}

Mit dem Konzept der bedingten Wahrscheinlichkeiten lassen sich nützliche Sätze der Wahrscheinlichkeitsalgebra gewinnen:

\defn[Multiplikationssatz für 2 Ereignisse]
Es seien $A,B\subset\Omega$ Ereignisse mit $P(A)>0$ und $P(B)>0$. Dann
gilt

(a) bei stochastischer Abhängigkeit:
\[ P(A\cap B)=P(A|B)P(B)=P(B|A)P(A);\]
(b) bei stochastischer Unabhängigkeit:
\[ P(A\cap B)=P(A)P(B).\]

\enddefn



## Bedingte Wahrscheinlichkeit
\framesubtitle{Multiplikationssätze}
Die Verallgemeinerung der Ergebnisse des Multiplikationssatzes für zwei Ereignisse führt zum Multiplikationssatz für $n$ Ereignisse:
\defn[Multiplikationssatz für $n$ Ereignisse]
Es seien $A_j\subset\Omega,$ $j=1,\ldots,n$ Ereignisse mit
$A_1\cap A_2\cap\ldots\cap A_{n-1}\neq\emptyset$. Dann gilt

(a) bei stochastischer Abhängigkeit:
\vspace{-0.5cm}

\begin{align*}
P(A_1\cap A_2\cap\ldots\cap A_n) = & P(A_1)P(A_2|A_1)P(A_3|A_1\cap
A_2)\cdot\ldots \\\nonumber
& \cdot P(A_n|A_1\cap A_2\cap\ldots\cap A_{n-1});
\end{align*}

(b) bei gemeinsamer stochastischer Unabhängigkeit:
\begin{equation}\label{gemWSUnabh}
P(A_1\cap A_2\cap\ldots\cap A_n) = P(A_1)P(A_2)\cdot\ldots\cdot P(A_n).
\end{equation}

\enddefn


## Bedingte Wahrscheinlichkeit

\framesubtitle{Multiplikationssätze}
\xmpl[Geburten]
Ein Ehepaar möchte wissen welche der drei folgenden Anordnungen von vier Kindern am wahrscheinlichsten ist: JJJJ, MMMM oder JMJM.\newline
\newline
Wir unterstellen, dass die Wahrscheinlichkeiten für einen *Jungen* (J) und ein *Mädchen* (M) jeweils 0.5 sind. Dann sind alle drei Anordnungsmöglichkeiten gleich wahrscheinlich.\newline
\newline
Der Grund dafür ist, dass die Ereignisse gemeinsam stochastisch unabhängig sind:
\[P(\text{JJJJ})=P(\text{MMMM})=P(\text{JMJM})=0.5\cdot 0.5\cdot 0.5\cdot 0.5=0.5^4=0.0625.\]
Die Wahrscheinlichkeit für 2 J und 2 M ist allerdings wesentlich höher:
\[P(\text{2J und 2M})=\dfrac{6}{16}=0.375.\]
\endxmpl



## Bedingte Wahrscheinlichkeit
\framesubtitle{Multiplikationssätze}
\exe
Eine Urne enthält 5 weiße und 5 rote Kugeln. Wie groß ist die Wahrscheinlichkeit, dass gleich die ersten drei Kugeln, die gezogen werden, rot sind (ZoZ)?
\endexe

\exe
In einer Urne befinden sich 3 schwarze und 2 weiße Kugeln. Wie groß ist die Wahrscheinlichkeit, nach dreimaligem Ziehen ohne Zurücklegen

(a) eine weiße Kugel im zweiten Zug,
(b) im dritten Zug die erste weiße Kugel,
(c) alle weißen Kugeln

zu ziehen?
\endexe



## Bedingte Wahrscheinlichkeit
\framesubtitle{Satz von der totalen Wahrscheinlichkeit}
Jede Wahrscheinlichkeit $P(B)>0$ kann auf bedingte Wahrscheinlichkeiten zurückgeführt werden:
\medskip
\defn[Satz von der totalen Wahrscheinlichkeit]\label{SatztotWS}
Es sei $\{A_1,\ldots,A_n\},$ mit $A_j$ für $j=1,\ldots,n$ ein vollständiges System von Ereignissen. Für das Ereignis $B$ gilt dann
\begin{align}
P(B) & =P(B|A_1)P(A_1)+P(B|A_2)P(A_2)+\ldots+P(B|A_n)P(A_n)\notag\\
 & = \sum^n_{j=1}\limits P(B|A_j)P(A_j).
\end{align}
\enddefn





## Bedingte Wahrscheinlichkeit
\vspace{.5cm}
\framesubtitle{Satz von der totalen Wahrscheinlichkeit}
\setlength{\medskipamount}{=6pt plus 0pt minus 6pt}

- Die folgende Abbildung zeigt eine solche Zerlegung:

\vspace{-.5cm}

```{r Zerlegung, echo = F, fig.width=0.45}
par(mar = c(2,2,2,2), xpd = F)
plot(c(-1, 2.5), c(-1.25, 1.25), xaxt = 'n', yaxt = 'n', type = "n", xlab = "", ylab = "", asp = 1)

draw.ellipse(x=1, y=0, a = 1.1, b = 0.7) #circle(x = 0, y = 0, radius = 1)
abline(v = c(-.6, .6, 1.4))
text(x = c(-.85,0:2), y = rep(1, 4), labels = paste0("$A_",1:4,"$"), cex = 1.2)
segments(x0 = 3.2, x1 = 2.05, y0 = -.35, y1 = -.25, xpd = T)
mtext("$B$", las = 1, side = 4, line = 0.3, at = -0.38)
```

- $P(B)$ ist gleich dem gewogenen arithmetischen Mittel der bedingten Wahrscheinlichkeiten $P(B|A_j)$ mit Gewichten $P(A_j)$:
\[ P(B)=\sum^n_{j=1}P(B|A_j)P(A_j).\]




## Bedingte Wahrscheinlichkeit
\framesubtitle{Satz von der totalen Wahrscheinlichkeit}
\exe
In Urne 1 befinden sich 14 weiße und 6 schwarze Kugeln, in Urne 2 befinden sich 2 weiße und 8 schwarze Kugeln und in Urne 3 befinden sich 3 weiße und 7 schwarze Kugeln.

Um aus einer Urne Kugeln entnehmen zu dürfen, muss man würfeln. Bei einer 1, 2 oder 3 darf man eine Kugel aus Urne 1 ziehen, bei einer 4 oder 5 aus Urne 2 und bei einer 6 aus Urne 3.

Wie groß ist die Wahrscheinlichkeit eine weiße Kugel zu ziehen?
\endexe

\exe
In einer Stadt sind 60% der Einwohner Frauen und 40% Männer. Es ist bekannt, dass 30% der Frauen und 50% der Männer rauchen.

Wie groß ist die Wahrscheinlichkeit, dass ein Raucher männlich ist?
\endexe



## Bedingte Wahrscheinlichkeit
\framesubtitle{Bayes'sches Theorem}

- Mit dem \hil{Satz für die bedingte Wahrscheinlichkeit} und dem \hil{Multiplikationssatz für 2 Ereignisse} lässt sich eine
weitere Regel gewinnen, vorausgesetzt $P(B)>0$:
\begin{align*}
 P(A_i|B)& = \frac{P(A_i\cap B)}{P(B)}\quad \text{(nach Satz für bedingte Wahrscheinlichkeit)} \\
 & = \frac{P(B|A_i)P(A_i)}{P(B)} \quad \text{(nach Multiplikationssatz)}
 \end{align*}
- Ersetzt man im letzten Bruch $P(B)$ durch den Satz der totalen Wahrscheinlichkeit \eqref{SatztotWS}, erhält man das \hil{Bayes'sche Theorem}:

\defn[Satz von Bayes]
 \[ P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum^n_{j=1}P(B|A_j)P(A_j)} \; \text{
 für alle } i=1, \ldots, n. \]
\enddefn


## Bedingte Wahrscheinlichkeit
\framesubtitle{Bayes'sches Theorem}

- Das Theorem von Bayes gibt an, wie die Wahrscheinlichkeit für ein Ereignis $A$ nach Durchführen eines Experimentes oder einer Beobachtung neu berechnet werden kann.
- Interpretiere hierzu das vollständige Ereignissystem $A_j$, $j=1,\ldots,n$ als Menge sich ausschließender Hypothesen oder disjunkter Zustände. Die Wahrscheinlichkeiten $P(A_j)$ sind aufgrund von Ausgangsinformationen bestimmt; sie sind daher \hil{a-priori-Wahrscheinlichkeiten} für jedes $A_j$.
- Mit dem Bayes'schen Theorem erhält man jetzt die \hil{a-posteriori-Wahrscheinlichkeiten} für $A_j$, wenn die Beobachtung $B$ bzw. das experimentelle Ergebnis $B$ vorliegt. Das Eintreten des Ereignisses $B$ ist eine zusätzliche Information.




## Bedingte Wahrscheinlichkeit
\framesubtitle{Bayes'sches Theorem}
\xmpl[Bayes'sches Theorem]
Ein Test erkennt eine Krankheit mit einer Wahrscheinlichkeit von 95%, wenn diese tatsächlich vorliegt. Es gibt aber auch *falsche positive* Befunde mit einer Wahrscheinlichkeit von 1%. 0.1% der Bevölkerung hat die Krankheit. Was ist die Wahrscheinlichkeit, dass eine Person die Krankheit hat, wenn der Test positiv ausfällt?
\begin{eqnarray*}
P(K|+)=\frac{P\left(+|K\right) P(K)}{P(+)}=\frac{0.95\cdot0.001}{0.95\cdot0.001+0.01\cdot 0.999}=0.0868,
\end{eqnarray*}
wobei $\Omega=K\cup \bar{K}$ mit $K\cap \bar{K}=\emptyset$ und daher:
\begin{eqnarray*}
P(+)&=&P(+\cap K)+P(+\cap \bar{K})\\
&=&P\left(+|K\right) P(K)+P\left(+|\bar{K}\right) P(\bar{K}).
\end{eqnarray*}
\endxmpl



## Bedingte Wahrscheinlichkeit
\framesubtitle{Bayes'sches Theorem}
\xmpl[*]
Der Satz von Bayes findet in sehr vielen unterschiedlichen Disziplinen Anwendung, z.B. wie \href{https://www.spiegel.de/politik/deutschland/kommentar-die-ermittlungen-im-fall-edathy-muessen-weitergehen-a-953963.html}{hier} zu sehen auch in der Justiz.
\medskip

Oder auch in Situationen des alltäglichen Lebens wie \href{http://www.businessinsider.com/dating-for-bayesians-heres-how-to-use-statistics-to-improve-your-love-life-2013-11}{hier}.
\medskip

Auch Spamfilter funktionieren nach diesem Prinzip.
\endxmpl



## Bedingte Wahrscheinlichkeit
\framesubtitle{Bayes'sches Theorem}
\exe
Das Angelrevier eines Hobbyanglers besteht aus den drei Seen S1, S2 und S3, wobei er keinen der drei Seen bevorzugt.
\medskip

Die Wahrscheinlichkeit dafür, dass er innerhalb einer Stunde einen Fisch fängt, beträgt für den See S1 60%, für den See S2 50% und für den See S3 80%.
\medskip

Nach einer Stunde Angelzeit kann sich der Hobbyangler über den Fang eines Fisches freuen. Wie groß ist die Wahrscheinlichkeit, dass der Fisch in dem See S2 geangelt wurde?
\endexe



## Bedingte Wahrscheinlichkeit
\framesubtitle{Bayes'sches Theorem}
\exe
Ein Autohersteller bezieht Airbags von 2 Zulieferern. Beim Zulieferer 1, der insgesamt 65% aller Airbags liefert, sind 98% der Airbags
fehlerfrei. 95% der von Zulieferer 2 gelieferten Airbags sind nicht defekt.

(a) Wenn der Hersteller ein fehlerhaftes Teil bekommt, wie groß ist die Wahrscheinlichkeit, dass es von Zulieferer 1 kommt?
(b) Wie groß ist die Wahrscheinlichkeit, dass ein fehlerhaftes Airbag von Zulieferer 2 kommt?
(c) Wie groß ist die Wahrscheinlichkeit, dass ein geliefertes Airbag defekt ist?

\endexe


## Kombinatorik
\framesubtitle{Grundlagen}

- Zur Berechnung von Wahrscheinlichkeiten nach dem klassischen Konzept müssen die \glqq{}günstigen\grqq{} Ausgänge zu den möglichen Ausgängen ins Verhältnis gesetzt werden.
- Oft ist aber gerade die Ermittlung der günstigen Ausgänge nicht einfach.
- Formeln für ihre Berechnung sind daher nützlich. Der Zweig der Mathematik, der sich hiermit beschäftigt, ist die \hil{Kombinatorik}.
- Sie untersucht die verschiedenen Möglichkeiten der Anordnung oder Auswahl von Objekten, im Folgenden \glqq{}Elemente\grqq{}.




<!--## Kombinatorik
\framesubtitle{Grundlagen}
Allgemein können bei der Anordnung/Auswahl von Elementen \hil{vier Kriterien} unterschieden werden:

(1) Sind die Elemente verschieden oder einige gleich,
(2) sollen nur einige Elemente ausgewählt oder alle angeordnet werden,
(3) spielt die Reihenfolge der Elemente eine Rolle oder nicht und
(4) soll eine Wiederholung der Elemente zulässig sein oder nicht?




## Kombinatorik
\framesubtitle{Grundlagen}
Mit diesen Kriterien lassen sich alle Kombinatorikprobleme klassifizieren.

- Werden alle $N$ Elemente einer Gesamtheit angeordnet, liegt eine \hil{Permutation} vor.
- Sind $n$ Elemente auszuwählen, ohne dabei auf die Reihenfolge ihrer Auswahl zu achten, spricht man von \hil{Kombinationen}.
- \hil{Variationen} ergeben sich, wenn die Reihenfolge bedeutsam ist. Variationen sind daher geordnete Kombinationen und werden mitunter auch so bezeichnet.
- Bei jeder der Klassifikationen ist schließlich noch zu unterscheiden, ob eine Wiederholung der Elemente möglich sein soll oder nicht.-->




## Kombinatorik
\framesubtitle{Permutation ohne Wiederholung}

- Die Entwicklung der Kombinatorikformeln lässt sich anschaulich an einer Gesamtheit mit $N$ nummerierten Kugeln $1,2,\ldots,N$ zeigen.
- Jede Zusammenstellung aller $N$ Kugeln (Zahlen) in irgendeiner Anordnung ist eine \hil{Permutation}. Die Anzahl unterscheidbarer Permutationen erhält man durch folgende Überlegung.




## Kombinatorik
\framesubtitle{Permutation ohne Wiederholung}

- Um die erste Stelle der Anordnung zu besetzen, stehen $N$ Kugeln zur Auswahl. Danach bleiben zur Besetzung der zweiten Stelle $N-1$, für die dritte Stelle $N-2$ Kugeln und schließlich für die letzte ($N$-te) Stelle nur noch eine Kugel übrig.
- Die Gesamtzahl unterscheidbarer Permutationen ist das Produkt aus all diesen Besetzungsmöglichkeiten, also
\[ N(N-1)(N-2)\cdot\ldots\cdot 1.\]




## Kombinatorik
\framesubtitle{Permutation ohne Wiederholung}

- Ein solches Produkt heißt \hil{$N$--Fakultät}, geschrieben als $N!$, wobei $N\in\mathbb{N}$.
- Für $N=4$ erhält man $4! = 4\cdot 3 \cdot 2 \cdot 1 = 24$, für  $N=1$: $1! = 1$.
- Damit die Rekursionsformel $N!=N(N-1)!$ uneingeschränkt auf $\mathbb{N}$ gilt, definiere zudem $0!=1$.
- Bezeichnet man die Permutationen mit $P$, gibt der folgende Satz die Anzahl der Anordnungen von $N$ verschiedenen Elementen wieder:
\defn[Permutation ohne Wiederholung]
 \[ P(N)=N!=N(N-1)(N-2)\cdot\ldots\cdot 1.\]
\enddefn




## Kombinatorik
\framesubtitle{Permutation ohne Wiederholung - QuizAcademy}

\QuizAcademy{Kombinatorik}\endQuizAcademy


## Kombinatorik
\framesubtitle{Permutation ohne Wiederholung}

\xmpl[Kugeln]
Es sollen drei Kugeln mit den Ziffern 1, 2 und 3
unterschiedlich angeordnet werden. Die möglichen Permutationen
lauten:
\begin{align*}
\begin{matrix}
1 &  2 & 3 \\
2 &  1 & 3
\end{matrix} \qquad\quad
\begin{matrix}
1 & 3 & 2 \\
2 & 3 & 1
\end{matrix} \qquad\quad
\begin{matrix}
3 & 1 & 2 \\
3 & 2 & 1
\end{matrix}
\end{align*}
Ihre Anzahl erhält man nach der Formel für Permutationen ohne Wiederholung als
\center $3! = 6$.
\endxmpl




<!--## Kombinatorik
\framesubtitle{Permutation mit Wiederholung}

- Wären die Kugeln 1 und 2 weiß $(w)$, die Kugel 3 rot $(r)$ und sollen jetzt die Permutationen hinsichtlich der Farbe gebildet werden, lassen sich die untereinander stehenden Permutationen nicht mehr unterscheiden, weil bei ihnen die weißen Kugeln auf denselben Positionen liegen.
- Es können nur Klassen gebildet werden, die durch die Positionen der weißen Kugeln definiert sind. In der ersten Klasse nehmen die weißen Kugeln die Plätze 1 und 2, in der zweiten die Plätze 1 und 3 und in der dritten die Plätze 2 und 3 ein.
- Unterscheidbar bleiben daher nur noch pro Klasse die Permutationen $wwr$, $wrw$ und $rww$. Diese Art der Klassierung lässt verallgemeinern: Von den $N$ Elementen einer Gesamtheit seien nur $m<N$ verschieden.
- Die Gesamtheit lässt sich dann in $m$ Gruppen zerlegen deren Elemente gleich sind und die $n_1$-mal, $n_2$-mal und schließlich $n_m$-mal vorkommen, mit $\sum^m_{i=1}n_i=N.$






## Kombinatorik
\framesubtitle{Permutation mit Wiederholung}

Die Anzahl unterscheidbarer Permutationen reduziert sich wegen der Merkmalsgleichheit einiger Elemente und wird als \hil{Permutationen mit Wiederholung} $P_W(N)$ bezeichnet. (Für Details siehe Buch S.\nbs 56.)
\defn[Permutation mit Wiederholung]
 \[ P_W(N)=\frac{N!}{\prod^m_{i=1}\limits n_i!}.\]
\enddefn




## Kombinatorik
\framesubtitle{Permutation mit Wiederholung}
\xmpl[Anordnung von Kugeln]
Von 10 durchnummerierten Kugeln sind fünf blau, vier rot und eine weiß. Die Anzahl der Permutationen hinsichtlich der Ziffern erhält man als:
\[ P(10)=10! = 3\, 628\, 800. \]
Sucht man die Anordnungen gemäß der Farbe, sind nur die Ausprägungen blau, rot und weiß relevant, die mit den Häufigkeiten $n_1=5,$ $n_2=4$ und $n_3=1$ vorkommen. Es folgt:
\[ P_W(10) = \frac{10!}{5!4!1!}=\frac{3\, 628\, 800}{2 880}=1260. \]
\endxmpl-->



## Kombinatorik
\framesubtitle{Permutationen ohne Wiederholung in \R}


```{r R-Beispiel1, echo = T}
### Beispiel: Kugeln (1/3)
prod(1:3)
factorial(3) # kürzer
```

<!--
## Kombinatorik
\framesubtitle{Permutationen mit und ohne Wiederholung in \R}
```{r R-Beispiel2}
# Beispiel: Anordnung von Kugeln
factorial(10)
factorial(10) / prod(factorial(c(5, 4, 1)))
### Beispiel Kugeln (2/3)
library(gtools)
permutations(n=3, r=3, v=1:3, repeats.allowed=F)
```


## Kombinatorik
\framesubtitle{Permutationen mit und ohne Wiederholung in \R}
```{r}
### Beispiel Kugeln (3/3)
library(gtools)
(y <- permutations(n=3, r=3, v=c('w', 'w', 'r'), set=F, repeats.allowed=F))
unique(y, MARGIN = 2)
```




## Kombinatorik
\framesubtitle{Kombination ohne Wiederholung}

- Die Auswahl von $n$ Elementen, ohne dabei auf die Reihenfolge der Entnahme zu achten, aus einer Gesamtheit von $N$ Elementen wird als \hil{Kombination} bezeichnet, abgekürzt $K(N,n)$.
- Die Anzahl möglicher Kombinationen hängt davon ab, ob Wiederholung der Elemente zulässig ist.
- Die Formel für die Anzahl an Kombinationen ohne Wiederholung mit $n<N$ kann über Permutationen entwickelt werden.
- Da bei Kombinationen die Reihenfolge unerheblich ist, lassen sich die $N$ Elemente in $n$ Elemente, die in eine Kombination aufgenommen werden und in $N-n$ ausgeschlossene Elemente unterteilen.
-->



## Kombinatorik
\framesubtitle{Herleitung Binomialkoeffizient}

- Wie zuvor: Urne mit $N$ nummerierten Kugeln $1,2,\ldots,N$
- Neue Situation: Es sollen $n$ Elemente aus der Urne mit $N$ Kugeln gezogen werden ($n<N$). Dabei spielt die Reihenfolge der Entnahme keine Rolle. 
<!--- Die Auswahl von $n$ Elementen, ohne dabei auf die Reihenfolge der Entnahme zu achten, aus einer Gesamtheit von $N$ Elementen wird als \hil{Kombination} bezeichnet, abgekürzt $K(N,n)$.-->
- Da die Reihenfolge unerheblich ist, lassen sich die $N$ Elemente in $n$ Elemente, die gezogen werden und in $N-n$ ausgeschlossene Elemente unterteilen.
- Die $N$ Elemente unterscheiden sich jetzt nur noch hinsichtlich dieser beiden Merkmale.
- Die Formel für die Anzahl an Möglichkeiten, $n$ Kugeln aus $N$ zu ziehen mit $n<N$ kann über Permutationen entwickelt werden.
<!--- - Berechnet man hierfür die Anzahl der Permutationen mit Wiederholung für die so unterteilte Gesamtheit, entspricht diese der Anzahl der \hil{Kombinationen ohne Wiederholung} von $n$ aus $N$ Elementen.
\defn[Kombination ohne Wiederholung]
\begin{equation} K(N,n)=\frac{N!}{n!(N-n)!}=\binom{N}{n}.\label{BinKoeff}
\end{equation}
\enddefn -->

## Kombinatorik

\vspace{1cm}
\framesubtitle{Herleitung Binomialkoeffizient}

\xmpl[Lottozahlen (6 aus 49)]
Bei der Lottoziehung wird $n=6$ mal aus einer Lostrommel mit $N=49$ nummerierten Kugeln gezogen. Für die Wahrscheinlichkeit beim Lotto zu gewinnen, d.h. alle 6 gezogenen Zahlen richtig zu tippen, muss die Anzahl der günstigen durch die Anzahl der möglichen Ausgänge dividiert werden.      
\medskip

Die Anzahl der möglichen Ausgänge können wir wie folgt herleiten:    
\medskip

Es gibt insgesamt $N=49$ Möglichkeiten, die erste Kugel zu ziehen.  
Danach gibt es noch $N-1=48$ Möglichkeiten, die zweite Kugel zu ziehen, usw.  
Schließlich gibt es noch $N-n+1 = 44$ Möglichkeiten, die 6. Kugel zu ziehen.
\medskip

Insgesamt erhalten wir 
\[49\cdot48\cdot47\cdot46\cdot45\cdot44 = N\cdot(N-1)\cdot\ldots\cdot(N-n+1)\]
Möglichkeiten, 6 Kugeln aus 49 Kugeln zu ziehen. 
<!---\[K(49,6)=\frac{49!}{6!(49-6)!}=13.983.816.\]
Da nur ein günstiger Ausgang existiert, beträgt die Wahrscheinlichkeit:
\[P(G)=\frac{1}{13.983.816}=0.0000000715\,\hat{=}\,0.00000715\%.\]
Aber das heißt nicht, dass es unmöglich ist. Siehe z.B. \href{http://www.spiegel.de/panorama/gesellschaft/drei-lottogewinne-in-einem-monat-us-paar-mit-gluecksstraehne-a-961831.html}{hier}.-->
\endxmpl

## Kombinatorik
\vspace{1cm}
\framesubtitle{Herleitung Binomialkoeffizient}

\xmpl[*]
\[49\cdot48\cdot47\cdot46\cdot45\cdot44 = N\cdot(N-1)\cdot\ldots\cdot(N-n+1)\]
kann auch über die Fakultät dargestellt werden 
\[49\cdot48\cdot47\cdot46\cdot45\cdot44 =\frac{49!}{43!}\]
Nun sind aber jeweils genau $6!$ der Möglichkeiten, 6 aus 49 Kugeln zu ziehen, Permutationen voneinander. Durch Division der $6!$ Permutationen der selben gezogenen Kugeln erhalten wir
\[\frac{49!}{43!6!}=13.983.816\]
Möglichkeiten 6 Kugeln aus 49 zu ziehen, ohne dabei auf die Reihenfolge der Entnahme zu achten. 
<!---Da nur ein günstiger Ausgang existiert, beträgt die gesuchte Wahrscheinlichkeit:
\[P(G)=\frac{1}{13.983.816}=0.0000000715\,\hat{=}\,0.00000715\%.\]
Aber das heißt nicht, dass es unmöglich ist. Siehe z.B. \href{http://www.spiegel.de/panorama/gesellschaft/drei-lottogewinne-in-einem-monat-us-paar-mit-gluecksstraehne-a-961831.html}{hier}.-->
\endxmpl

<!---
## Kombinatorik
\framesubtitle{Binomialkoeffizient}

 Allgemein erhält man
\[ \frac{N!}{n!(N-n)!}=\frac{N(N-1)\cdot\ldots\cdot(N-n+1)(N-n)(N-n-1)\cdot\ldots
 \cdot 1}{n(n-1)\cdot\ldots\cdot 1\cdot (N-n)(N-n-1)\cdot\ldots\cdot
 1}\\
 = \frac{N(N-1)\cdot\ldots\cdot(N-n+1)}{n(n-1)\cdot\ldots\cdot 1}\]
 Dies ist der \hil{Binomialkoeffizient} $\binom{N}{n}$, gelesen ,,$N$ über $n$''
 
\defn[Binomialkoeffizient]
\begin{equation} \frac{N!}{n!(N-n)!}=\binom{N}{n}.\label{BinKoeff}
\end{equation}
\enddefn-->


## Kombinatorik
\framesubtitle{Herleitung Binomialkoeffizient}

\xmpl[*]
Da nur ein günstiger Ausgang existiert, beträgt die gesuchte Wahrscheinlichkeit:
\[P(G)=\frac{1}{13.983.816}=0.0000000715\,\hat{=}\,0.00000715\%.\]
Aber das heißt nicht, dass es unmöglich ist. Siehe z.B. \href{http://www.spiegel.de/panorama/gesellschaft/drei-lottogewinne-in-einem-monat-us-paar-mit-gluecksstraehne-a-961831.html}{hier}
\endxmpl


## Kombinatorik
\framesubtitle{Binomialkoeffizient}

 Allgemein erhält man
\[ \frac{N!}{n!(N-n)!}=\frac{N(N-1)\cdot\ldots\cdot(N-n+1)(N-n)(N-n-1)\cdot\ldots
 \cdot 1}{n(n-1)\cdot\ldots\cdot 1\cdot (N-n)(N-n-1)\cdot\ldots\cdot
 1}\]
\[ = \frac{N(N-1)\cdot\ldots\cdot(N-n+1)}{n(n-1)\cdot\ldots\cdot 1}\]

 Dies ist der \hil{Binomialkoeffizient} $\binom{N}{n}$, gelesen \glqq{}$N$ über $n$\grqq{}
 
 
## Kombinatorik
\framesubtitle{Binomialkoeffizient}

\defn[Binomialkoeffizient]
\begin{equation} \frac{N!}{n!(N-n)!}=\binom{N}{n}.\label{BinKoeff}
\end{equation}
\enddefn




<!---
## Kombinatorik
\framesubtitle{Herleitung Binomialkoeffizient}

\xmpl[Lottozahlen (6 aus 49)]
Bei der Lottoziehung wird sechs mal aus einer Lostrommel mit 49 nummerierten Kugeln gezogen. Für die Wahrscheinlichkeit beim Lotto zu gewinnen, d.h. alle 6 gezogenen Zahlen richtig zu tippen, muss die Anzahl der günstigen durch die Anzahl der möglichen Ausgänge dividiert werden. Die Anzahl der möglichen Ausgänge berechnet sich als Kombination ohne Wiederholung:
\[K(49,6)=\frac{49!}{6!(49-6)!}=13.983.816.\]
Da nur ein günstiger Ausgang existiert, beträgt die Wahrscheinlichkeit:
\[P(G)=\frac{1}{13.983.816}=0.0000000715\,\hat{=}\,0.00000715\%.\]
Aber das heißt nicht, dass es unmöglich ist. Siehe z.B. \href{http://www.spiegel.de/panorama/gesellschaft/drei-lottogewinne-in-einem-monat-us-paar-mit-gluecksstraehne-a-961831.html}{hier}.
\endxmpl-->


## Kombinatorik
\framesubtitle{Beispiel Binomialkoeffizient}
\xmpl[Texas Hold'em]
Die Wahrscheinlichkeit bei der Poker-Variante Texas Hold'em eine Starthand mit zwei Assen zu erhalten, kann ebenfalls mit der Formel für die Kombination ohne Wiederholung berechnet werden. Anzahl der günstigen Ausgänge:
\[\binom{4}{2}=\frac{4!}{2!(4-2)!}=6:\]
$(\spadesuit,\clubsuit)$, $(\clubsuit,\diamondsuit)$, usw. Anzahl der möglichen Ausgänge (Starthände):
\[\binom{52}{2}=\frac{52!}{2!(52-2)!}=1326.\]
Wahrscheinlichkeit:
\[P(\text{zwei Asse})=\frac{6}{1326}=0.0045\,\hat{=}\,0.45\%.\]
\endxmpl



## Kombinatorik
\framesubtitle{Binomialkoeffizient \R}

```{r, eval = -1}
# Beispiel: Lottozahlen (6 aus 49)
# Anzahl möglicher Ergebnisse
factorial(49) / (factorial(6) * factorial(43))
choose(49, 6) # kürzer

1/choose(49,6) #WSK für 6 richtige im Lotto

### Beispiel Texas Hold'em
(hands <- choose(52,2))

(aces <- choose(4,2))

aces/hands #WSK für 2 Asse als Starthand
```

<!--## Kombinatorik
\framesubtitle{Kombination mit Wiederholung}

- Bei \hil{Kombinationen mit Wiederholung} kann jedes Element mehrmals ausgewählt werden.
- Da es nun vorkommen kann, dass eine Kombination $n$-mal dasselbe Element enthält, muss jedes Element $(n-1)$-mal zusätzlich zur Verfügung stehen.
- Die Anzahl der Kombinationen mit Wiederholung von $n$ aus $N$ Elementen, $K_W(N,n)$, beträgt daher:
\defn[Kombinationen mit Wiederholung]
\[ K_W(N,n)= \binom{N+n-1}{n}. \]
\enddefn




## Kombinatorik
\framesubtitle{Kombination mit Wiederholung}

- Die Wiederholungsmöglichkeit der Elemente erreicht man dadurch, dass die Kombinationen durch *Ziehen mit Zurücklegen* gebildet werden.
- Bei dieser Vorgehensweise erschöpft sich die Gesamtheit durch Entnahme ihrer Elemente nicht, und die bei Kombinationen ohne Wiederholung notwendige Einschränkung $n<N$ entfällt.




## Kombinatorik
\framesubtitle{Kombination mit Wiederholung}
\xmpl[Buchstabenkombination]
Mit den Buchstaben $b$ und $u$ sollen alle Kombinationen mit Wiederholung %zur Klasse 4 gebildet werden. Es gilt $N=2$ und $n=4$; somit beträgt
\[ K_W(2,4)=\binom{2+4-1}{4}=\binom{5}{4}=\frac{5\cdot4\cdot3\cdot2}{4 \cdot3\cdot2}=5. \]
Die fünf Kombinationen lauten $(bbbb),(bbbu),(bbuu),(buuu)$ und $(uuuu)$.
\endxmpl




## Kombinatorik
\framesubtitle{Kombination mit Wiederholung in \R}

```{r}
# library(gtools)
### Beispiel: Buchstabenkombinationen

# Anzahl an Kombinationen mit N = 2, n = 4
choose(5, 4)

# Die Kombinationen selbst:
combinations(n = 2, r = 4, v = c('b', 'u'), repeats.allowed = T)
```


## Kombinatorik
\framesubtitle{Variation ohne Wiederholung}

- \hil{Variationen ohne Wiederholung} $V(N,n)$ sind definitionsgemäß geordnete Kombinationen ohne Wiederholung.
- Jede dieser Kombinationen von $n$ aus $N$ Elementen kann auf $n!$ verschiedene Arten angeordnet (permutiert) werden.
- Da es insgesamt $\binom{N}{n}$ Kombinationen ohne Wiederholung gibt, ist die Anzahl der Variationen ohne Wiederholung $\binom{N}{n}n!$.

\defn[Variation ohne Wiederholungen]
\[ V(N,n)=\binom{N}{n}n!=\frac{N!}{(N-n)!}. \]
\enddefn



## Kombinatorik
\framesubtitle{Variation ohne Wiederholung}
\xmpl[Variation ohne Wiederholung]
\[\{a, b, c\}\]
Anordnungsmöglichkeiten, wenn man 2 aus den 3 Elementen durch ZoZ entnimmt und die Reihenfolge dabei wichtig ist:
\[V(3,2)= \dfrac{3!}{(3-2)!}=6\]
\[\{a, b\};\;\{a, c\};\;\{b, a\};\;\{b, c\};\;\{c, a\};\;\{c, b\}\]
\endxmpl



## Kombinatorik
\framesubtitle{Variationen ohne Wiederholung in \R}
```{r}
# Beispiel: Variation ohne Wiederholung
choose(3, 2) * factorial(2)
permutations(n = 3, r = 2, v = c('a', 'b', 'c'), repeats.allowed = F)
```


## Kombinatorik
\framesubtitle{Variation ohne Wiederholung}
\xmpl[WM-Kader]
Der WM-Kader von Jogi Löw besteht aus 23 Spielern, von denen 3 Torhüter, 8 Verteidiger, 10 Mittelfeldspieler und 2 Stürmer sind.
Vor jedem Spiel muss er sich für 11 Spieler (1 Torhüter, 4 Verteidiger, 5 Mittelfeldspieler und 1 Stürmer) entscheiden.
Die Anzahl aller Anordnungsmöglichkeiten kann für jede Position als Variation ohne Wiederholung berechnet werden und ergibt sich anschließend als Produkt:
\begin{eqnarray*}
V(3,1)\cdot V(8,4)\cdot V(10,5) \cdot V(2,1) &=& \dfrac{3!}{(3-1)!}\dfrac{8!}{(8-4)!}\dfrac{10!}{(10-5)!}\dfrac{2!}{(2-1)!}\\
&=& 304.819.200.
\end{eqnarray*}
\endxmpl



## Kombinatorik
\framesubtitle{Variation mit Wiederholung}
\defn[Variation mit Wiederholungen]
Bei Variationen mit Wiederholung $V_W(N,n)$ hat man $N$ verschiedene Besetzungsmöglichkeiten für jeden Platz in der Variation.
Diese Anzahl bestimmt sich zu
\[ V_W(N,n)=N^n. \]
\enddefn



## Kombinatorik
\framesubtitle{Variation mit Wiederholung}
\xmpl[Variation mit Wiederholung]
\[\{a, b, c\}\]
Anordnungsmöglichkeiten, wenn man 2 aus den 3 Elementen durch ZmZ entnimmt und die Reihenfolge dabei wichtig ist:
\[V_W(3,2)= 3^2=9\]
\[\{a, a\};\;\{b, b\};\;\{c, c\};\{a, b\};\;\{a, c\};\;\{b, a\};\;\{b, c\};\;\{c, a\};\;\{c, b\}\]
\endxmpl



## Kombinatorik
\framesubtitle{Variation mit Wiederholung}
\xmpl[Geburtstage]
Wie hoch ist die Wahrscheinlichkeit, dass bei $n$ Personen mindestens zwei von ihnen am gleichen Tag Geburtstag haben (ohne Beachtung des Jahrganges)?
Siehe Anhang.
<!-- %Die Anzahl aller möglichen Fälle lässt sich als Variation mit Wiederholung berechnen: -->
<!-- %\[V_W(365,n)=365^n.\] -->
<!-- %Z.B. ergeben sich für zwei Personen $365^2=133.225$ mögliche Fälle von Geburtstagsvariationen.\\[2ex] -->
<!-- %Falls alle $n$ Personen an unterschiedlichen Tagen Geburtstag haben sollen, ergibt sich die Anzahl der Geburtstagsvariationen als Variation ohne Wiederholung: -->
<!-- %\[V(365,n)=\dfrac{365!}{(365-n)!}.\] 
\endxmpl





## Kombinatorik
\framesubtitle{Variation mit Wiederholung}
\xmpl[Permutation von Kombinationen]
Für eine Gesamtheit mit $N=4$ verschiedenen Elementen kann man $\binom{4}{3}=4$ Kombinationen ohne Wiederholung zur Klasse $n=3$ bilden.
Jede Kombination kann $3!=6$ mal permutiert werden. Die Anzahl der \hil{Variationen ohne Wiederholung} beträgt daher 24.
Dasselbe Ergebnis stellt sich ein mit $\frac{4!}{1!}=4\cdot3\cdot2=24$.
Die Anzahl der \hil{Variationen mit Wiederholung} ist $V_W(4,3)=4^3=64$.
\endxmpl



## Kombinatorik
\framesubtitle{Übersicht}

\resizebox{.95\textwidth}{!}{
\begin{forest}
sn edges/.style={for tree={
 parent anchor=south,
 child anchor=north,
 minimum height=1cm,
 align=center,
 l sep=0.75cm,
 s sep=0.4cm}},
sn edges
[{$N$ Elemente}, align=center
  [{$A, B, C, D, \dots$}, name=LVL1, calign=first
    [{ja}, name=LVL2
      [{$A,B = B,A$}, name=LVL3
        [{\st{$A,A$}}, name=LVL4]
        [{$A,A$}]
      ]
      [{$A,B \neq B,A$}, name=VAR
        [{\st{$A,A$}}]
        [{$A,A$}]
      ]
    ]
    [{nein}
      [{$A,B \neq B,A$}, name=PER
        [{\st{$A,A$}}]
      ]
    ]
  ]
  [{$A, B, B, C, D, D, \dots$}
    [{nein}
      [{$A,B \neq B,A$}
        [{$A,A$}]
      ]
    ]
  ]
]
\node[anchor=west,align=right]
  at ([xshift=-2.8cm]LVL4.west) {mit oder ohne\\Wiederholung};
\node[anchor=west,align=right]
  at ([xshift=-4.1cm]LVL3.west) {Reihenfolge wesentlich\\oder nicht};
\node[anchor=west,align=right]
  at ([xshift=-3.7cm]LVL3.west|-LVL2) {Auswahl $n$ aus $N$};
\node[anchor=west,align=right]
  at ([xshift=-3.4cm]LVL3.west|-LVL1) {Elemente gleich\\oder verschieden};
\node[anchor=south,align=center]
  at ([yshift=-3.25cm]LVL3.south) {$\underbrace{\hspace{2cm}}_{\text{Kombinationen}}$\\[.25cm]$ \binom{N}{n} \hspace{.4cm}\binom{N+n-1 }{n}$};
\node[anchor=south,align=center]
  at ([yshift=-3.25cm]VAR.south) {$\underbrace{\hspace{2cm}}_{\text{Variationen}}$\\[.25cm]$\frac{N!}{(N-n)!} \hspace{.5cm} N^n$};
\node[anchor=south,align=center]
  at ([yshift=-3.25cm, xshift=1.3cm]PER.south) {$\underbrace{\hspace{3.5cm}}_{\text{Permutationen}}$\\[.25cm]$ N! \hspace{1.75cm} \frac{N!}{\prod_i n_i!}$};
\end{forest}
}
-->

## Kombinatorik
\framesubtitle{Aufgaben}
<!--\exe
Bei den Olympischen Sommerspielen starten 8 Sprinter zum 100m Lauf. Wie viele Anordnungsmöglichkeiten gibt es für Gold, Silber, Bronze?
\endexe-->
\exe
Eine Maschine kann 10 verschiedene Werkstücke produzieren. Wie viele verschiedene Fertigungsreihenfolgen dieser 10 Werkstücke gibt es?
\endexe
\exe
Eine Urne enthält 5 Bälle. Wie viele Anordnungsmöglichkeiten gibt es, wenn die Bälle mit den Ziffern 1 bis 5 durchnummeriert sind?
\endexe
\exe
Wie groß ist die Wahrscheinlichkeit bei einem Kartenspiel 13 Karten zu erhalten, von denen 7 Karten Pik sind?
(Hinweis: ein Kartenspiel besteht aus 52 Karten: 13 Kreuz, 13 Pik, 13 Herz, 13 Karo)
\endexe 



<!--## Kombinatorik
\framesubtitle{Aufgaben}
\exe
Auf einem Tippschein mit 12 Spielen wird getippt, welcher von drei möglichen Ausgängen jeweils eintreten wird. Wie viele Tippreihen sind möglich?
\endexe
\exe
Auf wie viele Arten kann man 4 verschiedene Briefmarkenmotive zu 1€, zu einem Porto von 5€ aufkleben, wenn die Reihenfolge unwichtig ist?
\endexe 
\exe
Wie viele Anordnungsmöglichkeiten der folgenden Buchstaben gibt es: STATISTIK?
\endexe-->



<!--## Kombinatorik
\framesubtitle{Aufgaben}
\exe
Wenn eine Münze 3 mal geworfen wird, wie viele Anordnungsmöglichkeiten der beiden Ausgänge Kopf und Zahl gibt es, wenn die Reihenfolge dabei wichtig ist?
\endexe
\exe
Wie viele Möglichkeiten gibt es aus den drei Buchstaben X, Y, Z
vierstellige Codes zu erzeugen?
\endexe
\exe 
Eine Urne enthält 5 Bälle. Wie viele Anordnungsmöglichkeiten gibt es, wenn es 3 weiße und 2 rote Bälle gibt?
\endexe-->



<!--## Anhang
\framesubtitle{Variation mit Wiederholung}
\xmpl[Geburtstage]
Wie hoch ist die Wahrscheinlichkeit, dass bei $n$ Personen mindestens zwei von ihnen am gleichen Tag Geburtstag haben (ohne Beachtung des Jahrganges)?
Die Anzahl aller möglichen Fälle lässt sich als Variation mit Wiederholung berechnen:
\[V_W(365,n)=365^n.\]
Z.B. ergeben sich für zwei Personen $365^2=133.225$ mögliche Fälle von Geburtstagsvariationen.
Falls alle $n$ Personen an unterschiedlichen Tagen Geburtstag haben sollen, ergibt sich die Anzahl der Geburtstagsvariationen als Variation ohne Wiederholung:
\[V(365,n)=\dfrac{365!}{(365-n)!}.\]
\endxmpl



## Anhang
\framesubtitle{Variation mit Wiederholung}
\xmpl[Geburtstage]
Somit beträgt die Wahrscheinlichkeit, dass bei $n$ Personen mindestens zwei von ihnen am gleichen Tag Geburtstag haben:
\[P(\text{mindestens 2 gleiche Geburtstage})=1-\dfrac{\dfrac{365!}{(365-n)!}}{365^n}.\]
Bei 23 Personen ist diese Wahrscheinlichkeit bereits:
\[1-\dfrac{\dfrac{365!}{(365-23)!}}{365^{23}}=1-\dfrac{\dfrac{365\cdot 364\cdot\ldots\cdot 343\cdot 342!}{342!}}{365^{23}}=1-\dfrac{364\cdot\ldots\cdot 343}{365^{22}}=0.5073.\]
\endxmpl-->



# Eindimensionale Zufallsvariablen



## Eindimensionale Zufallsvariablen
\framesubtitle{Zahlenzuordnung}

- Die in $\Omega$ aufgeführten Ausgänge $\omega_i$ müssen keine \hil{Zahlen} sein, sondern sind oft \hil{artmäßig} angegeben.
- Beim *Münzwurf* ist der artmäßig festgelegte Stichprobenraum
\[ \Omega=\{\omega_1: \text{\textit{Kopf liegt oben}},\; \omega_2: \text{\textit{Zahl liegt oben}}\}.\]

- Beim Würfelbeispiel ist die Überführung der Ausgänge in Zahlen einfach. Setzt man $\omega_i=i,$ $i=1,\ldots,6,$ enthält der Stichprobenraum nur Zahlen:
\[\Omega=\{1,\ldots,6\}.\]
- Der Münzwurf liefert keine natürliche Zahlenvorgabe. Die Zuordnung könnte lauten: $\text{\textit{Kopf}} \to 0$ und $\text{\textit{Zahl}}\to 1$, aber auch: $\text{\textit{Kopf}} \to 12$ und $\text{\textit{Zahl}}\to 27$.




## Eindimensionale Zufallsvariablen
\framesubtitle{Zahlenzuordnung}

\defn[Eindimensionale Zufallsvariable]
Eine Funktion $X$, die jedem Ausgang $\omega\in\Omega$ eine reelle Zahl $x\in \mathbb{R}$ zuordnet,
\[ X:\quad \Omega\longrightarrow\mathbb{R},\]
heißt \hil{eindimensionale Zufallsvariable} oder kurz auch \hil{Zufallsvariable}, wenn $\{\omega\in\Omega |X(\omega)\leq x\}$ für alle $x\in \mathbb{R}$ ein Ereignis ist.
\enddefn



## Eindimensionale Zufallsvariablen
\framesubtitle{Zahlenzuordnung}

- $x=X(\omega)$ ist \hil{Wert} bzw. \hil{Realisation} von $X$; die Menge  \[\{x|x=X(\omega)\in\mathbb{R}\; , \; \omega\in\Omega\}= X(\Omega)\]
 heißt \hil{Wertebereich}. Er gibt an, welche Werte die Zufallsvariable $X$ bei gegebenem $\Omega$ annehmen kann.
- Durch die Bedingung in der vorangeführten Definition lassen sich alle Ereignisse $A$ 
 als Intervalle der reellen Zahlen oder als reelle Zahlen selbst angeben, die jetzt die Ereignisse für $X$ darstellen.
- Umgekehrt folgt, dass für jedes Intervall $X\leq x$ ein Ereignis existiert. $X$ bildet somit $\Omega$ und jedes Ereignis auf die reellen Zahlen ab.




## Eindimensionale Zufallsvariablen
\framesubtitle{Zahlenzuordnung}
\vspace{1cm}
- Die nachfolgende Grafik veranschaulicht diesen Zusammengang:

```{r Zahlenzuordnung, echo = F, fig.width = 0.65}
library(shape) # for Arrows()
par(mar = c(4,4,3,3), xpd = NA)
plot(c(0, 4), c(0, 1), xaxt = 'n', yaxt = 'n', type = 'n', xlab = '', ylab = '', bty = 'n', xaxs = 'i')
axis(1, at = seq(1, 4, 0.5), labels = 2:8,  pos = 0)

Arrows(x0 = -.3, y0 = 0.85, x1 = 2, y1 = 0.05, arr.length = 0.3,
       code = 2, lty = par("lty"), arr.adj = 1,
       lwd = 1)
Arrows(x0 = 0.15, y0 = 0.85, x1 = 3.5, y1 = 0.05, arr.length = 0.3,
       code = 2, col = par("fg"), lty = par("lty"), arr.adj = 1,
       lwd = 1)
Arrows(x0 = 4, y0 = 0, x1 = 4.2, y1 = 0, arr.length = 0.3,
       code = 2, col = par("fg"), lty = par("lty"), arr.adj = 0,
       lwd = 1)

mtext(1, text = "$X$", at = 4.3, line = 0)
text(x = 0.5, y =  .9, labels = "$\\{ \\omega_1,\\; \\omega_2,\\; \\dots,\\; \\omega_m,\\; \\dots \\}$",
     cex = 1.3)
text(x = 0.2, y = .3, labels = "$X(\\omega_1)$", cex = 1.3)
text(x = 2.3, y = .6, labels = "$X(\\omega_2)$", cex = 1.3)
```










## Eindimensionale Zufallsvariablen
\framesubtitle{Zahlenzuordnung}

- Für \[A = \{\omega\in\Omega|X(\omega)\leq x\}\] existiert die \hil{Wahrscheinlichkeit} $P(A)$, die jetzt als die Wahrscheinlichkeit für das Ereignis $\{X\leq x\}$ interpretiert werden kann:
\[ P(A)=P(\{X\leq x\})=P(X\leq x).\]

- $X$ überträgt somit auch die Wahrscheinlichkeiten von den Ereignissen eines Zufallsexperimentes auf die reellen Zahlen.

- Spezifiziert man so für alle Ereignisse des neuen Stichprobenraumes $X(\Omega)$ die Wahrscheinlichkeiten, erhält man die \hil{Wahrscheinlichkeitsverteilung} von $X$.




## Eindimensionale Zufallsvariablen
\framesubtitle{Zahlenzuordnung}
\xmpl[Münzwurf]
Den Stichprobenraum für das *2-malige Werfen einer Münze* erhält man als
\[\Omega=\{(K,K),(K,Z),(Z,K),(Z,Z)\},\quad Z\textit{: Zahl},  K\text{\textit{: Kopf}}.\]

$X$ bildet die Ausgänge $\omega_i,$ $i=1,\ldots,4$ wie folgt in die reellen Zahlen ab
\begin{equation*}
X = \begin{cases}
0& (K,K)\\
1& (K,Z)\quad\text{oder}\quad(Z,K)\\
2& (Z,Z).
\end{cases}
\end{equation*}
\endxmpl



## Eindimensionale Zufallsvariablen
\framesubtitle{Zahlenzuordnung}
\xmpl[Münzwurf]
Als Wertebereich erhält man $X(\Omega)=\{0,1,2\}$. Für z.B. $X(\omega)=0$ lautet das zugehörige Ereignis $A_1=\{(K,K)\}$. $\{X\leq 1\}$ entspricht dem Ereignis $A_2=\{(K,K),(K,Z),(Z,K)\}$.
\medskip

Ist die Münze fair, sind alle Elementarereignisse gleich wahrscheinlich:
\[P(\{K,K\})=P(\{K,Z\})=P(\{Z,K\})=P(\{Z,Z\})=\frac{1}{4}.\]
$P(\{X=0\})$ und $P(\{X\leq 1\})$ ermittelt man dann über $P(A_1)$ und $P(A_2)$:
\[P(X=0)=P(A_1)=\dfrac{1}{4} \quad\text{und}\quad P(X\leq 1)=P(A_2)=\dfrac{3}{4}.\]
\endxmpl



## Eindimensionale Zufallsvariablen
\framesubtitle{diskrete / stetige Zufallsvariable}

- Ist $\Omega$ endlich oder abzählbar unendlich, liegt eine \hil{diskrete} Zufallsvariable vor.
- Bei einem überabzählbar unendlichen Stichprobenraum kann $X$ jeden beliebigen Zahlenwert eines vorgegebenen Intervalls der reellen Zahlen annehmen, wobei auch $(-\infty,\infty)$ zulässig ist.
- Die Zufallsvariable $X$ nennen wir dann \hil{stetig}.




## Eindimensionale Zufallsvariablen
\framesubtitle{diskrete / stetige Zufallsvariable - QuizAcademy}
\QuizAcademy{Zufallsvariablen 1}\endQuizAcademy



## Eindimensionale Zufallsvariablen
\framesubtitle{diskrete / stetige Zufallsvariable}

\xmpl[diskrete / stetige Zufallsvariable]\label{xmpl:versp}

(a) Ist $X$ die Anzahl der Würfe einer Münze, bis zum ersten Mal *Zahl* oben liegt, sind $\Omega$ und Wertebereich $X(\Omega)$ abzählbar unendlich.
\par\medskip
$X$ ist daher eine \hil{diskrete} Zufallsvariable mit $X(\omega_i) = x_i=i$ für $i=1,2\ldots,m,m+1,\ldots$.
(b) Bei *Verspätung eines Zuges* $X$ kann bei unendlich genauer Zeitmessung jede Verspätung eines Intervalls $[x_1,x_2] \subset\mathbb{R}$ eintreten.
\par\medskip
$X$ hat einen überabzählbar unendlichen Wertebereich und ist daher \hil{stetig}.

\endxmpl



## Eindimensionale Zufallsvariablen
\framesubtitle{Lineartransformation}

- Jede auf $X(\Omega)$ definierte reellwertige Funktion $g$ überführt $X$ in eine neue Zufallsvariable $Y=g(X)$.
- Auch $Y$ liegt der Stichprobenraum $\Omega$ zugrunde, denn $Y$ stellt eine geschachtelte Funktion dar: \[Y=g[X(\Omega)].\]
- $X$ sei im Beispiel \ref{xmpl:versp} \blue{(b)} in Minuten gemessen. Für die Verspätung in Sekunden resultiert eine Zufallsvariable $Y=60 X$. Ist $X=3$ (Minuten), folgt $Y=180$ (Sekunden). Die Wahrscheinlichkeit für eine Verspätung zwischen 2 und 3 Minuten ist dieselbe wie für eine Verspätung zwischen 120 und 180 Sekunden: 
\[ P(2\leq X\leq3)=P(120\leq Y\leq180).\]
- Man überträgt die Wahrscheinlichkeiten für Ereignisse aus $\Omega$ jetzt auf die Ereignisse von $Y$.


## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Verteilungsfunktion}
 
- Um z.B. die Wahrscheinlichkeit für Ereignisse der Art
\[\{X\leq x\},\: \{x_1 < X \leq x_2\},\:\{X=x_1\}\: \text{ oder }\: \{X\neq x_1\}\]
zu berechnen, ist die \hil{Verteilungsfunktion} hilfreich.

\defn[Verteilungsfunktion] \label{VtlgFunktion}
Eine Funktion $F$ mit
\begin{equation} F(x) = P(X\leq x),\quad x\in\mathbb{R},\label{VFunktion}
\end{equation}
heißt \hil{Verteilungsfunktion} von $X$.
\enddefn

## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Verteilungsfunktion}

- Aufgrund der \hil{Kolmogoroff--Axiome} hat $F(x)$ folgende Eigenschaften:
  (1) $F(x)$ ist \hil{monoton wachsend} mit Werten des Intervalls [0,1]: Gilt $x_2>x_1$, so folgt daraus $F(x_2)\geq F(x_1)$.
  (2) \[\lim_{x\rightarrow-\infty}\limits F(x)=F(-\infty)=0\] und \[\lim_{x\rightarrow\infty}\limits F(x)=F(\infty)=1.\]
  (3) $F(x)$ ist an jeder Stelle $x$ \hil{rechtsseitig stetig}, d.h. \[\lim_{h\rightarrow 0^+}\limits F(x+h)=F(x).\]
Die Notation $h\rightarrow 0^+$ besagt, dass $h$ mit einem Wert größer 0 startet und bei der Grenzwertberechnung sich daher von rechts der 0 nähert.




## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Verteilungsfunktion}

(4) Für jedes Ereignis $\{x_1<X\leq x_2\}$ gilt \[ P(x_1 < X \leq x_2)=P(X \leq x_2) - P(X \leq x_1) = F(x_2) -  F(x_1).\]
 
- Diese vier allgemeinen Eigenschaften gelten für diskrete und stetige Zufallsvariablen.
- Da aber die weitere mathematische Behandlung eine Fallunterscheidung verlangt, erfolgt eine getrennte Darstellung.




## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{diskrete Verteilungsfunktion}
\xmpl[Fairer Würfel]
Der Wertebereich ist $X(\Omega)=\{1,2,3,4,5,6\}$ mit $P(X=x_i)=\frac{1}{6}$ für $i=1,\ldots,6$.

Die Verteilungsfunktion $F(x)$ ist dann
\begin{equation*}
F(x)=
\begin{cases}
0& x < 1 \\
\frac{1}{6}& 1 \leq x < 2 \\
\frac{2}{6}& 2 \leq x < 3 \\
\vdots &\\
\frac{5}{6}& 5 \leq x < 6 \\
1& x \geq 6.
\end{cases}
\end{equation*}
\endxmpl



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{diskrete Verteilungsfunktion}

\xmpl[*]
```{r, echo = F, fig.width=0.75}
plot(c(0, 7), bty = "l", las = 1, xlim = c(0, 7), ylim = c(0, 1.2), type = "n", yaxt = "n", xaxt = "n", xlab = "$x$", ylab = "$F(x)$")
axis(1, at = 1:6)
axis(2, at = 1:6/6, labels = c(paste0("$\\frac{",1:5, "}{6}$"), "$1$"), las = 1)
segments(x0 = 1:6, y0 = seq(1/6, 1, 1/6), x1 = 2:7, y1 = seq(1/6, 1, 1/6), col = "tomato3")
segments(x0 = 1:6, y0 = rep(0, 6), x1 = 1:6, y1 = seq(1/6, 1, 1/6), lty = 2)
points(x = 1:6, y = seq(1/6, 1, 1/6), col = "tomato3")
```
\endxmpl



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{diskrete Verteilungsfunktion}

- Bei diskreten Zufallsvariablen ist $F(x)$ eine \hil{Treppenfunktion}.
- Die Stufenhöhen entsprechen den Wahrscheinlichkeiten für $X=x_i$.
- Die \hil{Wahrscheinlichkeitsfunktion} $f(x)$ einer diskreten Zufallsvariablen ordnet den $x_i=X(\omega_i)$ Wahrscheinlichkeiten zu.
- $f(x)$ heißt manchmal auch \hil{Wahrscheinlichkeitsmassefunktion}.



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Wahrscheinlichkeitsfunktion}
\defn[Wahrscheinlichkeitsfunktion]
Die Funktion \[f :\; X(\Omega)\longrightarrow\mathbb{R}^+\] mit
\begin{equation*}
f(x)=
\begin{cases}
P(X=x_i)=p_i& i=1,2,\ldots \\
0 & \text{sonst}.
\end{cases}
\end{equation*}

heißt \hil{Wahrscheinlichkeitsfunktion} von $X$ und ordnet jeder reellen Zahl $x_i$ die Wahrscheinlichkeit $P(X=x_i)$ zu.\mps
Für diese gilt
\[f(x_i)\geq 0\quad\text{und}\quad \sum_i f(x_i)=1.\]
\enddefn



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Wahrscheinlichkeitsfunktion}

\vspace{1cm}

\xmpl[*]
\begin{equation*}
f(x)=
\begin{cases}
P(X=x_i)=p_i=\frac{1}{6}& i=1,\ldots,6\\
0 & \text{sonst}.
\end{cases} 
\end{equation*}


```{r DiskreteWahrscheinlichkeitsFkt, echo=F, fig.width=0.7, fig.asp=0.5, eval =FALSE}
plot(c(0, 7), bty = "l", las = 1, xlim = c(0, 7), ylim = c(0, 2/6), type = "n", yaxt = "n", xaxt = "n", xlab = "$x$", ylab = "$f(x)$")
axis(1, at = 1:6)
axis(2, at = 1/6, labels = "$\\frac{1}{6}$", las = 1)
segments(x0 = 1:6, y0 = rep(0, 6), x1 = 1:6, y1 = rep(1/6, 6), lty = 2)
points(x = 1:6, y = rep(1/6, 6), col = "tomato3")
```

\endxmpl


## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Wahrscheinlichkeitsfunktion}


\begin{align*}
P(x_1< X \leq x_2) & = \sum_{x_1<x_i\leq x_2} f(x_i) = \sum_{x_1<x_i\leq x_2}P(X=x_i) \\
& =\sum_{i:x_1<x_i\leq x_2}p_i.
\end{align*}

- Die Intervallgrenzen $x_1$ und $x_2$ sind beliebig; sie müssen nicht Elemente von $X(\Omega)$ sein. Für bspw. $x_1=1.5$ und $x_2=4.8$ folgt für den \hil{fairen Würfel}
\begin{align*}
P(1.5<X\leq4.8) & = P(X=2)+P(X=3)+P(X=4) \\
                & = p_1+p_2+p_3=\frac{1}{2}.
\end{align*}




## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Wahrscheinlichkeitsfunktion}
\exe
Gegeben ist das Zufallsexperiment *dreimaliger Münzwurf einer unfairen Münze* mit $P(K)=\frac{1}{3}$ und $P(Z)=\frac{2}{3}$.

(a) Stellen Sie den Stichprobenraum $\Omega$ und den Wertebereich $X(\Omega)$ für $X$: *Anzahl Kopf* dar!
(b) Entwickeln Sie die Wahrscheinlichkeitsfunktion und stellen Sie diese grafisch dar!
(c) Entwickeln Sie die Verteilungsfunktion und stellen Sie diese auch grafisch dar!

\endexe



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Wahrscheinlichkeitsfunktion}
\vspace{1cm}

\exe
Für Zufallsvariable $X$: *Anzahl der Autoverkäufe pro Tag* gilt:
\begin{center}
\begin{tabular}{|c|cccccc|}\hline
$X$   & 0 & 1 & 2 & 3 & 4 & 5\\\hline
$f(x)$& 0.18 & 0.39 & 0.24 & 0.14 & 0.04 & 0.01\\
$F(x)$& 0.18 & 0.57 & 0.81 & 0.95 & 0.99 & 1\\\hline
\end{tabular}
\end{center}

Wie groß ist die Wahrscheinlichkeit, dass

(a) am Tag ein Auto verkauft wird?
(b) am Tag höchstens 2 Autos verkauft werden?
(c) am Tag mehr als 2, aber höchstens 4 Autos verkauft werden?
(d) am Tag mindestens 2, aber höchstens 4 Autos verkauft werden?

\endexe



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Wahrscheinlichkeitsdichte}

- Eine \hil{stetige Zufallsvariable} kann jeden Wert eines Intervalls der reellen Zahlen annehmen.
- $P(X\leq x)$ resultiert hier wegen der überabzählbar unendlich vielen Realisationen von $X$ durch \hil{Integration} einer geeigneten Funktion $f(x)$.
- Da $-\infty<X<\infty$ das sichere Ereignis ist, muss gelten:
 \[f(x)\geq 0\quad\text{und}\quad P(-\infty<X<\infty)=\int^\infty_{-\infty}\limits f(x)\, dx=1.\] Derartige Funktionen heißen \hil{Dichtefunktionen}.




## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Wahrscheinlichkeitsdichte}
\defn[Dichtefunktion] \label{DichteFkt}
Eine Funktion $f:\; X(\Omega)\longrightarrow\mathbb{R}^+$ heißt \hil{Wahrscheinlichkeitsdichte} oder \hil{Dichtefunktion}, wenn gilt
\[ F(x)=P(X\leq x) = \int_{-\infty}^x\limits f(u)\, du \quad \text{für jedes } x\in\mathbb{R}.\]
\enddefn



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Wahrscheinlichkeitsdichte}

\vspace{1cm}

- Für alle Werte von $x$, in denen $F(x)$ differenzierbar ist, erhält man
\[\frac{d F(x)}{dx} = f(x);\] d.h. die Dichtefunktion ist die erste Ableitung der Verteilungsfunktion.
- Die Wahrscheinlichkeit für ein Ereignis $(x_1<X\leq x_2)$ ist dann \begin{align*}
P(x_1<X\leq x_2) & = F(x_2) - F(x_1) = \int^{x_2}_{-\infty}\limits
f(x)\, dx - \int^{x_1}_{-\infty}\limits f(x)\, dx \\
& = \int^{x_2}_{x_1}\limits f(x)\, dx.
\end{align*}





## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Wahrscheinlichkeitsdichte}

```{r Wahrscheinlichkeitsdichte, echo = F, fig.width=0.6, fig.show="hold", fig.asp=0.95}

grid <- seq(-8, 7, 0.01)
plot(grid, dnorm(grid, 0, 1.5), bty = "l", xlim = c(-8, 5), ylim = c(0, 0.43), las = 1, type = 'l'
     , xaxt = 'n', xlab = '', ylab = '')
axis(1, at = 2, labels = '$x_2$')
polygon(x = c(-8, seq(-8, 2, .1), 2), y = c(0, dnorm(seq(-8, 2, .1),0, 1.5), 0), angle = 45, col = 'cornflowerblue', border = NA)
segments(x0 = 2, y0 = 0, y1 = dnorm(2, 0, 1.5), lty = 2)
legend(x = -7, y = 0.4, legend = '$\\int_{-\\infty}^{x_2} f(x)dx $', fill = 'cornflowerblue', bty = 'n', border = NA)


plot(grid, dnorm(grid, 0, 1.5), bty = "l", xlim = c(-8, 5), ylim = c(0, 0.43), las = 1, type = 'l'
     , xaxt = 'n', xlab = '', ylab = '')
axis(1, at = c(-2, 2), labels = c('$x_1$', '$x_2$'))
polygon(x = c(-8, seq(-8, -2, .1), -2), y = c(0, dnorm(seq(-8, -2, .1),0, 1.5), 0), col = 'darkolivegreen2', border = NA)
segments(x0 = -2, y0 = 0, y1 = dnorm(-2, 0, 1.5), lty = 2)
polygon(x = c(-2, seq(-2, 2, .1), 2), y = c(0, dnorm(seq(-2, 2, .1), 0, 1.5), 0), col = 'red2',  border = NA)
segments(x0 = 2, y0 = 0, y1 = dnorm(2, 0, 1.5), lty = 2)

legend(x = -7, y = 0.44, legend = c('$\\int_{-\\infty}^{x_1} f(x)dx $', '$\\int_{x_1}^{x_2} f(x)dx $'), fill = c('darkolivegreen2', 'firebrick3'), bty = 'n', border = NA, y.intersp = 2)
```




## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Wahrscheinlichkeitsdichte}
\vspace{1cm}

- Für einen einzelnen Wert gilt also im stetigen Fall, dass\[P(X=x_1) =\int^{x_1}_{x_1}\limits f(x)\, dx=0\] für beliebiges $x_1$.
- Das heißt aber nicht, dass diese Ereignisse unmöglich sind! Vielmehr zeigt es, dass $f(x)$ im stetigen Fall nicht die Wahrscheinlichkeit für $X=x$ angibt.
- Daher ist auch möglich, dass für einige Werte $f(x)>1$ gilt.
- Da im stetigen Fall immer gilt $P(X=x)=0$, stimmen folgende Wahrscheinlichkeiten überein:
\begin{align*}
P(x_1\leq X\leq x_2) & = P(x_1<X\leq x_2)=P(x_1\leq X<x_2) \\
& = P(x_1<X<x_2).
\end{align*}


## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Verteilungs- und Dichtefunktion}
\xmpl[Berechnung von Wahrscheinlichkeiten bei stetigen ZV]
Eine stetige Zufallsvariable $X$ hat die Verteilungsfunktion
\begin{equation*}
F(x)=
\begin{cases}
0  &  x \leq 0 \\
\frac{1}{8}x^3  & 0 < x < 2\\
1 & x \geq 2.
\end{cases}
\end{equation*}
Da $F(x)$ über den drei Teilintervallen differenzierbar ist, ist die Dichtefunktion die Ableitung von $F(x)$:
\begin{equation*}
 f(x) =
 \begin{cases}
  \frac{3}{8}x^2 & 0 < x \leq 2 \\
  0 & \text{sonst}.
  \end{cases}
\end{equation*}
\endxmpl



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Verteilungs- und Dichtefunktion - QuizAcademy}
\QuizAcademy{Zufallsvariablen 2}\endQuizAcademy



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Verteilungs- und Dichtefunktion}
\xmpl[Berechnung von Wahrscheinlichkeiten bei stetigen ZV]
$P(1<X\leq\frac{3}{2})$ lässt sich über $F(x)$ und $f(x)$ berechnen:
\[ P\left(1<X\leq\frac{3}{2}\right)=F\left(\frac{3}{2}\right) -F(1)=\frac{27}{64}-\frac{1}{8}=\frac{19}{64};\]
aber auch
\[\int^{\frac{3}{2}}_1\limits \frac{3}{8} x^2 \, dx =\frac{1}{8}x^3 \bigg|^{\frac{3}{2}}_1 =\frac{27}{64}-\frac{1}{8}=\frac{19}{64}.\]
Die folgenden Grafiken zeigen beide Funktionen.
\endxmpl



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Verteilungs- und Dichtefunktion}
Abbildung (a) gibt die Wahrscheinlichkeit als Differenz der beiden Funktionswerte, Abbildung (b) als Flächeninhalt wieder.
\xmpl[*]
```{r, echo = F, fig.show="hold", fig.width=0.49, fig.asp=0.9}
myCdf <- function(x) {
  ifelse(x >= 0 & x <= 2, 0.125 * x^3, 1)
}

myPdf <- function(x) {
  ifelse(x > 0 & x <= 2, .375 * x^2, 0)
}

grid <- seq(0, 2.5, 0.03)
plot(x = grid, y = myCdf(grid), type = "l", bty = "l",
     ylim = c(0, 1.6), xaxt = "n", yaxt = "n", xlab = "$x$", ylab = "$F(x)$")
axis(1, at = c(1, 1.5, 2), labels = c("$1$", "$\\frac{3}{2}$", "$2$"))
axis(2, at = round(myCdf(c(1, 3/2, 2)), 3), labels = c(0.125, 0.422, 1))
segments(x0 = c(1, 3/2), x1 = c(1, 3/2), y0 = 0, y1 = round(myCdf(c(1, 3/2)), 3), lty = 2)
segments(x0 = 0, x1 = c(1, 3/2), y0 = round(myCdf(c(1, 3/2)), 3), lty = 2)
text(labels = "(a)", x = 0.4, y = 1.5, las = 1)

grid <- seq(0, 2, 0.01)
plot(x = grid, y = myPdf(grid), type = "l", bty = "l",
     xlim = c(0, 2.25), ylim = c(0, 1.6), xaxt = "n", yaxt = "n", xlab = "$x$", ylab = "$f(x)$")
axis(1, at = c(1, 1.5, 2), labels = c("$1$", "$\\frac{3}{2}$", "$2$"))
axis(2, at = myPdf(c(1, 3/2, 2)), labels = round(myPdf(c(1, 3/2, 2)), 3))
segments(x0 = 2, y0 = 0, y1 = myPdf(2), lty = 2)
segments(x0 = 0, y0 = myPdf(c(1, 3/2)), x1 = c(1, 3/2), lty = 2)
polygon(x = c(1, seq(1, 3/2, .01), 3/2), y = c(0, myPdf(seq(1, 3/2, .01)), 0), col = "tomato3", border = NA)
text(labels = "(b)", x = 0.5, y = 1.5, las = 1)
```
\endxmpl


## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Verteilungs- und Dichtefunktion in \R}

```{r}
# Verteilungsfunktion definieren
myCdf <- function(x) {
  if(x < 0){                    # Wenn x < 0,
    cdf <- 0                    # dann nimmt cdf den Wert 0 an.
  } else if(x >= 0 & x <= 2){   # Sonst, wenn 0 <= x <= 2,
    cdf <- 0.125*x^3            # dann berechne die cdf mit 0.125*x^3.
  } else {
    cdf <- 1                    # Sonst nimmt cdf den Wert 1 an.
  }
  cdf
}

# Dichtefunktion definieren
myPdf <- function(x) ifelse(x > 0 & x <= 2, .375 * x^2, 0)

myCdf(1.5) - myCdf(1)
integrate(f = myPdf, lower = 1, upper = 1.5)
```





## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Verteilungs- und Dichtefunktion}
\exe
Für die Zufallsvariable $X$ gilt
\[f(x)=\left\{\begin{array}{ll}
\frac{1}{2}x & 0\leq x\leq 2\\[2mm]
0 & \text{sonst}.
\end{array}\right.\]

(a) Zeigen Sie, dass $f(x)$ eine Dichtefunktion ist und stellen Sie diese grafisch dar!
(b) Berechnen Sie die Verteilungsfunktion und stellen Sie diese grafisch dar!
(c) Bestimmen Sie die folgenden Wahrscheinlichkeiten:

\begin{center}
\begin{tabular}{ll}
\blue{(i)} $P(X=1)$ & \blue{(ii)} $P(X\leq 1)$ \\[2mm]
\blue{(iii)} $P(\frac{1}{2}\leq X\leq\frac{3}{2})$   & \blue{(iv)} $P(\frac{1}{2}< X\leq\frac{3}{2})$ 
\end{tabular}
\end{center}
\endexe



## \large Wahrscheinlichkeitsverteilungen von Zufallsvariablen
\framesubtitle{Verteilungs- und Dichtefunktion}
\exe
Die Zufallsvariable $X$ hat die folgende Verteilungsfunktion
\[F(x)=\left\{\begin{array}{ll}
0 & x\leq0\\
\frac{1}{27}(x-3)^3+1 & 0<x<3\\
1 & x\geq3.\end{array}\right.\]
Bestimmen Sie die Dichtefunktion!
\endexe




## Parameter von Verteilungen
\framesubtitle{Analogie zur deskriptiven Statistik}

- Wie in der deskriptiven Statistik lassen sich \hil{Parameter} zur Charakterisierung von jetzt Wahrscheinlichkeitsverteilungen entwickeln.
- Die wichtigsten Maßzahlen zur Beschreibung von Verteilungen sind \hil{Lage- und Streuungsparameter}.
- Die zentrale Stellung des arithmetischen Mittels in der deskriptiven Statistik nimmt in der Wahrscheinlichkeitstheorie der \hil{Erwartungswert} ein.
- Der mit E symbolisierte Erwartungswert soll zunächst an einem Beispiel erläutert werden.




## Parameter von Verteilungen
\framesubtitle{Erwartungswert}
\xmpl[Glücksspiel]
Für einen Einsatz von 2€ darf man einen fairen Würfel einmal werfen. Die Hälfte der geworfenen Augenzahl wird (in €) ausgezahlt.
\par\medskip
Welchen Gewinn kann ein Spieler erwarten?

- Die Auszahlungen transformieren die diskrete Zufallsvariable $X=\{1,2,3,4,5,6\}$ in eine neue Zufallsvariable mit Wertebereich \[\left\{\frac{1}{2},1,\frac{3}{2},2,\frac{5}{2},3\right\}.\]
- Berücksichtigt man die Einsatzkosten von 2€, erhält man als mögliche Gewinne
 \[\left\{-\frac{3}{2},-1,-\frac{1}{2},0,\frac{1}{2},1\right\}.\]

\endxmpl



## Parameter von Verteilungen
\framesubtitle{Erwartungswert}
\xmpl[Glücksspiel]

- Diese Werte sind die Realisationen der neuen Zufallsvariablen \[Y=g(X)=\frac{1}{2} X-2.\]
- Jeder Ausgang $y_i \in Y$ tritt ein mit Wahrscheinlichkeit \[P(Y=y_i)=P(X=x_i)=\frac{1}{6}.\]
- Der zu erwartende Gewinn ergibt sich intuitiv, indem jeder mögliche Ausgang mit seiner Eintrittswahrscheinlichkeit gewichtet wird: \[\E(Y)=\left(-\frac{3}{2}\right)\cdot \frac{1}{6}+\left(-1\right)\cdot\frac{1}{6}+ \left(-\frac{1}{2}\right)\cdot \frac{1}{6} + 0 \cdot \frac{1}{6}+\frac{1}{2}\cdot \frac{1}{6}+ 1 \cdot \frac{1}{6}=-0.25.\]
\vspace{-0.5cm}

\endxmpl



## Parameter von Verteilungen
\framesubtitle{Erwartungswert}
Sei $g(x)$ eine auf $X(\Omega)$ definierte reellwertige Funktion.
\defn[Erwartungswert einer diskreten Zufallsvariable]
Sei $X$ eine \hil{diskrete} Zufallsvariable mit der Wahrscheinlichkeitsfunktion $f(x_i)$. \linebreak Falls $\sum |g(x_i)|f(x_i)<\infty$, ist der \hil{Erwartungswert} von $g(X)$ gegeben als \[ \E[g(X)]=\sum^m_{i=1}g(x_i)f(x_i).\]
\enddefn
\defn[Erwartungswert einer stetigen Zufallsvariable]
Sei $X$ eine \hil{stetige} Zufallsvariable mit Dichtefunktion $f(x)$. \linebreak Falls $\int^\infty_{-\infty}|g(x)|f(x)\, dx<\infty$, so ist der  \hil{Erwartungswert} von $g(X)$ gegeben als \[ \E[g(X)]=\int^\infty_{-\infty}\limits g(x)f(x)\, dx.\]
\enddefn



## Parameter von Verteilungen
\framesubtitle{Erwartungswert}

- Das Integral in der vorangegangenen Definition bezieht sich auf den größtmöglichen \hil{Wertebereich} von $X$.
- Falls die Dichtefunktion nur über einem Intervall $a\leq X\leq b$ Werte größer als null annimmt, erfolgt die Integration über diesem Intervall.
- Für $g(X)=X^{\alpha}$, $\alpha \in \mathbb{N}$ heißt der Erwartungswert \hil{Anfangsmoment} \hil{der Ordnung {\boldmath{$\alpha$}}}:
\begin{equation}\label{Anfangsmomente}
\E(X^\alpha)=
\begin{cases}
\sum_{i=1}^m x_i^\alpha f(x_i) &  X\text{: diskret} \\[1ex]
\int_{-\infty}^\infty x^\alpha f(x)\, dx & X\text{: stetig}.
\end{cases}
\end{equation}



## Parameter von Verteilungen
\framesubtitle{Erwartungswert}

\defn[Erwartungswert einer Zufallsvariablen ($\alpha=1$)]
 Der Erwartungswert einer Zufallsvariablen $X$ ist definiert als
 \begin{equation*}
  \E(X)=\mu=
  \begin{cases}
   \sum_{i=1}^m  x_i f(x_i) & X\text{: diskret} \\[1ex]
   \int_{-\infty}^\infty x f(x) \, dx &  X\text{: stetig}.
  \end{cases}
 \end{equation*}
\enddefn

\xmpl[Erwartungswert - diskreter Fall]
Der Erwartungswert beim Werfen eines fairen Würfels beträgt
\[\E(X)=\sum^m_{i=1} x_i f(x_i)=\sum^m_{i=1} x_ip_i=1\cdot\frac{1}{6}+2\cdot\frac{1}{6}+\ldots+6\cdot\frac{1}{6}=3.5. \label{EWWuerfel}\]
Man sieht an diesem Ergebnis, dass der Erwartungswert diskreter Zufallsvariablen keine Realisation von $X$ sein muss.
\endxmpl



## Parameter von Verteilungen
\framesubtitle{Erwartungswert}
\xmpl[Roulette]
Beim Roulette kann man auf Zahlen $(0,1,2,\ldots,36)$ oder Farben (rot und schwarz) setzen. Wird die Farbe richtig vorhergesagt, ist der Gewinn gleich der Höhe des Einsatzes. Jeweils 18 Zahlen sind rot sowie schwarz (die Null hat keine Farbe).\par
Setzt man also 10€ auf rot, beträgt der erwartete Gewinn
\[\E(X)=10\cdot\frac{18}{37}+\left(-10\right)\cdot\frac{19}{37}=-0.2703.\]
\endxmpl


## Parameter von Verteilungen
\framesubtitle{Erwartungswert}
\xmpl[Erwartungswert - stetiger Fall]
Als Erwartungswert der stetigen Zufallsvariablen mit $f(x)=\frac{3}{8}x^2,$ $0<x\leq 2$ erhält man
\begin{align} 
\E(X)=\int^2_0\limits xf(x)\, dx = \int^2_0\limits x\frac{3}{8}x^2\, dx= \int^2_0\limits \frac{3}{8}x^3\, dx=\frac{3}{32}x^4 \Bigg|^2_0 =\frac{3}{2}.
\label{stetExpBsp}\end{align}
\endxmpl



## Parameter von Verteilungen
\framesubtitle{Rechenregeln für den Erwartungswert}

- Für den \hil{Erwartungswertoperator} $\E$ gelten unabhängig davon, ob $X$ diskret oder stetig ist, folgende Regeln für alle $a,b \in \mathbb{R}$:
\begin{eqnarray}
  & \text{\blue{(a)}}& \E(a) = a, \label{EKst} \\
  & \text{\blue{(b)}}& \E(a+bX)=a+b\E(X), \label{ElinTrafo} \\
  & \text{\blue{(c)}}& \E\left[\sum^n_{j=1}\limits c_jg_j(X)\right] = \sum^n_{j=1}\limits
  c_j\E[g_j(X)] = c_1 \E[g_1(X)]+\ldots+c_n \E[g_n(X)], \notag
  \end{eqnarray}
falls $g_j$ eine stetige Funktion für alle $j=1,\ldots,n$ ist.
Für deren Herleitungen siehe Buch S.\ 69 ff.




## Parameter von Verteilungen
\framesubtitle{arithmetisches Mittel und Erwartungswert}

- Der Erwartungswert einer diskreten Zufallsvariablen stimmt formal mit dem \hil{arithmetischen Mittel} \[\bar{x}=\sum^m_{i=1}\limits x_ih_i\]
 überein, wobei $h_i$ die relative Häufigkeit der $i$-ten Merkmalsausprägung im Datensatz bezeichnet.
- Trotz der formalen Übereinstimmung besteht zwischen beiden Maßzahlen ein erheblicher Unterschied, dessen Beachtung für ein genaues Verständnis beider Konzepte bedeutsam ist.
- Der Erwartungswert $\E(X)$ charakterisiert die Verteilung einer Zufallsvariablen gemäß der statistischen Wahrscheinlichkeitsdefinition.
- Dieser Wert bedarf keiner tatsächlichen Durchführungen und ist von diesen unabhängig.




## Parameter von Verteilungen
\framesubtitle{arithmetisches Mittel und Erwartungswert}

- Das arithmetische Mittel hingegen kennzeichnet den durchschnittlichen Wert eines Datensatzes, d.h. es müssen Beobachtungen vorliegen.
- Wird ein Experiment $n$-mal durchgeführt, so liegen die \hil{Realisationen} $x_1,\ldots,x_n$ von $X$ vor, für die das arithmetische Mittel \[\bar{x}=\dfrac{1}{n}\sum\limits^n_{j=1}x_j\] berechnet werden kann.
- Das arithmetische Mittel kennzeichnet jetzt den Datensatz, nicht jedoch $\E(X)$.
- Erhöht man $n$, wird sich $\bar{x}$ $\E(X)$ annähern. Dazu später mehr. 




## Parameter von Verteilungen
\framesubtitle{Varianz und Standardabweichung}

- \hil{Varianz} bzw. \hil{Standardabweichung} quantifizieren die \hil{Streuung} einer Verteilung.
- Auch die Varianz ergibt sich für eine spezielle Funktion $g(X)$ aus den Definitionen für den Erwartungswert. Für $g(X)=(X-\mu)^\alpha,$ $\alpha\in \mathbb{N}$, heißt der Erwartungswert
\begin{equation*}
  \E[(X-\mu)^\alpha] =
  \begin{cases}
   \sum_{i=1}^m (x_i-\mu)^\alpha f(x_i) & X\text{: diskret} \\[1ex]
   \int_{-\infty}^\infty (x-\mu)^\alpha f(x) \, dx & X\text{: stetig}
  \end{cases}
 \end{equation*}
\hil{Zentralmoment der Ordnung {\boldmath{$\alpha$}}}.
- Für $\alpha=2$ resultiert die Varianz als Zentralmoment zweiter Ordnung: \[ \var(X) = \E[(X-\mu)^2] = \sigma^2.\]




## Parameter von Verteilungen
\framesubtitle{Varianz und Standardabweichung}
\vspace{1cm}
\defn[Standardabweichung]
Die \hil{Standardabweichung} $\sigma$ ist definiert als $\sigma =\sqrt{\sigma^2}$.
\enddefn
\defn[Varianz: Spezieller Verschiebungssatz]
Die Varianz kann durch Anfangsmomente dargestellt werden:
\[ \sigma^2=\E[(X-\mu)^2]=\E(X^2)-2\mu
\E(X)+\E(\mu^2).\]
Die rechte Seite der Gleichung lässt sich zusammenfassen zu
\begin{equation}\label{VerschSatzVar}
\sigma^2=\E(X^2)-[\E(X)]^2=\E(X^2)-\mu^2.
\end{equation}
$\E(X^2)$ ist das Anfangsmoment zweiter Ordnung, $[\E(X)]^2$ das Quadrat des Anfangsmoments erster Ordnung.
\enddefn



## Parameter von Verteilungen
\framesubtitle{Varianz und Standardabweichung}
\xmpl[Varianz - diskreter Fall]
Die Varianz beim Werfen eines fairen Würfels beträgt
\[\sigma^2=\E(X^2)-[\E(X)]^2=\sum^m_{i=1}\limits x_i^2f(x_i)-\mu^2=1^2\cdot\frac{1}{6}+\ldots+6^2\cdot\frac{1}{6}-3.5^2=2.9167 \label{VarWuerfel}.\]
Standardabweichung: $\sigma=\sqrt{\sigma^2}=1.7078$.
\endxmpl
\xmpl[Roulette]
Setzt man also 10€ auf rot, beträgt die Varianz
\[\sigma^2=\E(X^2)-[\E(X)]^2=10^2\cdot\frac{18}{37}+\left(-10\right)^2\cdot\frac{19}{37}-\left(-0.2703\right)^2=99.927.\]
Standardabweichung: $\sigma=\sqrt{\sigma^2}=9.9963$.
\endxmpl



## Parameter von Verteilungen
\framesubtitle{Varianz und Standardabweichung}
\xmpl[Varianz - stetiger Fall]
Als Varianz der stetigen Zufallsvariablen mit $f(x)=\frac{3}{8}x^2,$ $0<x\leq 2$ erhält man
\begin{eqnarray}\notag
\sigma^2=\E(X^2)-[\E(X)]^2&=&\int^2_0\limits x^2f(x)\, dx-\left[\int^2_0\limits xf(x)\, dx\right]^2\\
&=& \int^2_0\limits x^2\frac{3}{8}x^2\, dx-\left[\int^2_0\limits x\frac{3}{8}x^2\, dx\right]^2\\
& \stackrel{\eqref{stetExpBsp}}{=} & \int^2_0\limits\frac{3}{8}x^4\, dx-\left(\frac{3}{2}\right)^2\\
&=&\frac{3}{40}x^5 \Bigg|^2_0-\frac{9}{4} =\frac{12}{5}-\frac{9}{4}=\frac{3}{20}.\label{stetVarBsp}
\end{eqnarray}
\endxmpl



## Parameter von Verteilungen
\framesubtitle{Rechenregeln für die Varianz}
Für die Varianz gelten die folgenden Regeln:
\begin{eqnarray}
  \var(a) & = & 0 \label{varKst}\\
  \var(a+bX) & = & b^2 \var(X)\label{varlinTrafo}
  \end{eqnarray}
für Konstanten $a,b \in \mathbb{R}$.

Ergebnis \eqref{varlinTrafo} weisen wir in \eqref{VarLinKomb3} nach.

## Parameter von Verteilungen
\framesubtitle{Erwartungswert und Varianz - QuizAcademy}
\QuizAcademy{Zufallsvariablen 3}\endQuizAcademy



## Parameter von Verteilungen
\framesubtitle{Erwartungswert und Varianz}
\exe
Berechnen Sie \blue{(a)} den Erwartungswert und \blue{(b)} die Varianz bzw. die Standardabweichung für die diskrete Zufallsvariable $X$: *Anzahl der Autoverkäufe pro Tag*
\begin{center}
\begin{tabular}{|c|cccccc|}\hline
$X$   & 0 & 1 & 2 & 3 & 4 & 5\\\hline
$f(x)$& 0.18 & 0.39 & 0.24 & 0.14 & 0.04 & 0.01\\
$F(x)$& 0.18 & 0.57 & 0.81 & 0.95 & 0.99 & 1\\\hline
\end{tabular}
\end{center}
\endexe
\exe
Berechnen Sie \blue{(a)} den Erwartungswert und \blue{(b)} die Varianz bzw. die Standardabweichung für die stetige Variable $X$ mit der Dichtefunktion
\[f(x)=\left\{\begin{array}{ll}-\frac{1}{72}x^2+\frac{1}{12}x+\frac{1}{12} & 0\leq x\leq 6\\ 
0 & \text{sonst}.\end{array}\right.\]
\endexe



## Parameter von Verteilungen
\framesubtitle{Erwartungswert und Varianz}
\exe
Für die diskrete Zufallsvariable $X$ ist folgende Wahrscheinlichkeitsfunktion gegeben
\[f(x)=\left\{\begin{array}{ll} 0.1& x=-1\\
                               0.3& x=0\\
                               0.6& x=1\\
                               0& \text{sonst}.\end{array}\right.\]

(i) Berechnen Sie \blue{(a)} den Erwartungswert und \blue{(b)} die Varianz bzw. die Standardabweichung für $X$!
(ii) Berechnen Sie \blue{(a)} den Erwartungswert und \blue{(b)} die Varianz bzw. die Standardabweichung für $Y=2X^2+1$!

\endexe





## Linearkombination
\framesubtitle{Drei Spezifikationen}
Mehrere Zufallsvariablen, die entweder alle diskret oder alle stetig sind, können zu einer neuen Zufallsvariablen verknüpft werden.

\defn[Linearkombination]
Gegeben seien die Zufallsvariablen $X_1, X_2,\ldots,X_n$ und Konstanten \[\lambda_1, \lambda_2,\ldots,\lambda_n.\]
  Die Zufallsvariable
  \[Y=\lambda_1X_1+\lambda_2X_2+\ldots+\lambda_nX_n =
  \sum\limits^n_{j=1}\lambda_jX_j\] heißt \hil{Linearkombination} der Zufallsvariablen $X_j$, $j=1,\ldots,n$.
\enddefn



## Linearkombination
\framesubtitle{Drei Spezifikationen}
Besonders wichtige Linearkombinationen sind die folgenden:
\medskip

(1) Sind alle $\lambda_j=1$, erhält man die \hil{Summe} der $n$ Zufallsvariablen $X_j$,
\begin{equation*}
    \tilde{S}=X_1+X_2+\ldots+X_n = \sum\limits^n_{j=1} X_j.
\end{equation*}
(2) Für $\lambda_j=1/n$ resultiert das \hil{arithmetische Mittel} der $X_j$,
\begin{equation*}
 \bar{X}=\frac{1}{n}X_1+\frac{1}{n}X_2+\ldots+\frac{1}{n}X_n=\frac{1}{n}
    \sum\limits^n_{j=1}X_j = \frac{1}{n}\tilde{S}.
    \end{equation*}
(3) $\lambda_j=0$ für $j=3,\ldots,n$ und $X_2=1$ liefert eine \hil{Lineartransformation}
 \begin{equation*}
      Y=\lambda_1X_1+\lambda_2,
      \end{equation*}
    da $Y$ eine lineare Funktion von $X_1$ ist.




## Linearkombination
\framesubtitle{Erwartungswert und Varianz - Rechenregeln}

- Bei der Varianzberechnung muss unterschieden werden, ob die Zufallsvariablen $X_j$ paarweise stochastisch unabhängig sind oder nicht.
- Es sei hier zunächst \hil{paarweise stochastische Unabhängigkeit} angenommen.


## Linearkombination
\framesubtitle{Erwartungswert und Varianz}
Fall \blue{(1)}:
\begin{align}\notag
\E(\tilde{S})& =\E(X_1+\ldots+X_n)=\E(X_1)+\ldots+\E(X_n) \\
&=\sum^n_{j=1} \E(X_j) =\sum^n_{j=1}\mu_j=:\mu_{\tilde{S}}.\label{EWSumme}
\end{align}
Die vorletzte Umformung zeigt, dass die Operatoren $\E$ und $\sum$ in ihrer Anwendung vertauscht werden können.
\begin{align}\notag
\var(\tilde{S}) & =\var(X_1+\ldots+X_n)=\sigma^2_1+\ldots+\sigma^2_n \\
& =\sum^n_{j=1}\sigma^2_j=:\sigma^2_{\tilde{S}}. \label{varSumme}
\end{align}
Diese Berechnung heißt \hil{Additionssatz für Varianzen (paarweise) stochastisch unabhängiger Zufallsvariablen}.



## Linearkombination
\framesubtitle{Erwartungswert und Varianz}
Fall \blue{(2)}:
\begin{align} \label{EW}
\E(\bar{X})& = \E\left(\frac{1}{n}\sum^n_{j=1} X_j\right) \stackrel{\eqref{ElinTrafo}}{=} \frac{1}{n}\E(\tilde{S}) = \frac{1}{n}\sum^n_{j=1}\limits \mu_j =: \mu_{\bar{X}}
\end{align}
sowie
\begin{align*} 
\var(\bar{X}) & = \var\left(\frac{1}{n}\tilde{S}\right) \\
& \stackrel{\eqref{varlinTrafo}}{=} \frac{1}{n^2} \var(\tilde{S}) \\
& \stackrel{\eqref{varSumme}}{=}\frac{1}{n^2} \sum_{j=1}^n \sigma_j^2 \\
& =: \sigma^2_{\bar{X}}.
\end{align*}




## Linearkombination
\framesubtitle{Erwartungswert und Varianz}
Fall \blue{(3)}:
\begin{align}
\E(Y) & = \E(\lambda_1 X_1+\lambda_2) \stackrel{\eqref{ElinTrafo}}{=} \lambda_1 \E(X_1) + \lambda_2 = \lambda_1 \mu_1 + \lambda_2 = \mu_Y \label{EWlinTrafo}
\end{align}
sowie
\begin{align}
\var(Y) & \stackrel{ \eqref{VerschSatzVar}}{=} \E[(Y-\mu_Y)^2] \notag \\
& \stackrel{\eqref{EWlinTrafo}}{=} \E[(\lambda_1 X_1 + \lambda_2 - \lambda_1 \mu_1 - \lambda_2)^2] \notag \\
& \stackrel{}{=} \E\{[\lambda_1(X_1 - \mu_1)]^2\} \notag \\
& \stackrel{ \eqref{ElinTrafo}}{=} \lambda_1^2 \E[(X_1 - \mu_1)^2] \notag \\
& \stackrel{}{=} \lambda_1^2 \var(X_1) = \lambda_1^2 \sigma_1^2 = \sigma_Y^2. \label{VarLinKomb3}
\end{align}

Damit ist \eqref{varlinTrafo} gezeigt.

## Linearkombination
\framesubtitle{Erwartungswert und Varianz}
- Für die Lineartransformation
\begin{equation}\label{stdRV}
Y=\lambda_1X_1+\lambda_2\quad\text{ mit }\quad\lambda_1=\frac{1}{\sigma_1}, \lambda_2=-\frac{\mu_1}{\sigma_1}
\end{equation}
folgt
\[\E(Y)=\frac{1}{\sigma_1} \mu_1-\frac{\mu_1}{\sigma_1}=0\] und \[\var(Y)=\frac{1}{\sigma^2_1}\sigma^2_1=1.\]
$Y$ heißt dann \hil{standardisierte} Zufallsvariable.
- Alle Zufallsvariablen mit endlichem Erwartungswert und einer endlichen Varianz größer als null lassen sich so standardisieren.
- Haben die Zufallsvariablen $X_j$, $j=1,2,\ldots,n$ im Fall \blue{(1)} alle dieselbe Verteilung und folgt diese dann auch für die Verteilung ihrer Summe $\tilde{S}=\sum^n_{j=1} X_j$, liegt die sogenannte \hil{Reproduktionseigenschaft} bzw. \hil{Reproduktivität} vor.




## Linearkombination
\framesubtitle{Erwartungswert und Varianz}
\exe
Ein Investor legt ein Viertel seines Vermögens in einem Immobilienfonds (IF) und den Rest in einem Aktienfonds (AF) an. Für diese gilt

\begin{center}
\begin{tabular}{ll}
$\E(\text{IF})=0.28$ & $\sigma_{\text{IF}}=0.2$ \\
$\E(\text{AF})=0.12$ & $\sigma_{\text{AF}}=0.06$.
\end{tabular}
\end{center}

(a) Wie groß ist die erwartete Gesamtrendite, wenn die Renditen beider Investitionsprojekte unabhängig voneinander sind?
(b) Wie groß ist die Varianz der Gesamtrendite, wenn die Renditen beider Investitionsprojekte unabhängig voneinander sind?

\endexe



## Tschebyscheff'sche Ungleichung
\framesubtitle{Abschätzung von Wahrscheinlichkeiten}

- Sind nur $\mu$ und $\sigma^2<\infty$ von $X$ bekannt, nicht jedoch ihre konkrete Verteilungsfunktion, lassen sich die Wahrscheinlichkeiten für komplementäre Ereignisse der Art
\[\{|X-\mu|<c\}=\{\mu-c<X<\mu+c\}\]
und
\[\{|X-\mu|\geq c\}=\{X\leq\mu-c,X\geq\mu+c\}\]
für jede beliebige Konstante $c>0$ \hil{abschätzen}.
- So gewinnt man zusätzlich zu $\mu$ und $\sigma^2$ weitere Informationen über Lage und Streuung der Verteilung von $X$.
- Die Abschätzung geschieht mit der \hil{Tschebyscheff'schen Ungleichung}, die in zwei äquivalenten Versionen angegeben werden kann.




## Tschebyscheff'sche Ungleichung
\framesubtitle{Abschätzung von Wahrscheinlichkeiten}

- Es lässt sich zeigen, dass für jedes $X$ mit $\sigma^2<\infty$ gilt
\begin{equation}\label{Tschebyscheff1}
P(|X-\mu|\geq c)\leq\frac{\sigma^2}{c^2} \text{ bzw. }P(|X-\mu|< c)>1-\frac{\sigma^2}{c^2}
\end{equation}
für alle $c>0$.
- Setzt man $c=k\sigma$, folgt die \hil{Tschebyscheff'sche Ungleichung}:
\begin{equation}\label{Tschebyscheff2}
 \text{\blue{(a)}}\;P(|X-\mu|\geq k\sigma)\leq \frac{1}{k^2}  \quad \text{bzw. \blue{(b)} }
 P(|X-\mu|< k\sigma)>1-\frac{1}{k^2}\,.
\end{equation}
- Nur für $c>\sigma$ bzw. $k>1$ liegen informative Abschätzungen vor.
- Die Abschätzung der Wahrscheinlichkeit liefert eine untere bzw. obere Grenze für die exakte Wahrscheinlichkeit.




## Tschebyscheff'sche Ungleichung
\framesubtitle{Abschätzung von Wahrscheinlichkeiten}
Der Approximationsfehler kann jedoch beträchtlich sein.
\xmpl[]
Die stetige Zufallsvariable $X$ mit der Dichtefunktion $f(x)=\frac{3}{8} x^2$, $0<x\leq2$, sonst null, hat einen Erwartungswert von $\mu \stackrel{\eqref{stetExpBsp}}{=}  \frac{3}{2}$ und eine Varianz von $\sigma^2 \stackrel{\eqref{stetVarBsp}}{=} \frac{3}{20}$ bzw. $\sigma\approx 0.3873$.\mps
Die exakte Wahrscheinlichkeit für das Ereignis $1\leq X\leq2$
beträgt
\[P(1\leq X\leq 2)=\int^2_1\limits\frac{3}{8} x^2\, dx =F(2)-F(1)=\frac{7}{8}= 0.875.\]
\endxmpl



## Tschebyscheff'sche Ungleichung
\framesubtitle{Abschätzung von Wahrscheinlichkeiten}
\xmpl[*]
Zur Abschätzung müssen $c$ und $k$ bestimmt werden. Aus
\[1\leq X\leq 2\Leftrightarrow\left|X-\frac{3}{2}\right|\leq\frac{1}{2},\]
folgt $c=\frac{1}{2}$. $k$ ergibt sich wegen $k\sigma =c$ durch Lösen der Gleichung
\[0.3873 k = \frac{1}{2}\]
als $k=1.291$.\mps
Damit kann die Abschätzung berechnet werden:
\[P\left(\left|X-\frac{3}{2}\right|\leq\frac{1}{2}\right)> 1-\frac{1}{(1.291)^2}\approx 0.4.\]
\endxmpl



## Tschebyscheff'sche Ungleichung
\framesubtitle{Abschätzung von Wahrscheinlichkeiten}
\exe
Für den *Fernsehkonsum pro Tag in den USA* ist bekannt: $\mu=4.4$h und $\sigma=2.1$h.\mps
Wie groß ist die Wahrscheinlichkeit, dass eine Person zwischen 0.2 und 8.6h fernsieht?
\endexe





<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %% Kapitel 3 -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!--  % Kürzungen gegenüber kompletter Fassung SoSe 2014: -->
<!--  % -->
<!--  % - Rechteckverteilung -->
<!--  % - F-Verteilung -->
<!--  % - Gleichverteilung -->
<!--  % - Laplaceverteilung -->
<!--  % - Bin: Nachweis Dichte hat Summe 1 -->
<!--  % - NegBin: Tumorbeispiel -->
<!--  % - Dichten chi^2 und t-Vtlg -->
<!--  % - FG chi^2 knapper -->
<!--  % - insgesamt weniger zu chi^2 -->
<!--  % - NV Reproduktivität gekürzt, redundante Herleitung Vtlg Xbar weg (kommt in Stichproben noch) -->
<!--  % - Einleitung NV gekürzt -->
<!--  % - Schreibweisen NegBin-Dichte -->
<!--  % - Poisson-Druckfehlerbsp weg -->
<!--  % - Poisson-Prozess-Bed. -->
<!--  % - Herleitung VF geometrische Vtlg -->
<!--  % - Zweipunktverteilung -->
<!--  % - Anhänge -->
<!--  % -->

# Ausgewählte theoretische Verteilungen

## Wahrscheinlichkeitsverteilungen

- \hil{Zufallsvariablen} und \hil{Wahrscheinlichkeitsverteilungen} abstrahieren von tatsächlichen, in der Realität ablaufenden Zufallsvorgängen.
- Eine wesentliche Aufgabe der Statistik liegt darin, aus \hil{theoretischen Verteilungen} die zu finden, die einen realen Zufallsvorgang am besten erfasst.
- Da viele ökonomische Zufallsvorgänge auf gleichen Voraussetzungen basieren, reichen einige ausgewählte Verteilungsfunktionen zur Lösung der meisten praktischen Probleme aus.



## Wahrscheinlichkeitsverteilungen
Bei diskreten Zufallsvariablen lassen sich theoretische Verteilungsfunktionen oft über ein \hil{Bernoulli-Experiment} entwickeln. Dies liegt vor, wenn ein Zufallsvorgang

1. nur in zwei Ausgänge $A$ und $\bar{A}$ mündet, deren
2. Wahrscheinlichkeiten über Durchführungen konstant bleiben und
3. die einzelnen Durchführungen unabhängig voneinander sind.

- Obwohl die dritte Bedingung suggeriert, dass bei einem Bernoulli-Experiment mehrere Durchführungen vorliegen müssen, bezeichnet man so auch die einmalige Durchführung.
- Bei Wiederholungen gibt man deren Anzahl an und nennt sie auch \hil{Bernoulli-Kette der Länge {\boldmath{$n$}}}.


## Wahrscheinlichkeitsverteilungen

- Die Ableitung theoretischer stetiger Verteilungen aus einfachen Annahmen eines Zufallsexperiments wie im diskreten Fall gelingt nur selten.
- Jedoch lassen sich aus diskreten Verteilungen nach Grenzübergang für die Anzahl der Durchführungen \hil{stetige Verteilungen} gewinnen.
- Auch eignen sich in vielen empirischen Situationen bestimmte theoretische stetige Verteilungen als \hil{Approximation}.


## Bernoulli-Verteilung

- Wenn $X$ die Werte $x_1=1$ und $x_2=0$ annimmt, liegt eine \hil{Nulleins-} oder \hil{Bernoulli-Verteilung} vor.
- Die \hil{Bernoulliverteilung} ist der wichtigste Spezialfall der Zweipunktverteilung (vgl.\ Buch auf S.\ 81).
- Die letzte Bezeichnung unterstreicht, dass $X$ auf einem Bernoulli-Experiment basiert. Die Wahrscheinlichkeitsfunktion ist \begin{eqnarray*}
 f(x)&=&\begin{cases}
  p & x=1 \\
  1-p & x=0 \\
  0& \text{sonst},
 \end{cases}\\
 &=&p^x(1-p)^{1-x} \ \ \text{für} \ \ x=0, 1, \ \ \text{sonst null}.
\end{eqnarray*}

## Bernoulli-Verteilung
\framesubtitle{Erwartungswert und Varianz}

Erwartungswert und Varianz lauten jetzt
\[ \mu=\sum\limits^2_{i=1}x_ip_i=1\cdot p+0\cdot(1-p)=p\]
sowie
\begin{align}
 \sigma^2 & \stackrel{\eqref{VerschSatzVar}}{=} \E(X^2) - [\E(X)]^2 \notag \\
 &\stackrel{\eqref{Anfangsmomente}}{=} \sum_{i=1}^2 x_i^2p_i - p^2 \notag \\
 & = 1^2p + 0^2(1-p) - p^2 \notag \\ & = p-p^2 \notag \\& = p(1-p). \label{BernouVar}
\end{align}

## Bernoulli-Verteilung
\framesubtitle{in \R}

```{r Bernoulli}
# Stichprobe der Größe 1
rbinom(n = 1, size = 1, prob = 0.5)
# Stichprobe der Größe 100000
x <- rbinom(n = 100000, size = 1, prob = 0.5)
mean(x) # Mittelwert, nicht Erwartungswert!
var(x)
```

## Gleichverteilung
Siehe Buch, S.\nbs 82.

## Binomialverteilung
Viele praktische Zufallsvorgänge besitzen die Struktur eines Bernoulli-Experiments, d.h. sie münden in nur zwei Ausgänge $A$ und $\bar{A}$ und sind beliebig oft unter (mehr oder weniger) gleichen Bedingungen wiederholbar. Beispiele:

(a) Glücksspiel mit dem Ausgang $A$ als Gewinn,
(b) Geburten, wobei $A$ die Geburt eines Mädchens anzeigt,
(c) Überprüfung einer Anzeigekampagne mit $A$: Kenntnis der Aktion,
(d) Qualitätskontrolle bei der Serienproduktion mit $A$: *defektes Produkt*.

## Binomialverteilung

- Diese Vorgänge können als *Ziehen mit Zurücklegen aus einer Urne* idealisiert werden, wobei die Kugeln gemäß $A$ oder $\bar{A}$ unterteilt sind.
- Die dafür definierte Zufallsvariable $X_j$ ist dann pro Durchführung $j,$ $j=1,\ldots,n$, bernoulliverteilt.
 - Hierbei bildet $X_j$ $A$ in die 1 und das Komplementärereignis $\bar{A}$ in die 0 ab:
\[X_j(\Omega)=\{0,1\}\quad\text{mit}\quad X_j(A)=1\quad\text{und}\quad X_j(\bar{A})=0.\]
 - Wegen der beliebigen Wiederholbarkeit gilt für jedes $j$
\[ P(X_j=0)= 1-p=q \quad \text{ und } \quad P(X_j=1)=p. \]


## Binomialverteilung

- Bei $n$-maliger Durchführung eines Bernoulli-Experiments stellt $X=X_1+X_2+\ldots + X_n$ die Anzahl der Eintritte von $A$ in einer \hil{Bernoulli-Kette} der Länge $n$ dar.
- Der Wertebereich für $X$ entspricht den möglichen Summenwerten:
\[X=\{x|x=0,1,2,\ldots,n\}.\]
- Das Ereignis $X=x$ tritt ein, wenn $x$ der Zufallsvariablen $X_1,\ldots,X_n$ eins und die übrigen $n-x$ Variablen null sind, z.B.\nbs die Folge
\[ \underset{x\text{-mal}}{\underbrace{1\: 1 \ldots 1}}\; \underset{(n-x)\text{-mal}}{\underbrace{0\: 0 \ldots 0}} \]

## Binomialverteilung
\framesubtitle{Wahrscheinlichkeitsfunktion}
\vspace{0.5cm}

- Da die Durchführungen und damit die $X_j$ unabhängig sind, ist die Wahrscheinlichkeit für das Eintreten dieser Kette
 \[p^x(1-p)^{n-x}.\]
- Das Ereignis $X=x$ tritt aber auch bei jeder Permutation dieser Kette ein, also insgesamt $\binom{n}{x}$-mal (s.\nbs auch \eqref{BinKoeff}).
- Die Wahrscheinlichkeitsfunktion für $X$ lautet daher
 \begin{equation}
  f(x)=
  \begin{cases}
   \dbinom{n}{x}p^x(1-p)^{n-x}& x=0,1,\ldots,n \label{DichteBinomial}\\
   0& \text{sonst}.\notag
  \end{cases}
 \end{equation}
- Eine solche Zufallsvariable heißt \hil{binomialverteilt}, symbolisiert als $B(n,p)$.
- Die Binomialverteilung hängt also von $n$ und $p$ ab. Hier ein Video zur Geschichte der Binomialverteilung. 

## Binomialverteilung
\framesubtitle{Verteilungsfunktion}
Die Verteilungsfunktion \eqref{VFunktion}, $F(x)$, ist
\begin{equation*}
 F(x)=
 \begin{cases}
  0 & x < 0 \\
  \sum\limits_{k=0}^{\lfloor x\rfloor} \dbinom{n}{k} p^k(1-p)^{n-k} & 0 \leq x < n,\quad k=0,1,\ldots,\lfloor x\rfloor \\
  1 & x \geq n.
 \end{cases}
\end{equation*}
Hier stellt $\lfloor x\rfloor$ den ganzzahligen Teil von $x$ dar, z.B. $\lfloor 3.81\rfloor=\text{int}(3.81)=3$.


## Binomialverteilung
\framesubtitle{Wahrscheinlichkeitsfunktion}

- Erst die Festlegung von $n$ und $p$ spezifiziert eine konkrete Verteilung.
- Für $n=1$ resultiert als Spezialfall die Zweipunktverteilung.
- Die nachfolgende Abbildung zeigt den Effekt, den $p$ bei konstantem $n$ (hier $n=10$) auf $f(x)$ ausübt.
  - Für $0<p<0.5$ ist $f(x)$ linkssteil.
  - Für $0.5<p<1$ ist $f(x)$ rechtssteil.
  - Für $p=0.5$ ist $f(x)$ symmetrisch.


## Binomialverteilung
\framesubtitle{Variation von $p$ bei Konstanz von $n$}


```{r BinomialVtlg1, echo = F, fig.width=0.49, fig.show="hold"}
# layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))

plotBinom <- function(size, prob) {
  plot(x=0:size, y=dbinom(0:size, size=size, prob=prob),
       xlab="", ylab="", ylim=c(0, .35), col="coral3",
       xlim = c(0,size))
  text(x = size/2, y = 0.3, labels = paste0("$n = ", size, "$"))
  segments(x0=0:size, y0=0, y1=dbinom(0:size, size=size, prob=prob),
           lty=3, col="tomato3", lwd = 2)
}

plotBinom(size = 10, prob = 0.2)
plotBinom(size = 10, prob = 0.8)
plotBinom(size = 10, prob = 0.5)
```

## Binomialverteilung
\framesubtitle{Wahrscheinlichkeitsfunktion}

- Den Effekt von $n$ bei konstantem $p$ verdeutlicht die folgende Abbildung.
- In allen vier Fällen ist $p=0.2$, die Anzahl der Durchführungen ist $n=10$, $n=30$, $n=50$ und $n=100$.
- Die Symmetrie wird mit zunehmendem $n$ ausgeprägter.

## Binomialverteilung
\framesubtitle{Wahrscheinlichkeitsfunktion}

```{r BinomialVtlg2, echo = F, fig.show="hold", fig.width=0.49}
plotBinom(prob = 0.2,  size = 10)
plotBinom(prob = 0.2,  size = 30)
plotBinom(prob = 0.2,  size = 50)
plotBinom(prob = 0.2,  size = 100)
```

## Binomialverteilung
\framesubtitle{Verteilungsfunktion für $p=0.2$ und $n=10$}
$F(x)$ macht bei $x=2$, wo $f(x)$ ein Maximum besitzt, den größten Sprung.


```{r BinomialVtlg3, echo=F}
plot(x = 0:6, y = pbinom(0:6, size = 10, prob = 0.2), col = 'steelblue',
     xlim = c(0, 11), ylim = c(0,1.2), yaxt = 'n', xlab = '', ylab = '')
axis(2, at = seq(0,1,0.2))
segments(x0 = 0:6, x1 = c(1:6, 10), y0 = pbinom(0:6, size = 10, prob = 0.2), col = "tomato3")
segments(x0 = 0:6, y0 = 0, y1 = pbinom(0:6, size = 10, prob = 0.2), lty = 3, lwd = 2)
points(x = 0:6, y = pbinom(0:6, size = 10, prob = 0.2), col = "tomato3")
mtext(text = c("$x$", "$F(x)$"), side = 1:2, at = c(11, 1.2), line = c(0.25, 0.5))
```

## Binomialverteilung
\framesubtitle{Erwartungswert und Varianz}
\vspace{1cm}
- Da $X$ als $X=\sum^n_{j=1} X_j$ definiert ist und jedes $X_j$ einer Bernoulli-Verteilung mit dem Erwartungswert $\E(X_j)=p$ folgt, gilt
\begin{align}
\mu=\E\left(\sum^n_{j=1}X_j\right)\stackrel{\eqref{EWSumme}}{=}\sum^n_{j=1} \E(X_j)=\sum\limits^n_{j=1}p=np. \label{EwertBinomial}
\end{align}

- Die Varianz von $X$ lässt sich mit dem \hil{Additionssatz für Varianzen stochastisch unabhängiger Zufallsvariablen} \eqref{varSumme} ermitteln.
- Da jedes $X_j$, $j=1,\ldots,n$ dieselbe Varianz $\var(X_j)=pq$ besitzt, folgt für die Summe $X$
\begin{align}
\sigma^2= \var\left(\sum\limits^n_{j=1}X_j\right)=\sum\limits^n_{j=1}\var(X_j)=\sum\limits^n_{j=1}pq \stackrel{\eqref{BernouVar}}{=} n p q. \label{VarBinomial}
\end{align}


## Binomialverteilung
\framesubtitle{Varianz}

- Der Einfluss der \hil{Scharparameter} $n$ und $p$ kann an den Gleichungen \[\mu=np\qquad\text{ und }\qquad\sigma^2=npq\] erkannt werden.
- Erhöht man $n$ bei konstantem $p$, steigen $\mu$ und $\sigma^2$; für die drei Verteilungen in der Abbildung erhält man $\mu_1=2$, $\sigma^2_1=1.6$, $\mu_2=6$, $\sigma^2_2=4.8$ und $\mu_3=10$, $\sigma^2_3=8$.
- Eine größeres $p$ bei konstantem $n$ lässt $\mu$ wachsen, während $\sigma^2$ nur bis $p=0.5$ steigt, dann wieder fällt.


## Binomialverteilung
\framesubtitle{Beispiel - QuizAcademy}
\QuizAcademy{Verteilungen 1}\endQuizAcademy


## Binomialverteilung
\framesubtitle{Beispiel}
\xmpl[Glücksrad]
Bei einem Glücksspiel betrage die Wahrscheinlichkeit zu gewinnen $p=0.2$.\mps
Die Binomialverteilung $B(5,0.2)$ liefert die Wahrscheinlichkeit, dass bei 5 Teilnahmen mindestens einmal gewonnen wird, wenn die Spiele unabhängig sind. $P(X\geq 1)$ berechnet sich nach dem Additionssatz für disjunkte Ereignisse als
\begin{align*}
P(X\geq1) & = f(x=1)+\ldots+f(x=5) \\
& = \binom{5}{1}\cdot 0.2\cdot 0.8^4 +\ldots+
\binom{5}{5}\cdot 0.2^5\cdot 0.8^0 \\
& = 0.4096+0.2048+0.0512+0.0064+0.0003=0.6723.
\end{align*}
\endxmpl

## Binomialverteilung
\framesubtitle{Beispiel}

\xmpl[Glücksrad]
 Schneller findet man die gesuchte Wahrscheinlichkeit über das Komplemantärereignis -- kein Gewinn --, das zu $X=0$ führt. Aus
 \[P(X=0)+P(X\geq1)=1\]
 folgt
 \[P(X\geq1)=1-P(X=0)=1-\binom{5}{0}\cdot 0.2^0\cdot 0.8^5=1-0.3277=0.6723.\]
$F(x)$ liefert die gesuchte Wahrscheinlichkeit $P(X\leq x)$ direkt.
\endxmpl

```{r BinomBspR}
1 - pbinom(q = 0, size = 5, prob = 0.2)
pbinom(q = 0, size = 5, prob = 0.2, lower.tail = F)
```

## Binomialverteilung
\framesubtitle{in \R}

```{r BinominR}
x <- rbinom(n = 10000, size = 5, prob = 0.2)
head(x, n = 20)
mean(x) # Stichprobenmittelwert
var(x)  # Stichprobenvarianz

dbinom(x = 0:5, size = 5, prob = 0.2) # Wahrscheinlichkeitsfunktion
pbinom(q = 0:5, size = 5, prob = 0.2) # Verteilungsfunktion
sum(x == 2) / 10000 # relative Häufigkeit der Realisationen gleich zwei
```


## Binomialverteilung
\framesubtitle{Beispiel}

\xmpl[Produktion]
Bei einer Fertigung seien 5% der Produkte fehlerhaft. Zur Qualitätsprüfung werden 5 Produkte entnommen. Gesucht sind die Wahrscheinlichkeiten für genau ein bzw. genau zwei defekte Produkte.
\begin{align*}
P(X=1) = f(x=1)&= \binom{5}{1}\cdot 0.05^1\cdot (1-0.05)^{5-1}\\
& = \frac{5!}{1!(5-1)!}\cdot 0.05\cdot 0.95^4 =0.2036.\\
P(X=2) = f(x=2)&= \binom{5}{2}\cdot 0.05^2\cdot (1-0.05)^{5-2}=0.0214.
\end{align*}

```{r}
dbinom(x = 1:2, size = 5, prob = 0.05)
```

\endxmpl


## Binomialverteilung
\framesubtitle{Reproduktivität}

- Die Summe $\tilde{S}=X+Y$ zweier unabhängiger und binomialverteilter Zufallsvariablen $X$ und $Y$ mit demselben $p$ ist ebenfalls binomialverteilt.
- Aus $X$: $B(m,p)$ und $Y$: $B(n,p)$ folgt also $\tilde{S}$: $B(m+n,p)$ -- die Binomialverteilung ist \hil{reproduktiv}.
- Denn: Da $X$ die Summe von $m$ unabhängigen, zweipunktverteilten Zufallsvariablen und $Y$ die Summe von $n$ unabhängigen, zweipunktverteilten Zufallsvariablen ist, ist $\tilde{S}$ die Summe von $m+n$ unabhängigen zweipunktverteilten Zufallsvariablen.
- Die Wahrscheinlichkeitsfunktion $f(s)$ für $s=0,1,2,\ldots,m+n$ ist daher analog zu $f(x)$
\[f(s)=\binom{m+n}{s} p^sq^{m+n-s},\] null sonst.


## Geometrische Verteilung

- Die Binomialverteilung liefert die Wahrscheinlichkeit für $x$-mal *Erfolg* in einer Bernoulli-Kette mit Länge $n$. 
- Manchmal interessiert aber die Wahrscheinlichkeit, dass eine Bernoulli-Kette die Länge $n$ erreicht, bevor zum ersten Mal *Erfolg* eintritt.
- $X$ ist nun definiert als Anzahl der Fehlversuche bis zum ersten Erfolg.
- Bei $X=0$ tritt der Erfolg bereits bei der ersten Durchführung ein, bei $X=x$ bei der $(x+1)$-ten Wiederholung.


## Geometrische Verteilung
\framesubtitle{Wahrscheinlichkeitsfunktion}

- Die Wahrscheinlichkeiten für diese Ereignisse liefert der Multiplikationssatz für unabhängige Ereignisse \eqref{gemWSUnabh}.
- Ist die Erfolgswahrscheinlichkeit wieder $p$ (und für *Misserfolg* $q=1-p$), ergibt sich \[ P(X=0)=p,\; P(X=1)=pq,\; P(X=2)=pq^2, \text{ usw.} \]
- Verallgemeinert erhält man die Wahrscheinlichkeitsfunktion $f(x)$ als
\begin{equation}
  f(x)=
  \begin{cases}
   pq^x & x=0,1,2,\ldots \label{DichteGeomVtlg}\\
   0 & \text{sonst}.\notag
  \end{cases}
\end{equation}


## Geometrische Verteilung
\framesubtitle{Verteilungsfunktion}
\vspace{1cm}

- Eine solche Zufallsvariable heißt \hil{geometrisch verteilt}; die Verteilung hängt nur von $p$ ab. Kurzform: $G(p)$.
- Für $f(x)$ gilt mit der Summenformel für eine geometrische Reihe
\begin{eqnarray*}
 \sum\limits^\infty_{x=0}f(x)&=&p+pq+pq^2+\ldots \\
 &=&p(1+q+q^2+\ldots )\\
 &=& p\sum\limits^\infty_{x=0}q^x=p\frac{1}{1-q}=p\frac{1}{p}=1.
\end{eqnarray*}
- Für $x=0,1,2,\ldots$ lautet die Verteilungsfunktion
\vspace{-0.5em}
\begin{equation*}
  F(x)=\sum\limits^x_{k=0}p q^k.
\end{equation*}


## Geometrische Verteilung
\framesubtitle{Verteilungsfunktion}

Das Buch auf S.\ 94-95 zeigt, dass $F(x)$ geschrieben werden kann als
 \begin{equation*}
  F(x)=
  \begin{cases}
   0& x<0 \\
   1-q^{[x]+1}& x\geq 0 \\
   1& x=\infty.
  \end{cases}
 \end{equation*}
 Beispiele für $p=0.6$:
 
```{r GeometrischeVtlg1, echo = F, fig.asp=0.75, fig.show="hold", fig.width=0.49}
plot(x = 0:4, y = dgeom(0:4, prob = 0.6), xlim = c(0,4.5), ylim = c(0, 1.2),
     xlab = "$x$", ylab = "$f(x)$", xaxt = "n", yaxt = "n", col = "tomato3")
axis(2, at = seq(0,1,0.2))
axis(1, at = 0:4)
segments(x0 = 0:4, y0 = 0, y1 = dgeom(0:4, prob = 0.6), ylim = c(0, 1.2), lty = 3)
points(x = 0:4, y = dgeom(0:4, prob = 0.6), col = "tomato3")
#mtext(text = c('$x$', '$f(x)$'), side = 1:2, line = c(0.25, 0.5), at = c(4.5, 1.2))

plot(x = 0:4, y = pgeom(0:4, prob = 0.6), xlim = c(0,4.5), ylim = c(0, 1.2),
     xlab = "x", ylab = "$F(x)$", xaxt = "n", yaxt = "n", col = "tomato3")
axis(2, at = seq(0,1,0.2))
axis(1, at = 0:4)
segments(x0 = 0:4, y0 = 0, y1 = pgeom(0:4, prob = 0.6), ylim = c(0, 1.2), lty = 3)
segments(x0 = 0:4, x1 = 1:5, y0 = pgeom(0:4, prob = 0.6), col = "tomato3")
points(x = 0:4, y = pgeom(0:4, prob = 0.6), col = "tomato3")
```


## Geometrische Verteilung
\framesubtitle{Verteilungsfunktion}

- Je größer $p$, desto schneller strebt $f(x)$ gegen null und $F(x)$ gegen eins.
- $q^{x+1}$ ist die Komplementärwahrscheinlichkeit zu $F(x)$.
- Da $F(x)=P(X\leq x)$, ist $P(X>x)=q^{x+1}$.
- Damit $X>x$ überhaupt eintreten kann, müssen zunächst genau $x+1$ Misserfolge vorliegen; die Anzahl nachfolgender Fehlversuche ist dann unerheblich.


## Geometrische Verteilung
\framesubtitle{Wartezeitverteilung - Beispiel}
\xmpl[Investition]
 Bei einem Investitionsobjekt beträgt die Wahrscheinlichkeit, Gewinn pro Periode zu erzielen: $p=0.7$.\mps
Wenn die Periodengewinne unabhängig sind, liefert die geometrische Verteilung die Wahrscheinlichkeit für den ersten Gewinn in der vierten Periode.\mps
$X$ nimmt den Wert $x=3$ an, $F(x)$ ist an der Stelle $x=3$
 \[F(3)=1-0.3^4=0.9919.\]
 $F(3)=P(X\leq3)$ ist die Wahrscheinlichkeit dafür, dass es höchstens drei Perioden dauert, bis in der vierten Periode der erste Gewinn realisiert wird. Man bezeichnet $G(p)$ wegen dieser Interpretation auch als \hil{Wartezeitverteilung}.
\endxmpl



## Geometrische Verteilung
\framesubtitle{Wartezeitverteilung - Beispiel}
\xmpl[Investition]
 Die Wahrscheinlichkeit der geometrischen Verteilung $f(x=3)=0.7\cdot 0.3^3=0.0189$ darf nicht mit der Wahrscheinlichkeit der Binomialverteilung verwechselt werden, die man für $n=4$ und $x=1$ (Erfolg) erhält. Diese beträgt
 \[f(x=1)=\binom{4}{1}0.7\cdot 0.3^3=0.0756.\]
 Sie gibt die Wahrscheinlichkeit einer Bernoulli-Kette der Länge $n=4$ mit einem *Erfolg* an. Da dieser nicht erst bei der vierten Durchführung eintreten muss, ist die Wahrscheinlichkeit notwendigerweise größer als bei der geometrischen Verteilung.
\endxmpl


## Geometrische Verteilung
\framesubtitle{Wartezeitverteilung - Beispiel}

\xmpl[*]
```{r}
# Verteilungsfunktion der geometrischen Verteilung
pgeom(q = 3, prob = 0.7)
# Alternativ Berechnung über die Dichtefunktion
sum(dgeom(x = 0:3, prob = 0.7))

# Zufallszahlen aus der Verteilung ziehen
rgeom(n = 100, prob = 0.7)
# 3 tritt sehr selten auf, 4 so gut wie nie
```
\endxmpl


## Geometrische Verteilung
\framesubtitle{Erwartungswert und Varianz}
Erwartungswert und Varianz der geometrischen Verteilung lauten \[  \mu=\frac{q}{p} \]
bzw. \[\sigma^2 = \frac{q}{p^2}.\]
Eine ausführliche Herleitung der beiden Parameter findet sich im Anhang.


## Geometrische Verteilung
\framesubtitle{Erwartungswert und Varianz - QuizAcademy}
\QuizAcademy{Verteilungen 2}\endQuizAcademy


## Negative Binomialverteilung

- Die Problemstellung der geometrischen Verteilung lässt sich verallgemeinern: Wie oft ist ein Bernoulli-Experiment durchzuführen, bis genau $r$ Erfolge eingetreten sind?
- $X$ ist jetzt definiert als die Anzahl der Misserfolge bis zum $r$-ten Erfolg.
- Damit $r$ Erfolge eintreten können, muss das Bernoulli-Experiment mindestens $r$-mal durchgeführt werden.
- $X=0$ bedeutet, dass alle $r$ Durchführungen Erfolge sind; $X=1$ bedeutet, dass sich der $r$-te Erfolg in der $(r+1)$-ten Durchführung einstellt. 
- Allgemein bedeutet $X=x$, dass der $r$-te Erfolg bei der $(x+r)$-ten Ausführung vorliegt.
- Bevor der $r$-te Erfolg eintritt, müssen in $x+r-1$ Durchführungen genau $r-1$ Erfolge realisiert worden sein; in der letzten, der $(x+r)$-ten Wiederholung muss *Erfolg* eintreten.




## Negative Binomialverteilung

- Die Wahrscheinlichkeit für $r-1$ Erfolge bei $x+r-1$ Durchführungen erhält man mit der Binomialverteilung \eqref{DichteBinomial} als
 \[ \binom{x+r-1}{r-1} p^{r-1}q^x. \]
- Die Erfolgswahrscheinlichkeit im $(x+r)$-ten Durchgang beträgt wie bei allen anderen Durchführungen $p$.
- Die Wahrscheinlichkeit, dass der $r$-te Erfolg genau bei der $(r+x)$-ten Durchführung eintritt, ist wegen Unabhängigkeit der Experimente das Produkt beider Wahrscheinlichkeiten.

- Damit lautet
 \begin{equation*}
  f(x)=
  \begin{cases}
   \dbinom{x+r-1}{r-1}p^rq^x& x=0,1,2,\ldots \\
   0& \text{sonst}.
  \end{cases}
 \end{equation*}


## Negative Binomialverteilung
\framesubtitle{Verteilungsfunktion}

- Eine solche Zufallsvariable heißt \hil{negativ binomialverteilt} oder auch \hil{Pascal-verteilt}.
- Die negative Binomialverteilung hängt von den Parametern $r$ und $p$ ab; symbolisiere sie daher mit $NB(r,p)$.
- Die Verteilungsfunktion lautet
\begin{equation*}
  F(x) =
  \begin{cases}
   0& x<0 \\
   \sum\limits_{k=0}^{[x]}\dbinom{k+r-1}{r-1}p^rq^k& 0\leq x,\quad k=0,1,\ldots[x].
  \end{cases}
 \end{equation*}





## Negative Binomialverteilung
\framesubtitle{Wahrscheinlichkeits- und Verteilungsfunktion}
In der Abbildung sind Wahrscheinlichkeits- und Verteilungsfunktion der negativen Binomialverteilung für $r=4$ und $p=0.5$ dargestellt.

```{r NegBinomialVtlg1, echo=F, fig.show="hold",  fig.asp=0.75,  fig.width=0.49}
p <- 0.5
n <- 15
plot(0:n, y = dnbinom(x = 0:n, size = 4, prob = p),
     xlim = c(0, 15), ylim = c(0, 0.17), xlab = "$x$", ylab = "$f(x)$", col = "tomato3")
segments(x0 = 0:n, y0 = 0, y1 = dnbinom(x = 0:n, size = 4, prob = p), lty = 3)
points(0:n, y = dnbinom(x = 0:n, size = 4, prob = p), col = "tomato3")

plot(0:n, y = pnbinom(q = 0:n, size = 4, prob = p), yaxt = "n",
     xlim = c(0, 17), ylim = c(0, 1.2), xlab = "$x$", ylab = "$F(x)$", col = "tomato3")
axis(2, at = seq(0, 1, .2))
segments(x0 = 0:n, x1 = 1:16, y0 = pnbinom(q = 0:n, size = 4, prob = p), col = "tomato3")
segments(x0 = 0:n, y0 = 0, y1 = pnbinom(q = 0:n, size = 4, prob = p), lty = 3)
points(0:n, y = pnbinom(q = 0:n, size = 4, prob = p), col = "tomato3")
```




## Negative Binomialverteilung
\framesubtitle{Erwartungswert und Varianz}

- Die Herleitung des Erwartungswertes und der Varianz finden sich im Buch auf S.\ 96-97. 
- Erwartungswert und Varianz für $X$ betragen
\begin{eqnarray*}
  \mu &=& %\E\left(\sum_{j=1}^r X_j \right) = \sum_{j=1}^r \E(X_j) =
  \frac{rq}{p} \quad \text{und} \\
  \sigma^2 &=& %\var\left(\sum_{j=1}^r X_j\right)  = \sum_{j=1}^r \var(X_j) =
  \frac{rq}{p^2}.
\end{eqnarray*}


## Negative Binomialverteilung
\framesubtitle{Beispiel}
\xmpl[Glücksspiel]
Bei einem Glücksspiel hat derjenige gewonnen, der 4-mal die 6 mit der geringsten Anzahl an Würfen erzielt. Die Wahrscheinlichkeit, dass ein Teilnehmer dieses Ziel mit sechs Würfen erreicht, lässt sich mit der negativen Binomialverteilung berechnen.\mps
In diesem Fall muss beim sechsten Wurf zum vierten Mal die 6 eintreten. Die Parameter der negativen Binomialverteilung lauten $p=\frac{1}{6}$ und $r=4$; $X$ nimmt den Wert zwei an. Es ergibt sich \[f(2)= \binom{5}{3}p^4q^2=0.0054.\]
\endxmpl



## Negative Binomialverteilung
\framesubtitle{Beispiel}
\xmpl[*]
Der Erwartungswert des Spiels beträgt dann \[\mu = \frac{4\cdot5/6}{1/6}=20,\] d.h. der vierte Erfolg kann nach 20 Misserfolgen erwartet werden bzw. es muss bis zum vierten Erfolg durchschnittlich 24-mal gespielt werden.\mps
Die Varianz folgt als \[\sigma^2=\frac{4\cdot 5/6}{1/36}=120,\] oder $\sigma=10.9545$. Je größer $\sigma^2$, desto geringer ist die Information, die der Erwartungswert, als mittlere Wartezeit interpretiert, liefert.
\endxmpl




## Negative Binomialverteilung
\framesubtitle{Beispiel}

\xmpl[*]
```{r,  fig.width=0.7, fig.asp=0.5}
dnbinom(x = 2, size = 4, prob = 1/6)
plot(x = 0:60, y = dnbinom(x = 0:60, size = 4, prob = 1/6), pch = 19,
     col = "tomato3", ylab = "$f(x)$", xlab = "$x$")
```
\endxmpl

## Hypergeometrische Verteilung

- Den bisher behandelten Verteilungen liegt *Ziehen mit Zurücklegen* zugrunde.
- Die \hil{hypergeometrische Verteilung} ergibt sich, wenn die Zufallsvorgänge durch *Ziehen ohne Zurücklegen* charakterisiert werden können, bei denen wieder nur das Eintreten einer Eigenschaft $A$ relevant ist.
- Ausgangspunkt ist daher eine endliche Grundgesamtheit vom Umfang $N$, bei der $M<N$ Elemente eine Eigenschaft $A$ besitzen, die übrigen $N-M$ Elemente nicht.
- Die Zufallsvariablen $X_j$, $j=1,\ldots,n$, sind eins, wenn das gezogene Element bei der $j$-ten Entnahme $A$ aufweist; sie sind null wenn nicht.
- Zieht man $n$-mal ohne Zurücklegen, gibt $X=\sum^n_{j=1}X_j$ an, wie viele der Entnahmen die Eigenschaft $A$ aufweisen.


## Hypergeometrische Verteilung
\framesubtitle{Wahrscheinlichkeits- und Verteilungsfunktion}

- Um Wahrscheinlichkeiten zu bestimmen, muss zunächst die Anzahl der Möglichkeiten, $n$ Elemente aus $N$ auszuwählen, berechnet werden.
- Es gibt $\binom{N}{n}$ Möglichkeiten, siehe \eqref{BinKoeff}.
- Wir nehmen an, dass sie alle die gleiche Ziehungswahrscheinlichkeit besitzen.
- Für $P(X=x)$ benötigt man die Anzahl der Möglichkeiten, die $x$ Elemente mit $A$ enthalten.
- Es gibt $\binom{M}{x}$ Möglichkeiten, $x$ aus $M$ Elementen ohne Zurücklegen auszuwählen, siehe wiederum \eqref{BinKoeff}.


## Hypergeometrische Verteilung
\framesubtitle{Wahrscheinlichkeits- und Verteilungsfunktion}

- Zu jeder dieser Möglichkeiten existieren $\binom{N-M}{n-x}$ Möglichkeiten, $n-x$ Elemente aus $N-M$ Elementen auszuwählen, die nicht $A$ aufweisen.
- Insgesamt gibt es somit $\binom{M}{x}\binom{N-M}{n-x}$ Möglichkeiten, die $x$ Elemente mit Eigenschaft $A$ enthalten.
- Das Verhältnis dieser Anzahl zu den insgesamt möglichen Auswahlen ist die Wahrscheinlichkeit für $X=x$.


## Hypergeometrische Verteilung
\framesubtitle{Wahrscheinlichkeits- und Verteilungsfunktion}
- Die Wahrscheinlichkeitsfunktion resultiert also als
 \begin{eqnarray*}\notag
  & & f(x)=
  \begin{cases}
   \frac{\binom{M}{x}\binom{N-M}{n-x}}{\binom{N}{n}} & \max\{0,n+M-N\} \leq x \leq \min\{n,M\} \\ 0 &  \text{sonst}.
  \end{cases} \\
 \end{eqnarray*}
- Die Verteilungsfunktion folgt hieraus durch Summation über $k$:
 \begin{eqnarray*}\notag
  & & F(x)=
  \begin{cases}
   0 &  x<\max\{0,n+M-N\}=a \\
   \frac{\sum_{k=a}^{[x]}\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}} &  a \leq x \leq \min\{n,M\}, \quad k=a,\ldots, [x]\\
   1 & x>\min\{n,M\} \\
  \end{cases} \\
 \end{eqnarray*}
- Die Beschränkungen für den Wertebereich von $X$ werden im Buch auf S.\ 109 hergeleitet.


## Hypergeometrische Verteilung
\framesubtitle{Wahrscheinlichkeits- und Verteilungsfunktion}

- $f(x)$ und $F(x)$ zeigen, dass die hypergeometrische Verteilung die Parameter $N,$ $M$ und $n$ besitzt; kürze sie daher mit $H(N,M,n)$ ab.
- In der folgenden Abbildung unterscheiden sich die $f(x)$ nur in $M$:

```{r HyperGeomVtlg1, echo=F, fig.show="hold", fig.width=0.49}
plot(0:2, y = dhyper(x = 0:2, m = 2, n = 10-2, k = 5), xlab = "$x$", ylab = "$f(x)$",
     xlim = c(0, 5.5), ylim = c(0, 0.55), col = "tomato3", main="$N=10$, $n=5$, $M=2$")
segments(x0 = 0:2, y0 = 0, y1 = dhyper(x = 0:2, m = 2, n = 10-2, k = 5), lty = 3)
points(0:2, y = dhyper(x = 0:2, m = 2, n = 10-2, k = 5), col = "tomato3")


plot(0:5, y = dhyper(x = 0:5, m = 6, n = 10-6, k = 5), xlab = "$x$", ylab = "$f(x)$",
     xlim = c(0, 5.5), ylim = c(0, 0.55),col = "tomato3", main="$N=10$, $n=5$, $M=6$")
segments(x0 = 1:5, y0 = 0, y1 = dhyper(x = 1:5, m = 6, n = 10-6, k = 5), lty = 3)
points(1:5, y = dhyper(x = 1:5, m = 6, n = 10-6, k = 5), col = "tomato3")
```

- Mit zunehmendem $M$ wandert $f(x)$ nach rechts.


## Hypergeometrische Verteilung
\framesubtitle{Erwartungswert und Varianz}

- Erwartungswert und Varianz der hypergeometrischen Verteilung sind (Herleitung siehe Anhang):
\[  \mu=np\quad\text{bzw.}\quad\sigma^2=np(1-p)\dfrac{N-n}{N-1}.\]
- Binomial- und hypergeometrische Verteilung besitzen denselben Erwartungswert, obwohl sich die zugrunde liegenden Zufallsvorgänge unterscheiden (vgl. \eqref{EwertBinomial}).
- Für den Erwartungswert ist es daher unbedeutend, ob bei dem Urnenmodell das entnommene Element zurückgelegt wird oder nicht.
- Die \hil{Varianz} der hypergeometrischen Verteilung stimmt bis auf den Faktor $\frac{N-n}{N-1}$ auch mit der für die Binomialverteilung überein (vgl. \eqref{VarBinomial}).


## Hypergeometrische Verteilung
\framesubtitle{Korrekturfaktor}

- Da sich beim *Ziehen mit Zurücklegen* die Grundgesamtheit nicht erschöpft, nennt man $\frac{N-n}{N-1}$ den \hil{Korrekturfaktor für endliche Gesamtheiten}.
- Bei $N=n$ ist, logischerweise, $\sigma^2=0$ (warum?).
- Für hinreichend große $N$ gilt \[\frac{N-n}{N-1}\approx \frac{N-n}{N}=1-\frac{n}{N}.\]
- Der Korrekturfaktor ist bei kleinem Auswahlsatz $n/N$ fast eins.
- Dann bleibt die Wahrscheinlichkeit, Elemente mit $A$ zu erhalten, auch beim *Ziehen ohne Zurücklegen* von Entnahme zu Entnahme fast gleich.
- Die Binomialverteilung kann als Approximation für die hypergeometrische Verteilung dienen, wenn der \hil{Auswahlsatz} $n/N$ kleiner als 5% ist.


## Hypergeometrische Verteilung
\framesubtitle{Vergleich Binomial- und hypergeometrische Verteilung}

```{r HyperGeoAnhang, echo=c(1:5), fig.width=0.6, eval=FALSE}
N <- 200; n <- 5
x <- 0:N; M <- 80
p <- M/N

plot(x, dbinom(x,n,p), xlim = c(0,n), cex = 1.3, col = "blue", lwd = 2, ylab = "Dichte")
points(x, dhyper(x, m = M, n = N-M, k = n), col = "red", lwd = 2)
legend("topright", legend = c("Binom.","Hyper."), col = c("blue","red"), pch = 20)
```


```{r HyperGeoAnhang2, echo=FALSE, eval=TRUE, fig.width=0.6}
N <- 200
n <- 5
x <- 0:N
M <- 80
p <- M/N

plot(x, dbinom(x,n,p), xlim = c(0,n), cex = 1.3,
     col = "blue", lwd = 2, ylab = "Dichte")
points(x, dhyper(x, m = M, n = N-M, k = n), col = "red", lwd = 2)
legend("topright", legend = c("Binom.","Hyper."), col = c("blue","red"), pch = 20)
```



## Hypergeometrische Verteilung
\framesubtitle{Beispiel}
\xmpl[Lotto, 6 aus 49 (bis 2013)]
 Man kann sechs der Zahlen 1 bis 49 auf $\binom{N}{n}=\binom{49}{6}=13\,983\, 816$ unterschiedliche Arten ankreuzen. Durch Ziehen ohne Zurücklegen werden sechs Gewinnzahlen ermittelt.\mps Damit sind die 49 Zahlen in eine Teilmenge mit den $M=6$ Gewinnzahlen und eine andere Teilmenge der $N-M=43$ restlichen Zahlen aufgeteilt.\mps
Die Wahrscheinlichkeit, dass in einer Kombination $x=0,1,2,\ldots,6$ Gewinnzahlen vorkommen, erhält man aus
\[f(x)=\dfrac{\binom{6}{x}\binom{43}{6-x}}{\binom{49}{6}}.\]
\endxmpl


## Hypergeometrische Verteilung
\framesubtitle{Beispiel}
\xmpl[*]
Die Wahrscheinlichkeiten für $x=0, x=3$ und $x=5$ betragen gerundet \[ f(x=0)=0.4360,\; f(x=3)=0.0177 \text{ und } f(x=5)=0.00002.\]
Nach der Ziehung der sechs Gewinnzahlen wird eine Zusatzzahl gezogen. Die Wahrscheinlichkeit für das Ereignis *drei Gewinnzahlen (GZ) und die Zusatzzahl (ZZ)* erhalten wir aus
\begin{eqnarray*}
P(\text{GZ}=x\text{ und ZZ})&=&P(\text{ZZ}|\text{GZ}=x)P(\text{GZ}=x)\\
&=&\frac{6-x}{43}f(x).
\end{eqnarray*}
\endxmpl


## Hypergeometrische Verteilung
\framesubtitle{Beispiel}
\xmpl[*]
```{r}
# WSK für 0, 1, 2, ..., 6 Gewinnzahlen
round(dhyper(x = 0:6, m = 6, n = 43, k = 6), 5)

# 3 Gewinnzahlen + Zusatzzahl
# m = 6 -> 6 mögliche Treffer, m = 3 -> nur noch 3 mögliche Treffer
dhyper(x = 3, m = 6, n = 43, k = 6) * dhyper(x = 1, m = 3, n = 40, k = 1)
```
\mps
Man bemerke (siehe \texttt{?dhyper}), dass $\texttt{m}=M$, $\texttt{n}=N-M$ und $\texttt{k}=n$.

Beachte außerdem im letzten Schritt, dass $M=6-x$, $x=1$, $n=1$ und $N=43$, so dass
\[
\frac{{6-x \choose 1}{43-(6-x) \choose 0}}{{43 \choose 1}}=\frac{6-x}{43}
\]
\endxmpl


## Ausgewählte diskrete Verteilungen
\framesubtitle{Aufgaben}
\exe
Geben Sie für $X$, das aus dem Zufallsexperiment *einmaliger Würfelwurf* resultiert, die Wahrscheinlichkeits- und die Verteilungsfunktion an!
\endexe
\exe
Aus einer Urne mit zwei weißen Kugeln und einer schwarzen Kugel wird
einmal gezogen. $X$ nimmt 1 an, wenn *weiß* gezogen
wird und 0, wenn *schwarz* gezogen wird. Geben Sie für $X$
die Wahrscheinlichkeits- und die Verteilungsfunktion an!
\endexe


## Ausgewählte diskrete Verteilungen
\framesubtitle{Aufgaben}
\exe
In einer Urne befinden sich 30 grüne und 60 schwarze Kugeln. Es werden 4 Kugeln mit Zurücklegen aus der Urne entnommen.

(a) Wie groß ist die Wahrscheinlichkeit 2 grüne Kugeln zu ziehen?
(b) Wie groß ist die Wahrscheinlichkeit höchstens 2 grüne Kugeln zu ziehen?
(c) Berechnen Sie den Erwartungswert und die Varianz für $X$!

\endexe


## Ausgewählte diskrete Verteilungen
\framesubtitle{Aufgaben}
\exe
In einer Urne befinden sich 10 Kugeln von denen 6 schwarz und die restlichen weiß sind.
Wenn 5 Kugeln aus der Urne ohne Zurücklegen gezogen werden, wie groß ist die Wahrscheinlichkeit, dass 3 davon schwarz sind?
\endexe
\exe
Zufallsexperiment: Einmaliger Würfelwurf.

(a) Wie groß ist die Wahrscheinlichkeit, dass im 8. Wurf das 3. Mal die 6 gefallen ist?
(b) Wie groß ist die Wahrscheinlichkeit, dass spätestens im 8. Wurf die dritte 6 fällt?
(c) Wie oft muss durchschnittlich gewürfelt werden bis die dritte 6 fällt?

\endexe


## Ausgewählte diskrete Verteilungen
\framesubtitle{Aufgaben}
\exe
Bei der Untersuchung von Investitionstätigkeiten von Bankangestellten hat sich herausgestellt, dass pro Jahr 15 Transaktionen an der Börse getätigt werden.

(a) Wie groß ist die Wahrscheinlichkeit, dass in einem Jahr eine Transaktion durchgeführt wird?
(b) Wie groß ist die Wahrscheinlichkeit, dass in einem Monat eine Transaktion durchgeführt wird?

\endexe

\exe
Ein Basketballspieler hat während seiner Karriere 84% seiner Freiwürfe verwandelt.
Wie groß ist die Wahrscheinlichkeit, dass er in einem Spiel 6 von 16 Freiwürfen nicht verwandelt?
\endexe


## Ausgewählte diskrete Verteilungen
\framesubtitle{Aufgaben}
\vspace{1cm}
\exe
In einer Gruppe von 10 Personen präferieren 6 Personen Cola und die restlichen Personen Fanta.
Wenn 3 Personen aus dieser Gruppe nach ihrem Lieblingsgetränk gefragt werden, wie groß ist die Wahrscheinlichkeit, dass die Mehrheit Fanta bevorzugt?
\endexe
\exe
Ein Student kommt nach einer Studentenparty angetrunken nach Hause. Er hat einen Schlüsselbund mit 8 Schlüsseln und sucht nach dem Haustürschlüssel.

(a) Wie groß ist die Wahrscheinlichkeit, dass der 6. Schlüssel, den er ausprobiert in das Schloss passt?
(b) Wie groß ist die Wahrscheinlichkeit, dass spätestens der 6. Schlüssel passt?
(c) Wie viele Schlüssel muss er durchschnittlich ausprobieren bis er den richtigen Schlüssel gefunden hat?

\endexe


## Ausgewählte diskrete Verteilungen
\framesubtitle{Aufgaben}
\exe
Ein Spieler setzt beim Roulettespiel bei jedem der 200 durchgeführten Spiele auf die 17. Die Wahrscheinlichkeit einmal zu gewinnen ist $P(\text{Zahl}\; 17)=\frac{1}{37}=0.027$.\mps
Wie groß ist die Wahrscheinlichkeit, dass er genau acht Mal gewinnt?
\endexe


## Normalverteilung

- Die wohl wichtigste Verteilung der Statistik ist die \hil{Normalverteilung}. Sie wurde von de Moivre, Laplace und Gauß entwickelt.
- Die Normalverteilung beschreibt bestimmte konkrete empirische Verteilungen recht gut.
- Für die Induktive Statistik aber noch bedeutsamer ist, dass bei hinreichend großem Stichprobenumfang auch viele in der Schätz- und Testtheorie auftretenden theoretischen Verteilungen durch die Normalverteilung gut approximiert werden.


## Normalverteilung
\framesubtitle{Dichte- und Verteilungsfunktion}

- Die Dichte der Normalverteilung lautet \[f(x) \stackrel{\text{Def.} \, \ref{DichteFkt}}{=}\frac{1}{\sqrt{2\pi}\sigma}\text{e}^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2} \; , \quad -\infty < x < \infty, \] mit Erwartungswert $\mu$ und Varianz $\sigma^2$ als Scharparameter.

- Daraus erhält man die Verteilungsfunktion als
\begin{equation*}
F(x) \stackrel{\text{Def.} \, \ref{VtlgFunktion}}{=}\frac{1}{\sqrt{2\pi}\sigma}\int\limits_{-\infty}^x  \text{e}^{-\frac{1}{2}\left(\frac{u-\mu}{\sigma}\right)^2} \, du  \; , \quad -\infty < x < \infty.
\end{equation*}
- Die Normalverteilung wird abgekürzt mit $N(\mu,\sigma^2)$.


## Normalverteilung
\framesubtitle{Dichte- und Verteilungsfunktion für $\mu=0$ und $\sigma^2=1$}

```{r NormalVtlg1, echo=F, fig.show="hold", fig.asp=0.5, fig.width=0.6}
par(mar=c(3.5, 3.5, 1, 1))
x <- seq(-4, 4, by = .125)
plot(x, dnorm(x), type = "l", lwd = 2, xaxt="n", col="tomato3", ylab="$f(x)$", xlab="")
abline(v=0, lty=2)

legend("topright", legend="$\\mu=0 \\quad \\sigma=1$", col = "tomato3", bty = "n", lty = 1, lwd = 2)

plot(x, pnorm(x), type = "l", lwd = 2, col="tomato3", ylab = "$F(x)$", xlab = "$x$")
abline(v = 0, h = 0.5, lty = 2)
```


## Normalverteilung
\framesubtitle{Dichte- und Verteilungsfunktion}
\exe
Zeigen Sie, dass die Dichtefunktion der Normalverteilung an der Stelle $x=\mu$ ein Maximum besitzt!
\endexe
 
- Die Dichtefunktion ist symmetrisch um $\mu$; an den Stellen $\mu+x$ und $\mu-x$ nimmt $f(x)$ für alle $x\in\mathbb{R}$ dieselben Werte an.
- Die Dichtefunktion fällt rechts und links von $\mu$ streng monoton und nähert sich asymptotisch der Abszisse für $x\rightarrow\pm\infty$.
- Wegen der \hil{Symmetrie} gilt Median = Modus = Erwartungswert.
- Also hat die Verteilungsfunktion bei $x=\mu$ einen Wendepunkt.


## Normalverteilung
\framesubtitle{Variation von $\mu$ und $\sigma$}

- Wird $\mu$ variiert, verschiebt sich die Dichtefunktion auf der $x$-Achse, ohne dass sich ihre Form ändert.
- Die Auswirkung einer Variation von $\sigma$ illustriert der Funktionswert $f(x)$ an der Stelle $x=\mu$: \[f(\mu)=\frac{1}{\sigma\sqrt{2\pi}}.\]
- Nimmt $\sigma$ zu, wird $f(\mu)$ kleiner. Daher verläuft die Dichtefunktion mit zunehmendem $\sigma$ flacher.
- Die nächsten Abbildungen zeigen $f(x)$
  (a) für $\mu=3$ und $\mu=4$ bei konstantem $\sigma$, sowie
  (b) für $\sigma^2=0.25$, $\sigma^2=1$ und $\sigma^2=4$ bei konstantem $\mu=0$.

## Normalverteilung
\framesubtitle{Variation von $\mu$ und $\sigma$}
 
```{r NormalVtlg2, echo = F, fig.show="hold", fig.asp=0.75, fig.width=0.49}
grid <- seq(0, 7, .03)

plot(x = grid, y = dnorm(grid, 3, 1), type = "l", xlab = "$x$", ylab = "$f(x)$", ylim = c(0, .7), xaxt = "n", col="tomato3", lwd=2)
lines(x=grid, y=dnorm(grid, 4, 1), lwd=2)
segments(x0 = 3:4, y0 = 0, y1 = dnorm(3, 3, 1), lty = 3, lwd=2)
axis(1, at = 3:4)
title(main = "(a)", adj=0)

grid <- seq(-3.5, 3.5, .03)
plot(x = grid, y = dnorm(grid, 0, .5), type = "l", xlab = "$x$", ylab = "$f(x)$", xaxt = "n", col = "tomato3", lwd=2)
lines(x = grid, y = dnorm(grid, 0, 1), col = "dodgerblue3", lwd=2)
lines(x = grid, y = dnorm(grid, 0, 2), col = "forestgreen", lwd=2)
axis(1, at = -1:1)
legend(x = 1, y = .7, legend = c("$0.25$","$1$","$4$"), col = c("tomato3", "dodgerblue3", "forestgreen"), lty = 1, lwd=2, bty = "n", title = "$\\sigma^2$")
title(main = "(b)", adj=0)
```


## Normalverteilung
\framesubtitle{Reproduktivitätseigenschaft}

 - Die Normalverteilung besitzt die \hil{Reproduktivitätseigenschaft}, d.h.\ Linearkombinationen von normalverteilten $X_1,\ldots,X_n$ sind auch normalverteilt.
- Zwei wichtige Spezialfälle:
  (a) Die \hil{Summe} $\tilde{S}$ der $X_j$, $\tilde{S}=\sum^n_{j=1}X_j$ und 
  (b) das \hil{arithmetische Mittel} $\bar{X}=\frac{1}{n}\sum^n_{j=1}X_j$
  sind normalverteilt.
- Im folgenden Kapitel mehr zu den dann resultierenden Parametern der Normalverteilung von $\tilde{S}$ und $\bar{X}$.




## Normalverteilung
\framesubtitle{Lineartransformation}

- Ist $X:N(\mu_X, \sigma^2_X)$, dann ist auch $Y=a+bX$ normalverteilt mit Parametern \[\mu_Y\stackrel{\eqref{EWlinTrafo}}{=}a+b\mu_X\quad\text{und}\quad \sigma^2_Y\stackrel{\eqref{VarLinKomb3}}{=}b^2\sigma^2_X.\]
- Die Transformation
\begin{equation}\label{stdNormalTrafo}
Y=a+b X\quad\text{mit}\quad a=-\mu_X/\sigma_X\quad\text{und}\quad b=1/\sigma_X
\end{equation}
führt zu einer standardisierten Zufallsvariablen (s.\ \eqref{stdRV}), mit $\mu_Y=0$ und $\sigma^2_Y=1$.
- Ist $X$ normalverteilt, so ist die standardisierte Zufallsvariable $Z$ $N(0,1)$-verteilt, d.h.\ \hil{standardnormalverteilt}.
- Zusammengefasst: \[  Z=\dfrac{X-\mu}{\sigma}, \text{ mit } X:\; N(\mu,\sigma^2) \text{ und }  Z:\; N(0,1).\]


## Standardnormalverteilung
\framesubtitle{Dichte- und Verteilungsfunktion}

- Für die Wahrscheinlichkeiten gilt somit
 \begin{align}
 P(X\leq a) = P\left(Z\leq \dfrac{a-\mu}{\sigma}\right). \label{StdNormVtlgWsk}
 \end{align}
- Dichte- und Verteilungsfunktion von $Z$:
\[f(z)= \frac{1}{\sqrt{2\pi}}\text{e}^{-\frac{z^2}{2}},\quad F(z)=\frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^z \text{e}^{-\frac{u^2}{2}}\; du.
\]
- Die Dichtefunktion hat das Maximum an der Stelle $z=\mu=0$ und Wendepunkte bei $z=\pm\sigma=\pm 1$; der Wendepunkt der Verteilungsfunktion $F(z)$ liegt bei $z=\mu=0$.




## Standardnormalverteilung
\framesubtitle{Dichte- und Verteilungsfunktion}

- Die folgenden Graphen zeigen Dichte- und Verteilungsfunktion einer standardnormalverteilten Zufallsvariable:

```{r StdNormalVtlg1, echo = F, fig.show="hold", fig.asp=0.75, fig.width=0.49}
grid <- seq(-3, 3, .03)
plot(x = grid, y = dnorm(grid), type = "l", xlab="$z$", ylab="$f(z)$", ylim = c(0, .5), col = "tomato3", xaxt = "n", las=1, lwd=2)
axis(1, at = -1:1, labels = c("$-1$", "0", "$1$"))
polygon(x = c(seq(-4, 1, .01), 1), y = c(dnorm(seq(-4, 1, .01)), 0), col = alpha("tomato3", 0.5), border = NA)
segments(x0 = c(-1, 1), y0 = 0, y1 = dnorm(1), lty = 3, lwd=2)
title(main = "$(a)$", adj=0)

plot(x = grid, y = pnorm(grid), type = "l", xlab = "$z$", ylab="$F(z)$", ylim = c(0, 1), xaxt = "n", yaxt = "n", col = "tomato3", lwd=2)
segments(x0 = c(1, -3), x1 = 1, y0 = c(0, pnorm(1)), y1 = pnorm(1), lty = 3, lwd=2)
axis(1, at = 0:1, labels = c(0, "$1$"))
axis(2, at = c(0, 0.33, 0.66, 1), las=1)
axis(2, at = round(pnorm(1), 3), las=1)
title(main="$(b)$", adj=0)
```


## Standardnormalverteilung
\framesubtitle{Dichte- und Verteilungsfunktion - QuizAcademy}
\QuizAcademy{Verteilungen 3}\endQuizAcademy


## Standardnormalverteilung
\framesubtitle{Dichte- und Verteilungsfunktion}

- \hil{Symmetrie} der Normalverteilung um Null impliziert, dass \[P(Z\leq -z_1)=P(Z\geq z_1)=1- P(Z\leq z_1)\] für jedes $z_1 \in\mathbb{R}$.
- Die rötliche Fläche in \blue{(a)} hat dieselbe Größe wie die Ordinate $F(z_1)$ in (b): $P(Z\leq z_1)$.
 - Die Wahrscheinlichkeit, dass $Z$ Werte im Intervall $[z_1,z_2]$ annimmt, berechnet sich (wie üblich) als \[P(z_1\leq Z\leq z_2) = P(Z\leq z_2)-P(Z<z_1)=F(z_2)-F(z_1).\]


## Standardnormalverteilung
\framesubtitle{Tabellierung der Verteilungsfunktion}

- Früher hat man $F(z)$ in folgender Tabelle abgelesen:
\vspace{-0.25em}
\begin{scriptsize}
\setlength{\tabcolsep}{.5mm}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c}\cline{1-11}
{\boldmath{$z$}} & {\bf 0.00}   & {\bf 0.01}   & {\bf 0.02}   & {\bf 0.03}   & {\bf 0.04}
& {\bf 0.05} & {\bf 0.06} & {\bf 0.07} & {\bf 0.08} & {\bf 0.09} \\ \cline{1-11}
{\bf 0.0} & 0.5000 & 0.5040 & 0.5080 & 0.5120 & 0.5160  &   0.5199  &   0.5239  &   0.5279  & 0.5319  &   0.5359 \\
{\bf 0.1} &   0.5398  &   0.5438  &   0.5478  &   0.5517  & 0.5557  &   0.5596  &   0.5636  &   0.5675  & 0.5714  &   0.5753 \\
{\bf 0.2} &   0.5793  &   0.5832  &   0.5871  &   0.5910  & 0.5948  &   0.5987  &   0.6026  &   0.6064  & 0.6103  &   0.6141 \\
{\bf 0.3} &   0.6179  &   0.6217  &   0.6255  &   0.6293  & 0.6331  &   0.6368  &   0.6406  &   0.6443  & 0.6480  &   0.6517 \\
{\bf 0.4} &   0.6554  &   0.6591  &   0.6628  &   0.6664  & 0.6700  &   0.6736  &   0.6772  &   0.6808  & 0.6844  &   0.6879 \\
{\bf 0.5} &   0.6915  &   0.6950  &   0.6985  &   0.7019  & 0.7054  &   0.7088  &   0.7123  &   0.7157  & 0.7190  &   0.7224 \\
{\bf 0.6} &   0.7257  &   0.7291  &   0.7324  &   0.7357  & 0.7389  &   0.7422  &   0.7454  &   0.7486  & 0.7517  &   0.7549 \\
{\bf 0.7} &   0.7580  &   0.7611  &   0.7642  &   0.7673  & 0.7704  &   0.7734  &   0.7764  &   0.7794  & 0.7823  &   0.7852 \\
{\bf 0.8} &   0.7881  &   0.7910  &   0.7939  &   0.7967  & 0.7995  &   0.8023  &   0.8051  &   0.8078  & 0.8106  &   0.8133 \\
{\bf 0.9} &   0.8159  &   0.8186  &   0.8212  &   0.8238  & 0.8264  &   0.8289  &   0.8315  &   0.8340  & 0.8365  &   0.8389 \\
{\bf 1.0} &   0.8413  &   0.8438  &   0.8461  &   0.8485  & 0.8508  &   0.8531  &   0.8554  &   0.8577  & 0.8599  &   0.8621 \\ \cline{1-11}
{\bf 1.1} &   0.8643  &   0.8665  &   0.8686  &   0.8708  & 0.8729  &   0.8749  &   0.8770  &   0.8790  & 0.8810  &   0.8830 \\
{\bf 1.2} &   0.8849  &   0.8869  &   0.8888  &   0.8907  & 0.8925  &   0.8944  &   0.8962  &   0.8980  & 0.8997  &   0.9015 \\
{\bf 1.3} &   0.9032  &   0.9049  &   0.9066  &   0.9082  & 0.9099  &   0.9115  &   0.9131  &   0.9147  & 0.9162  &   0.9177 \\
{\bf 1.4} &   0.9192  &   0.9207  &   0.9222  &   0.9236  & 0.9251  &   0.9265  &   0.9279  &   0.9292  & 0.9306  &   0.9319 \\
{\bf 1.5} &   0.9332  &   0.9345  &   0.9357  &   0.9370  & 0.9382  &   0.9394  &   0.9406  &   0.9418  & 0.9429  &   0.9441 \\
{\bf 1.6} &   0.9452  &   0.9463  &   0.9474  &   0.9484  & 0.9495  &   0.9505  &   0.9515  &   0.9525  & 0.9535  &   0.9545 \\
{\bf 1.7} &   0.9554  &   0.9564  &   0.9573  &   0.9582  & 0.9591  &   0.9599  &   0.9608  &   0.9616  & 0.9625  &   0.9633 \\
{\bf 1.8} &   0.9641  &   0.9649  &   0.9656  &   0.9664  & 0.9671  &   0.9678  &   0.9686  &   0.9693  & 0.9699  &   0.9706 \\
{\bf 1.9} &   0.9713  &   0.9719  &   0.9726  &   0.9732  & 0.9738  &   0.9744  &   0.9750  &   0.9756  & 0.9761  &   0.9767 \\
{\bf 2.0} &   0.9772  &   0.9778  &   0.9783  &   0.9788  & 0.9793  &   0.9798  &   0.9803  &   0.9808  & 0.9812  &   0.9817 \\ \cline{1-11}
\end{tabular}
\end{center}
\end{scriptsize}

## Standardnormalverteilung
\framesubtitle{Beispiele}
\xmpl[Intervallwahrscheinlichkeit]
Es ist $P(Z\leq1)=F(1)=0.8413$. Ferner ist \[P(Z\leq -1)=P(Z\geq1)=1-P(Z\leq 1)=1-F(1)=0.1587.\]
Damit ergibt sich die Wahrscheinlichkeit für Werte im zentralen Schwankungsintervall $-1\leq Z \leq1$ als \[ P(-1\leq Z\leq1)=P(Z\leq1)-P(Z\leq-1)\approx 0.8413-0.1587=0.6826. \]
\endxmpl


## Standardnormalverteilung
\framesubtitle{Beispiele}

\xmpl[*]
```{r}
pnorm(c(-1, 1), mean = 0, sd = 1) # oder pnorm(c(-1, 1))
pnorm(1) - pnorm(-1)
# Symmetrie: Flächeninhalt von -Inf bis 0 entspricht dem von 0 bis Inf
pnorm(0)
```
\endxmpl



## Standardnormalverteilung
\framesubtitle{Beispiele}

\xmpl[Schwankungsintervall]
$X$ ist normalverteilt $N(5,16)$. Standardisieren liefert
\[Z=\frac{X-\mu}{\sigma}=\frac{X-5}{4}.\]
Dann gilt bspw.
\begin{align*}
P(X\geq8)& \stackrel{\eqref{StdNormVtlgWsk}}{=} P\left(Z\geq
\dfrac{8-\mu}{\sigma}\right)=P(Z\geq 0.75) \\
& =1 -P(Z<0.75)=1-F(0.75).
\end{align*}
Für $z=0.75$ erhält man $F(0.75)=P(Z\leq 0.75)=0.7734$.
Die gesuchte Wahrscheinlichkeit beträgt daher
\[ P(X\geq8) = 1-0.7734=0.2266. \]
\endxmpl



## Standardnormalverteilung
\framesubtitle{Beispiele}
\xmpl[*]
```{r}
1 - pnorm(8, mean = 5, sd = 4)
# Oder kürzer:
1 - pnorm(0.75)
```
\mps
Für etwa $P(3\leq X\leq8)$ muss noch die untere Wahrscheinlichkeit abgezogen werden.

```{r}
pnorm(0.75) - pnorm(-0.5) # wegen Standardisierung (3-5)/4=-0.5
pnorm(8, mean = 5, sd = 4) - pnorm(3, mean = 5, sd = 4) # einfacher
```

\endxmpl








## Standardnormalverteilung

\exe
Berechnen Sie für $Z: N(0,1)$ folgende Wahrscheinlichkeiten:

(a) $P(Z\leq 3)$
(b) $P(Z<3)$
(c) $P(Z\leq 2.34)$
(d) $P(Z\geq 1.2)$

\endexe
\exe
Berechnen Sie für $X: N(8,25)$ folgende Wahrscheinlichkeiten:

(a) $P(X\leq 10)$
(b) $P(X\geq 20)$

\endexe



## Standardnormalverteilung
\exe
In einem Produktionsprozess ist das Füllvolumen von Bierflaschen normalverteilt mit einem Erwartungswert von 500 ml und einer Varianz von 25 ml$^2$.

(a) Welche Mindestfüllmenge kann die Firma garantieren, wenn die Rücklaufquote $1\%$ betragen soll?
(b) Wie groß ist die Mindestfüllmenge, wenn man sie mit der Ungleichung von Tschebyscheff abschätzt?

\endexe


## Quantile

- Oft benötigen wir die Werte $x$, die für vorgegebene Wahrscheinlichkeiten die Gleichung \[F(x)=P(X\leq x)=1-\alpha\] erfüllen. Diese nennen wir \hil{$1-\alpha$-Quantile} einer Verteilung. Links von $x$ liegen also $1-\alpha$ der Wahrscheinlichkeitsmasse. Ist $F$ invertierbar, so gilt \[x=F^{-1}(1-\alpha)\]

\vspace*{-0.5cm}

```{r, echo=F, fig.width=0.55}
grid <- seq(-3, 3, .03)
plot(x = grid, y = pnorm(grid), type = "l", xlab = "$z$", ylab = "$F(z)$", ylim = c(0, 1), xaxt = "n", yaxt = "n", col = "tomato3")
segments(x0 = -3, x1 = qnorm(0.75), y0 = 0.75, lty = 3)
segments(x0 = qnorm(0.75), y0 = 0, y1 = 0.75, lty = 3)

axis(1, at = qnorm(0.75), labels = "$F^{-1}(1-\\alpha)$")
axis(2, at = c(0.75, 1), labels = c("$1-\\alpha$", 1))
shape::Arrows(x0 = qnorm(0.75), x1 = qnorm(0.75), y0 = 0.75, y1 = 0, lty = 1, code = 2, arr.type = "triangle", arr.length = 0.09, arr.width = 0.06,
              arr.adj = 1, arr.col = "black", segment = F)
```


## Quantile

- Quantile der Normalverteilung etwa berechnet man in R bequem mit der Funktion \linebreak `qnorm(p, mean, sd)`.

```{r}
# Quantile der Standardnormalverteilung
x <- c(0, 0.25, 0.5, 0.75, 1)
qnorm(p = x, mean = 0, sd = 1)
```

\vspace{0.5cm}
\QuizAcademy{Verteilungen 4}\endQuizAcademy

## $\chi^2$-Verteilung

- Ausgangspunkt sind $n$ Zufallsvariablen $Z_1,\ldots, Z_n$, die unabhängig und $N(0,1)$-verteilt sind. Die neue Zufallsvariable \[ X = Z_1^2 + Z_2^2 + \ldots + Z_n^2 \] folgt einer \hil{$\chi^2$-Verteilung}.
- Die Anzahl der unabhängigen Zufallsvariablen $n$ in dieser Summe wird als \hil{Freiheitsgrad} bezeichnet.
- Erwartungswert und Varianz der $\chi^2$-Verteilung sind
\begin{equation*}
\mu=n \quad \text{und}\quad \sigma^2=2n.
\end{equation*}
- Die Freiheitsgrade $n$ sind also Scharparameter der Verteilung. Daher schreibt man die $\chi^2$-Verteilung kompakt als $\chi^2(n)$.
- Weitere Details finden sich im Buch auf S.\ 135-138.


## $\chi^2$-Verteilung
\framesubtitle{Erzeugen einer $\chi^2$-verteilten ZV in \R}

```{r, fig.width=0.5, echo=TRUE, eval=FALSE}
Wiederholungen <- 20000
FG <- 7 # Freiheitsgrade der chi-Quadrat-Verteilung
Z  <- replicate(Wiederholungen, rnorm(FG)) # 20000 Spalten à 7 N(0,1)-ZVen
X  <- colSums(Z^2) # Spaltensummen der Quadrate

# Histogramm der quadrierten Spaltensummen:
hist(X, freq = F, col="lightblue", breaks = 40, ylab="Dichte", main="")
grid <- seq(0, 60, 0.03) # x-Achse definieren
# theoretische Dichte hinzufügen:
lines(grid, dchisq(x = grid, df = FG), type = 'l', lwd = 2, col="tomato3")
```
\vspace{-1.75em}
```{r, echo=FALSE, fig.width=0.5}
Wiederholungen <- 20000
FG <- 7 # Freiheitsgrade der chi-Quadrat-Verteilung
Z  <- replicate(Wiederholungen, rnorm(FG)) # 20000 Spalten à 7 N(0,1)-ZVen
X  <- colSums(Z^2) # Spaltensummen der Quadrate

# Histogramm der quadrierten Spaltensummen:
hist(X, freq = F, col="lightblue", breaks = 40, ylab="Dichte", main="")
grid <- seq(0, 60, 0.03) # x-Achse definieren
# theoretische Dichte hinzufügen:
lines(grid, dchisq(x = grid, df = FG), type = 'l', lwd = 2, col="tomato3")
```


## $t$-Verteilung

- Bildet man aus unabhängigen Zufallsvariablen $Z:\; N(0,1)$ und $Y:\; \chi^2(n)$ den Quotienten
\begin{equation} X=\frac{Z}{\sqrt{Y/n}}\label{tVtlg},
\end{equation}
dann ist $X$ \hil{{\boldmath{$t$}}-verteilt} mit $n$ Freiheitsgraden.
- Die $t$-Verteilung hängt auch von den Freiheitsgraden $n$ ab; kürze sie daher mit $t(n)$ ab.
- Der \hil{Erwartungswert} ist für $n>1$ definiert und ist dann immer null. Die \hil{Varianz} ist
\begin{equation*}
\sigma^2=\frac{n}{n-2} \quad \text{für } n>2.
\end{equation*}
- Die $t$-Verteilung ist symmetrisch um $\mu=0$, d.h. $F(-x)= 1-F(x)$.



## $t$-Verteilung

```{r tVtlg1, echo=c(1:5), fig.width=0.7,eval=FALSE}
grid <- seq(-3, 3, .01)
plot(x = grid, y = dnorm(grid), type = "l", 
     col = "tomato3", xlab = "", ylab = "", lwd = 2)
lines(x = grid, y = dt(grid, df = 4), col = "dodgerblue3", lwd = 2)
lines(x = grid, y = dt(grid, df = 2), col = "forestgreen", lwd = 2)
lines(x = grid, y = dt(grid, df = 25), col = "black", lty = 1, lwd = 2)
legend(x = 1, y = 0.4, legend = c("$N(0,1)$", "$t(25)$","$t(4)$", "$t(2)$"), lty = 1, col = c("tomato3", "black", "dodgerblue3", "forestgreen"), bty = "n", lwd = 2)
```


```{r tVtlg1x, echo=c(1:5), echo=FALSE, fig.width=0.7}
grid <- seq(-3, 3, .01)
plot(x = grid, y = dnorm(grid), type = "l", 
     col = "tomato3", xlab = "", ylab = "", lwd = 2)
lines(x = grid, y = dt(grid, df = 4), col = "dodgerblue3", lwd = 2)
lines(x = grid, y = dt(grid, df = 2), col = "forestgreen", lwd = 2)
lines(x = grid, y = dt(grid, df = 25), col = "black", lty = 1, lwd = 2)
legend(x = 1, y = 0.4, legend = c("$N(0,1)$", "$t(25)$","$t(4)$", "$t(2)$"), lty = 1, col = c("tomato3", "black", "dodgerblue3", "forestgreen"), bty = "n", lwd = 2)
```


## $t$-Verteilung

- Die Abbildung zeigt $f(x)$ der $t$-Verteilung für $n=\{2, 4, 25\}$ und die Dichte der $N(0,1)$-Verteilung.
- Für kleines $n$ verläuft die $t$-Verteilung flacher als die $N(0,1)$-Verteilung.
- Dieser Unterschied verringert sich aber mit wachsendem $n$, so dass es ab $n\geq 30$ zulässig ist, anstelle der $t$- die $N(0,1)$-Verteilung zu verwenden.


```{r tVtlg2, echo=T}
x <- seq(-5,5)
round(dnorm(x) - dt(x, 30), 3)
```

## $t$-Verteilung
\exe
Finden Sie das folgende Quantil der $t$-Verteilung:
\[t(0.95;30).\]
\endexe


## Anhang
\framesubtitle{Geometrische Verteilung}

- Definitionsgemäß erhält man $\mu$ der geometrischen Verteilung als
\begin{equation*}
 \mu=\sum\limits^\infty_{x=0}x p q^x = p q
 \sum\limits^\infty_{x=1}x q^{x-1}.
\end{equation*}
- Um zu ermitteln, ob die Summe existiert und welchen Wert sie annimmt, wird der Ausdruck $x q^{x-1}$ als Ableitung der Funktion $q^x$ nach $q$ geschrieben
\[\dfrac{d}{dq}(q^x)=x q^{x-1}.\]
- Bei endlichen Summen können die beiden Operationen $\sum$ und $\frac{d}{dp}$ vertauscht werden:
\[ \sum\limits^\infty_{x=1}xq^{x-1}=\sum\limits^\infty_{x=1}\frac{d}{dq}(q^x) =\frac{d}{dq}\left(\sum\limits^\infty_{x=1}q^x\right). \]




## Anhang
\framesubtitle{Geometrische Verteilung}

- Wegen $|q|<1$ existiert $\sum^\infty_{x=1}q^x$; aus der Summenformel für unendliche geometrische Reihen folgt \[\sum\limits^\infty_{x=1}q^x=\frac{q}{1-q}.\]
Die Ableitung dieser Summe lautet \[\frac{d}{dq}\left(\frac{q}{1-q}\right)=\frac{1}{(1-q)^2}.\]
Damit gilt
\begin{equation*}
 \sum\limits^\infty_{x=1}x q^{x-1}=\frac{1}{(1-q)^2}\text{ für }
 |q|<1.
\end{equation*}
- Durch Einsetzen von $\sum\limits^\infty_{x=1}x q^{x-1}=\frac{1}{(1-q)^2}$ in  $\mu=\sum\limits^\infty_{x=0}x p q^x = p q\sum\limits^\infty_{x=1}x q^{x-1}$
Daher folgt für den \hil{Erwartungswert}
\vspace{-0.65em}
\begin{equation*}
 \mu=\sum\limits^\infty_{x=0}x p q^x = p q\sum\limits^\infty_{x=1}x q^{x-1}=\frac{pq}{(1-q)^2}=\frac{q}{p}.
\end{equation*}

## Anhang
\framesubtitle{Geometrische Verteilung}

- Zur Berechnung der \hil{Varianz} folgt man dem Ansatz
\[\sigma^2=\E(X^2)-\mu^2.\]
- Da $\mu$ bereits vorliegt, fehlt nur noch $\E(X^2)$:
\[ \E(X^2)=\sum\limits^\infty_{x=0}x^2 p q^x = p q \sum\limits^\infty_{x=1}x^2 q^{x-1}. \]
- Die letzte Summe lässt sich wieder als Ableitung nach $q$ darstellen
\[ \sum\limits^\infty_{x=1}x^2 q^{x-1}=\frac{d}{dq} \left(\sum\limits^\infty_{x=1}x q^x\right)=\frac{d}{dq}\left( q\sum_{x=1}^\infty xq^{x-1}\right). \]

## Anhang
\framesubtitle{Geometrische Verteilung}

- Ersetzt man jetzt die rechte Summe durch Gleichung
\[\sum\limits^\infty_{x=1}x q^{x-1}=\frac{1}{(1-q)^2},\]
folgt
\begin{equation*}
 \sum_{x=1}^\infty x^2q^{x-1} =
 \frac{d}{dq}\left[\frac{q}{(1-q)^2}\right] = \frac{1+q}{(1-q)^3}
 \text{ für } |q|<1.
\end{equation*}
- Also ist
\[ \E(X^2)=pq\frac{1+q}{(1-q)^3}=\frac{q(1+q)}{(1-q)^2}. \]
- Die Varianz der geometrischen Verteilung folgt jetzt als
\[\sigma^2 = \E(X^2) - \mu^2 = \frac{q(1+q)}{(1-q)^2} -  \frac{q^2}{p^2} = \frac{q}{(1-q)^2}= \frac{q}{p^2}.\]



## Anhang
\framesubtitle{Hypergeometrische Verteilung}

- Für Erwartungswert und Varianz einer hypergeometrischen Verteilung ist folgende Umformung des Binomialkoeffizienten hilfreich. Es gilt
\begin{equation*}
 \binom{M}{x}=\dfrac{M!}{x!(M-x)!}  =\dfrac{M(M-1)!}{x(x-1)!(M-x)!}=\dfrac{M}{x}\binom{M-1}{x-1}.
\end{equation*}
- Der Erwartungswert kann dann wie folgt aus seiner Definitionsgleichung ermittelt werden \[\mu = \sum_{x=0}^n x\frac{\binom{M}{x}\binom{N-M}{n-x}}{\binom{N}{n}} = \sum_{x=1}^n x
\frac{\frac{M}{x}\binom{M-1}{x-1}\binom{N-M}{n-x}}{\frac{N}{n}\binom{N-1}{n-1}} = n\frac{M}{N}\sum_{x=1}^n
\frac{\binom{M-1}{x-1}\binom{N-M}{n-x}}{\binom{N-1}{n-1}}.\]





## Anhang
\framesubtitle{Hypergeometrische Verteilung}

- $M/N$ stellt die Wahrscheinlichkeit dar, dass beim ersten Zug ein Element mit der Eigenschaft $A$ eintritt.
- Diese Wahrscheinlichkeit beträgt $p$. Setzt man $z=x-1$, kann die letzte Summe geschrieben werden
\[ \sum_{x=1}^n \frac{\binom{M-1}{x-1}\binom{N-M}{n-x}}{\binom{N-1}{n-1}} = \sum_{z=0}^{n-1} \frac{\binom{M-1}{z}\binom{N-M}{n-1-z}}{\binom{N-1}{n-1}}. \]
- Der letzte Bruch ist aber die Wahrscheinlichkeitsfunktion einer hypergeometrisch verteilten Zufallsvariablen $Z$ mit den Parametern $N-1,$ $M-1$  und $n-1: f(z|N-1, M-1, n-1)$.
- Da ihre Summe über $z=0,\ldots,n-1$ gleich eins ist, ist der Erwartungswert $\mu$ einer hypergeometrisch verteilten Zufallsvariablen
\begin{equation*}
 \mu=np.
\end{equation*}


## Anhang
\framesubtitle{Hypergeometrische Verteilung}

- Forme zur Ermittlung der \hil{Varianz} den Verschiebungssatz um:
\[ \sigma^2=\E(X^2)-[\E(X)]^2=\E[X(X-1)]+\E(X)-[\E(X)]^2. \]
- Beachtet man, dass außerdem gilt
\[ \binom{M}{x}=\dfrac{M(M-1)}{x(x-1)}\binom{M-2}{x-2}\quad
\text{und} \quad \binom{N}{n}=\dfrac{N(N-1)}{n(n-1)}\binom{N-2}{n-2}, \]
ergibt sich
\[ \E[X(X-1)]=\sum\limits^n_{x=0}x(x-1)\dfrac{\binom{M}{x}\binom{N-M}{n-x}}{\binom{N}{n}}= \dfrac{M(M-1)}{\frac{N(N-1)}{n(n-1)}}\sum\limits^n_{x=2}\dfrac{\binom{M-2}{x-2}\binom{N-M}{n-x} }{\binom{N-2}{n-2}}. \]
- Setzt man jetzt $z=x-2$ und stellt den Bruch vor dem letzten Summenzeichen um, erhält man
\[ \E[X(X-1)]=\dfrac{p(M-1)n(n-1)}{(N-1)}\sum\limits^{n-2}_{z=0}\dfrac{\binom{M-2}{z} \binom{N-M}{n-2-z}}{\binom{N-2}{n-2}}, \text{ mit } p=\frac{M}{N}.
\]


## Anhang
\framesubtitle{Hypergeometrische Verteilung}

- Der letzte Bruch ist analog zu oben die Wahrscheinlichkeitsfunktion einer hypergeometrisch verteilten Zufallsvariablen mit $N-2$, $M-2$ und $n-2$. Da auch ihre Summe eins ist, resultiert
\[ \E[X(X-1)]=\dfrac{p(M-1)n(n-1)}{(N-1)}=\dfrac{n p(pN-1)(n-1)}{N-1}. \]
- Es sind nun alle Terme bestimmt, so dass
\[ \sigma^2=\dfrac{n p(p N-1)(n-1)}{N-1}+np-n^2p^2,\]
oder nach einigen Umformungen
\[  \sigma^2=np(1-p)\dfrac{N-n}{N-1}.\]


# Grundzüge der Stichprobentheorie

## Stichproben und Stichprobenfunktion
\framesubtitle{Grundgesamtheit}

- Enthält eine Grundgesamtheit zu viele Elemente, sind \hil{Stichproben} notwendig.
- Eine Grundgesamtheit kann endlich oder unendlich viele Elemente enthalten:
  - Theoretische Grundgesamtheiten sind oft (überabzählbar) unendlich, wie z.B. bei stetigen Zufallsvariablen.
  - Reale Grundgesamtheiten sind meistens sehr groß, aber häufig endlich (z.B. deutsche Bevölkerung).


## Stichproben und Stichprobenfunktion


- Bei unbekannter Verteilung in der Grundgesamtheit muss jede statistische Information hierüber durch Stichproben erhoben werden.
- Das \hil{Ziehen einer Stichprobe} besteht darin, $n$ Elemente aus der Grundgesamtheit auszuwählen.
- So entsteht eine Stichprobe mit dem Umfang oder der Länge $n$.
- Um die Wahrscheinlichkeitstheorie für die Stichprobenanalyse nutzbar zu machen, muss das \hil{Auswahlverfahren} randomisiert sein.
- Bei zufälliger Auswahl liegen \hil{Zufallsstichproben} vor.


## Stichproben und Stichprobenfunktion
\framesubtitle{QuizAcademy}
\QuizAcademy{Stichproben 1}\endQuizAcademy


## Stichproben und Stichprobenfunktion
- Jede zufällige Ziehung aus der Grundgesamtheit kann als eigenständige Zufallsvariable $X_1,\ldots,X_n$ aufgefasst werden.
- Ihre Verteilung hängt von der Verteilung von $X$ in der Grundgesamtheit ab.
- Die $X_j$ bezeichnet man als \hil{Stichprobenvariablen}.
- Wird das Element nach Ziehung wieder in die \glqq{}Urne\grqq{} zurückgelegt, spricht man von \hil{Zufallsstichproben mit Zurücklegen}.
- Geschieht dies nicht, liegen \hil{Zufallsstichproben ohne Zurücklegen} vor (von denen wir im Folgenden abstrahieren wollen).
- Je größer die Grundgesamtheit, desto bedeutungsloser die Unterscheidung ZmZ/ZoZ.


## Stichproben und Stichprobenfunktion
- Eine konkrete Zufallsstichprobe, auch als \hil{Stichprobenrealisation} bzw. \hil{Stichprobenergebnis} bezeichnet, sind also die Zahlen, die sich aus den Realisationen der $X_j$ ergeben, \[ X_1=x_1, \quad X_2=x_2,\quad\ldots,X_n=x_n \] oder als $n$-Tupel geschrieben: $(x_1,\ldots,x_n).$
- Alle möglichen Stichproben stellen die Ausgänge und diese zusammen den \hil{Stichprobenraum} des Zufallsvorgangs \glqq{}Ziehen einer Stichprobe\grqq{} dar.


## Stichproben und Stichprobenfunktion
\rem[Uneingeschränkte Zufallsstichprobe]
Jedes Element der Grundgesamtheit besitzt dieselbe Wahrscheinlichkeit, in eine Zufallsstichprobe mit dem Umfang $n$ zu gelangen.
\endrem

- ZmZ und ZoZ liefern immer \hil{uneingeschränkte Zufallsstichproben} und die $X_j$ sind identisch verteilt wie $X$.
- Bei ZmZ sind die Stichprobenvariablen zusätzlich stochastisch unabhängig. Man erhält dann  \hil{unabhängige Zufallsstichproben}.
- Uneingeschränkte und unabhängige Zufallsstichproben heißen \hil{einfache Zufallsstichproben}.


## Stichproben und Stichprobenfunktion

- Eine Funktion $T=T(X_1,\ldots,X_n),$ $n\geq 1$, die nur von Stichprobenvariablen, nicht aber von unbekannten Parametern abhängt, heißt \hil{Stichprobenfunktion} bzw. \hil{Statistik}.
- Stichprobenfunktionen sollen Stichprobenergebnisse in aussagekräftige, einfache statistische Größen überführen.
- So stellen
\[ T=\frac{1}{n}\sum\limits^n_{j=1} X_j \ \ \text{oder} \ \ T=X_1-5 \]
Stichprobenfunktionen bzw. Statistiken dar, nicht aber \[Z=\frac{X-\mu}{\sigma},\] wenn $\mu$ und $\sigma$ unbekannt sind.


## Stichproben und Stichprobenfunktion

- Da die Argumente von $T$ Zufallsvariablen sind, ist $T$ selbst eine Zufallsvariable, deren Verteilung \hil{Stichprobenverteilung} heißt.
- Die Varianz der Stichprobenverteilung nennt man \hil{Fehlervarianz}, die positive Wurzel hieraus \hil{Standardfehler}.
- Eine Realisation von $T$ aufgrund einer konkreten Stichprobe wird mit
 \[t=T(x_1,\ldots,x_n)\]
 bezeichnet.
- Im Folgenden betrachten wir einige spezielle Stichprobenverteilungen.


## Stichprobenverteilung des arithmetischen Mittels
- Ziehen aller möglicher Stichproben aus einer Grundgesamtheit und jeweiliges Berechnen der Stichprobenfunktion arithmetisches Mittel, $\bar{X}=\frac{1}{n}\sum^n_{j=1} X_j$, liefert dessen \hil{Stichprobenverteilung}.
- Wir betrachten zunächst Erwartungswert und Varianz der Verteilung.
- Der Erwartungswert von $\bar{X}$ ist
\begin{equation} \E(\bar{X})=\E\left(\frac{1}{n}\sum_{j=1}^n X_j\right) = \frac{1}{n}\sum_{j=1}^n \E(X_j)= \frac{n}{n}\mu=\mu,
\label{EWMWsrs}
\end{equation}
eine Vereinfachung von \eqref{EW}, da nun wegen der identischen Verteilung alle Erwartungswerte gleich sind.


## Stichprobenverteilung des arithmetischen Mittels

- Bei einfachen Stichproben erhält man die Fehlervarianz als
\begin{align}
\var(\bar{X}) =\sigma^2_{\bar{X}}&=\var\left(\frac{1}{n}\sum_{j=1}^n X_j\right)
\stackrel{\eqref{varlinTrafo}}{=} \frac{1}{n^2}\sum_{j=1}^n\var(X_j) \notag\\
& =\frac{1}{n^2}\sum_{j=1}^n
\sigma^2 =\frac{1}{n}\sigma^2.\label{varMWsrs}
\end{align}

- Um die Verteilung von $\bar{X}$ zu bestimmen, muss unterschieden werden, ob die Verteilung für $X$ in der Grundgesamtheit bekannt ist oder nicht.
- Ist $X$ \hil{normalverteilt} mit $\mu$ und $\sigma^2$, dann ist aufgrund der Reproduktivitätseigenschaft der Normalverteilung auch $\bar{X}$ normalverteilt mit (bei Unabhängigkeit) den oben abgeleiteten Parametern
\[ \mu_{\bar{X}}=\mu \quad \text{und} \quad \sigma^2_{\bar{X}}=\frac{\sigma^2}{n}. \]


## Stichprobenverteilung des arithmetischen Mittels
\framesubtitle{Beispiele}
\xmpl[Körpergröße deutscher Männer]
Die Körpergröße deutscher Männer sei in der Grundgesamtheit normalverteilt mit $\mu=178$ cm und $\sigma^2=64\text{ cm}^2$. Das Stichprobenmittel $\bar{X}$ ist dann normalverteilt mit $\mu_{\bar{X}}=178$ und $\sigma^2_{\bar{X}}=64/n$. Wird eine Stichprobe mit dem Umfang $n=100$ gezogen, ist $\sigma^2_{\bar{X}}=64/100=0.64$.\mps
Die Wahrscheinlichkeit, dass eine Stichprobe einen Mittelwert von $\bar{x}=180$ oder größer liefert, ist mit `1-pnorm(180, 178, sqrt(0.64))` `r paste0("$=", round(1-pnorm(180,178, sqrt(0.64)), 4), "$")`
\begin{align*}
  P(\bar{X}\geq 180) & =
  %P\left(Z\geq\frac{180-\mu}{\sigma/\sqrt{n}}\right)=P\left(Z\geq \frac{180-178}{0.8}\right)\\
  %& =P(Z\geq2.5) = 1-P(Z<2.5)=1-0.9938=
  0.0062.
 \end{align*}
\endxmpl


## Stichprobenverteilung des arithmetischen Mittels


\xmpl[*]
```{r, fig.width=0.5}
mu   <- 178; sigma <- 8; N <- 100 # EW, Standardabw., Stichprobengröße
grid <- seq(mu - 3, mu + 3, .01)
# ziehe 5000 Stichproben der Größe 100 und berechne deren Stichprobenmittel:
est  <- colMeans(replicate(5000, rnorm(N, mu, sigma)))
hist(est, freq = F, col = "lightblue", breaks = 40, ylab = "Dichte", main = "")
lines(x = grid, y = dnorm(grid, mu, sigma/sqrt(N)), col = "tomato3", lwd = 2)
```
\endxmpl



## Stichprobenverteilung des arithmetischen Mittels
\framesubtitle{Aufgaben}
\exe
Die vom Unternehmen A hergestellten Glühlampen besitzen eine normalverteilte Lebensdauer mit einem Erwartungswert von 800 Stunden
und einer Standardabweichung von 40 Stunden.\mps
Eine Geschäftsfiliale bekommt eine Lieferung von 100 Glühlampen. Wie groß ist die Wahrscheinlichkeit, dass die durchschnittliche
Lebensdauer weniger als 790 Stunden beträgt?
\endexe



## Das Gesetz der großen Zahlen
Was passiert mit dieser Verteilung, wenn $n$ sehr groß wird?
Betrachte hierzu $\bar{X}$ als Folge von Zufallsvariablen und schreibe deswegen $\bar{X}(n)$.
\defn[Schwaches Gesetz der großen Zahlen]\label{GdgZ}
Das \hil{schwache Gesetz der großen Zahlen} lautet
\[ \lim\limits_{n\rightarrow\infty} P[|\bar{X}(n)-\mu|>\varepsilon] =0 \quad \text{bzw.} \quad \lim\limits_{n\rightarrow\infty} P[|\bar{X}(n)-\mu|<\varepsilon] =1. \]
\enddefn
\vspace{0.5cm}
Mehr hierzu im Buch auf S.\ 171-176.



## Zentraler Grenzwertsatz von Lindeberg-Levy

Der \hil{zentrale Grenzwertsatz von Lindeberg-Levy} liefert bei \hil{unbekannter Verteilung} von $X$ eine Approximation der Verteilung von $\bar{X}$.
\defn[Zentraler Grenzwertsatz von Lindeberg-Levy]
Die Verteilung der standardisierten Summe und des standardisierten Durchschnitts, also
\begin{equation}\label{ZGWS}
\frac{\tilde{S}(n)-n\mu}{\sqrt{n}\sigma} \quad \text{bzw.}\quad \frac{\bar{X}(n)-\mu}{\sigma/\sqrt{n}} \to_DN(0,1),
\end{equation}
d.h., konvergiert für $n\rightarrow\infty$ gegen die \hil{Standardnormalverteilung} $N(0,1)$.
\enddefn
\vspace{0.5cm}
Dieser basiert auf der Annahme, dass $X_1,\ldots,X_n$ unabhängig identisch verteilt sind mit $\E(X_j)=\mu$ und $\var(X_j)=\sigma^2>0$ für alle $j$. Mehr nicht!



## Zentraler Grenzwertsatz von Lindeberg-Levy
Aus dem zentralen Grenzwertsatz von \hil{Lindeberg-Levy} folgt daher für ein großes $n$, dass

  (a) die Summe $\tilde{S}(n)$ asymptotisch normalverteilt $N(n \mu, n \sigma^2)$ und
  (b) der Durchschnitt $\bar{X}(n)$ asymptotisch normalverteilt $N\left(\mu, \sigma^2/n\right)$

ist.
Siehe \texttt{clt.R}.



## Zentraler Grenzwertsatz von Lindeberg-Levy
\framesubtitle{Aufgaben}
\exe
Die maximale Traglast eines der beiden Besucheraufzüge im Reichstagsgebäude in Berlin beträgt 4000 kg.
Nach Herstellerangaben entspricht dies einer Kapazität von 45 Personen.
Das Gewicht der Personen, die den Aufzug benutzen, beträgt im Durchschnitt 88 kg bei einer Varianz von 16 kg$^2$.\mps
Wie groß ist die Wahrscheinlichkeit, dass bei 45 Personen die maximale Traglast überschritten wird?
\endexe



## Zentraler Grenzwertsatz von Lindeberg-Levy
\framesubtitle{Aufgaben}
\exe
2500 Unternehmensberater wurden bzgl. ihres Jahreseinkommens befragt. Es ergab sich ein Durchschnittseinkommen von 51.800 Euro mit einer Standardabweichung von 4000 Euro.\mps Wie groß ist die Wahrscheinlichkeit, dass bei einer Stichprobe im Umfang von $n=30$ der Mittelwert um höchstens 500 Euro vom Erwartungswert abweicht?
\endexe



## Stichprobenverteilung des Anteilswertes

Siehe Buch ab S.\ 192.



## \large Verteilung von Quotienten aus Stichprobenfunktionen

- Aus \eqref{stdNormalTrafo} folgt, dass der standardisierte Mittelwert
\begin{equation}\label{ZN01}
Z=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\sim N(0,1),
\end{equation}
also $N(0,1)$-verteilt ist, wenn $X$ normalverteilt ist.
- Dieses Ergebnis setzt aber die Kenntnis der Varianz $\sigma^2$ von $X$ in der Grundgesamtheit voraus. Ist $\sigma^2$ \hil{unbekannt}, ist $Z$ keine Stichprobenfunktion mehr.
- Ersetzt man $\sigma^2$ durch die \hil{Stichprobenvarianz} \[S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2,\] geht $Z$ über in
\[ Z(t)=\frac{\bar{X}-\mu}{S/\sqrt{n}}. \]


## \large Verteilung von Quotienten aus Stichprobenfunktionen

- $Z(t)$ ist jetzt aber nicht mehr standardnormalverteilt.
- $Z(t)$ ist bei bekanntem $\mu$ eine Stichprobenfunktion.
- Um das Verteilungsgesetz von $Z(t)$ zu entwickeln, erweitere
\[Z(t)=\frac{\bar{X}-\mu}{S/\sqrt{n}}=\frac{\overset{N(0,1),\; s.~\eqref{ZN01}}{\overbrace{\sqrt{n}\frac{\bar{X}-\mu}{\sigma}}}}{
  \sqrt{\underset{\chi^2}{\underbrace{{\frac{(n-1)S^2}{\sigma^2}}}}\Bigl/n-1}}.
 \label{tVert}\]
- Der Zähler $Z$ in $Z(t)$ ist standardnormalverteilt.
- $(n-1)S^2/\sigma^2$ ist bei normalverteiltem $X$ $\chi^2$-verteilt mit $n-1$ Freiheitsgraden.


## \large Verteilung von Quotienten aus Stichprobenfunktionen

- Wir erinnern uns (vgl.\ \eqref{tVtlg}), dass der Quotient aus einer $N(0,1)$- und der Wurzel einer $\chi^2$-verteilten Zufallsvariablen geteilt durch ihre Freiheitsgerade \hil{$t$-verteilt} ist, sofern die ZVen \hil{unabhängig} sind.
- Da $\bar{X}$ und $S^2$ auf derselben Stichprobe beruhen, hängt $S^2$ von $\bar{X}^2$ ab. Die beiden Zufallsvariablen $\bar{X}$ und $S^2$ sind daher i.A.\ stochastisch abhängig.
- Eine Ausnahme liegt vor, wenn $X$ in der Grundgesamtheit \hil{normalverteilt} ist: Dann sind $\bar{X}$ und $S^2$ unabhängig.
- Ist also $X$ in der Grundgesamtheit normalverteilt, dann folgt $Z(t)$ einer $t$-Verteilung mit $n-1$ Freiheitsgraden.


## \large Verteilung von Quotienten aus Stichprobenfunktionen
\framesubtitle{Beispiel}
\xmpl
Aus einer normalverteilten Grundgesamtheit mit $\mu=10$ und unbekannter Varianz wird eine Stichprobe mit $n=9$ gezogen.\mps
Die Stichprobenvarianz sei $s^2=9$.
Die Variable
\[ Z(t)=\frac{\bar{X}-10}{3/\sqrt{9}}=\bar{X}-10 \]
ist $t$-verteilt mit $n-1=8$ Freiheitsgraden.
\endxmpl



## \large Verteilung von Quotienten aus Stichprobenfunktionen
\framesubtitle{QuizAcademy}
\QuizAcademy{Stichproben 2}\endQuizAcademy



## \large Verteilung von Quotienten aus Stichprobenfunktionen

\xmpl
Das Stichprobenmittel für $n=9$ betrage $\bar{x}=12$, so dass $Z(t)=2$. Die Wahrscheinlichkeit dafür, dass $\bar{X}$ kleiner gleich 12 ist, beträgt
\[ P(\bar{X}\leq 12)=P[Z(t)\leq 2]. \]
$P[Z(t)\leq 2]$ ermitteln wir in \R durch `r runidue::s(pt(2, 8))`.
\endxmpl



## \large Verteilung von Quotienten aus Stichprobenfunktionen

- Für ein großes $n$ ist die zuvor vorgestellte Vorgehensweise auch bei \hil{unbekannter Verteilung} von $X$ und unbekannter Varianz gültig.
- Aufgrund des Gesetzes der großen Zahlen lässt sich die Varianz für großes $n$ gut über die Stichprobenvarianz approximieren, also $S^2\approx\sigma^2$.
- Also gilt \[ Z(t)=\frac{\bar{X}-\mu}{S/\sqrt{n}}\approx\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\sim N(0,1), \]
d.h.\ wir verwenden den Zentralen Grenzwertsatz.




<!-- % Kürzungen gegenüber kompletter Fassung SoSe 2014: -->
<!-- % -->
<!-- % - Anteilswerte -->
<!-- % - MSE kürzer -->
<!-- % - Anhänge -->

# Statistische Schätzverfahren

## Statistische Schätzverfahren


- Bei vielen empirischen Untersuchungen sind die Parameter in der Grundgesamtheit unbekannt.
- Die \hil{statistische Schätztheorie} befasst sich mit Möglichkeiten, diese Parameter aus Stichproben zu schätzen.
- Die Quantifizierung der Sicherheit solcher Schätzungen nutzt die Ergebnisse der Stichprobentheorie.
- Die Schätzung eines unbekannten Parameters $\theta$ einer Grundgesamtheit wird mit einer Stichprobenfunktion durchgeführt, die jetzt \hil{Schätzfunktion} oder \hil{Schätzer} heißt.
- Schätzer haben also auch eine Verteilung.




## Statistische Schätzverfahren
\framesubtitle{Eigenschaften von Schätzern}

- Die Schätzfunktion lautet \[ \hat{\theta}=T_{\theta} (X_1,\ldots,X_n).\] $\hat{\theta}$ ist dann eine Zufallsvariable mit Schätzwerten, die unterschiedlich stark von $\theta$ abweichen.
- Einsetzen von Stichprobenrealisationen in den Schätzer liefert einen konkreten Schätzwert. Dieser Schätzwert heißt \hil{Punktschätzung}.
- Eine \hil{Intervallschätzung} liefert eine Intervallvorschrift, die mit vorgegebener Wahrscheinlichkeit den unbekannten Parameter $\theta$ enthält.


## Statistische Schätzverfahren
\framesubtitle{Eigenschaften von Schätzern}


- Wir suchen Schätzer, die bestimmte Qualitätskriterien erfüllen.
- Die Abweichung $\hat{\theta}-\theta$ heißt \hil{Schätzfehler}.
- Eine wünschenswerte Eigenschaft ist, dass der Erwartungswert des Schätzfehlers für jeden Stichprobenumfang $n$ null ist.
- Formal: \[ \E(\hat{\theta}_n-\theta)=0 \quad \text{oder}\quad \E(\hat{\theta}_n)=\theta \quad  \text{für alle } n. \]
- Ein Schätzer $\hat{\theta}_n$, die dieses Kriterium erfüllt, heißt \hil{erwartungstreu}, \hil{unverzerrt} oder \hil{unbiased}.
- Andernfalls \hil{nicht erwartungstreu} bzw. \hil{verzerrt}.
- Es gibt oft mehrere erwartungstreue Schätzer.
- Aus der Klasse erwartungstreuer Schätzer liegt es nahe, den mit der \hil{kleinsten Varianz} zu bestimmen.




## Statistische Schätzverfahren
\framesubtitle{Eigenschaften von Schätzern - QuizAcademy}
\QuizAcademy{Schätzfrage}\endQuizAcademy



## Statistische Schätzverfahren
\framesubtitle{Eigenschaften von Schätzern}

- Hier sind die Stichprobenverteilungen für zwei erwartungstreue Schätzer $\hat{\theta}_{1,n}$ und $\hat{\theta}_{2,n}$ dargestellt.
$\hat{\theta}_{1,n}$ ist effizienter als $\hat{\theta}_{2,n}$, da $\var(\hat{\theta}_{2,n})>\var(\hat{\theta}_{1,n})$.


```{r, echo = F, fig.width=0.7}
grid <- seq(0, 8, .03)
plot(x = grid, y = dnorm(grid, 4, 1), type = "l", col = "tomato3", xaxt = "n", yaxt = "n", xlab = "$\\hat{\\theta}_{1,n},\\hat{\\theta}_{2,n}$", ylab = "$f(\\hat{\\theta})$", lwd=2)
lines(x = grid, y = dnorm(grid, 4, 2), col = "dodgerblue3", lwd=2)
segments(x0 = 4, y0 = 0, y1 = dnorm(4, 4, 1), lty = 3)
axis(1, at = 4, labels = "$\\theta$")
legend(x = 6, y = 0.4, legend = c("$\\hat{\\theta}_{1,n}$", "$\\hat{\\theta}_{2,n}$"),
       lty = c(1, 1), col = c("tomato3", "dodgerblue3"), bty = "n", lwd=2)
```


## Statistische Schätzverfahren
\framesubtitle{Eigenschaften von Schätzern}

- Den Vorteil einer erwartungstreuen und effizienten Schätzung kann man analytisch zeigen. Die Tschebyscheff-Ungleichung \eqref{Tschebyscheff1} liefert \[ P(|X-\mu|\geq \varepsilon)\leq \frac{\sigma^2}{\varepsilon^2}. \]
- Setzt man $X=\hat{\theta}$, $\mu=\E(\hat{\theta})=\theta$ und $\sigma^2=\var(\hat{\theta})$ folgt
\[ P(|\hat{\theta}-\theta|\geq\varepsilon)\leq \dfrac{\var(\hat{\theta})}{\varepsilon^2}. \]
- Je kleiner $\var(\hat{\theta})$, desto geringer wird die Wahrscheinlichkeit, dass der absolute Schätzfehler $|\hat{\theta}-\theta|$ größer als $\varepsilon$ ist.
- Daher ist $\hat{\theta}_{1,n}$ dem Schätzer $\hat{\theta}_{2,n}$ vorzuziehen.


## Statistische Schätzverfahren
\framesubtitle{Eigenschaften von Schätzern}

- Eine weitere wünschenswerte Eigenschaft von erwartungstreuen Schätzern ist die Konvergenz der Varianz mit zunehmendem $n$ gegen null.
- Für $n\rightarrow\infty$ folgt
\[ \lim\limits_{n\rightarrow\infty} P(|\hat{\theta}_n-\theta|\geq\varepsilon)=0 \quad \text{für } \lim\limits_{n\rightarrow\infty}\var(\hat{\theta}_n)=0. \]
- Schätzfunktionen mit der Eigenschaft \[\lim\limits_{n\rightarrow\infty} P(|\hat{\theta}_n-\theta|<\varepsilon)=1 \] heißen (schwach) \hil{konsistent}.
- Diese Eigenschaft entspricht formal dem Gesetz der großen Zahlen (siehe\ \ref{GdgZ}).




## Statistische Schätzverfahren
\framesubtitle{Eigenschaften von Schätzern}

- Bei verzerrten Schätzern ist der Erwartungswert des Schätzfehlers nicht null: \[\E(\hat{\theta}_n) \neq\theta.\]
- Die Differenz $\E(\hat{\theta}_n)-\theta$ misst die \hil{Verzerrung}.
- Bei einigen verzerrten Schätzern stellt sich für $n\rightarrow\infty$ Erwartungstreue ein: \[ \lim\limits_{n\rightarrow\infty}\E(\hat{\theta}_n - \theta)=0 \quad \text{bzw.}\quad \lim\limits_{n\rightarrow\infty} \E(\hat{\theta}_n) =\theta. \]
- Diesen Fall nennt man \hil{asymptotische Erwartungstreue}.
- Auch asymptotisch erwartungstreue Schätzer können konsistent sein.
- Betrachte hierzu den \hil{mittleren quadratischen Fehler} (MSE)
\[ \text{MSE}(\hat{\theta}_n)=\E[(\hat{\theta}_n-\theta)^2]. \]


## Statistische Schätzverfahren
\framesubtitle{Mean Squared Error}

- Das Buch auf S.\ 214 zeigt, dass der mittlere quadratische Fehler die Summe von Varianz $\var(\hat{\theta}_n)$ und quadrierter Verzerrung $[\E(\hat{\theta}_n)-\theta]^2$ ist:
\[ \text{MSE}(\hat{\theta}_n)=\var(\hat{\theta}_n)+[\E(\hat{\theta}_n)-\theta]^2. \]
- Der verzerrte Schätzer ist konsistent, wenn gilt
\[ \lim\limits_{n\rightarrow\infty}\text{MSE}(\hat{\theta}_n)= \lim\limits_{n\rightarrow\infty} \var(\hat{\theta}_n)+ \lim\limits_{n\rightarrow\infty}[\E(\hat{\theta}_n)-\theta]^2 =0. \]
- Da weder die Varianz noch die quadrierte Verzerrung negativ werden können, müssen beide Grenzwerte null sein.
- Damit ist eine hinreichende Bedingung für \hil{Konsistenz} gefunden. \[ \text{\blue{(a)}} \;\lim\limits_{n\rightarrow\infty}[\E(\hat{\theta}_n)-\theta]^2=0 \quad \text{und}\quad \text{\blue{(b)}} \; \lim\limits_{n\rightarrow\infty} \var(\hat{\theta}_n)=0 \]




## Intervallschätzung
\framesubtitle{Mean Squared Error}
\xmpl[$\bar{X}$ als Schätzer für $\mu$]
$\bar{X}$ ist ein erwartungstreuer Schätzer für $\mu$. Der $\text{MSE}(\bar{X})$ ist daher gleich der Varianz,
\begin{align*}
\text{MSE}(\bar{X})&= \var(\bar{X})\\&\stackrel{\eqref{varMWsrs}}{=}\frac{\sigma^2}{n}.
\end{align*}

\endxmpl



## Statistische Schätzverfahren
\framesubtitle{Konsistenz}

\xmpl[Werfen eines Würfel]
Die ZV $X$ sei die realisierte Augenzahl beim Werfen eines Würfels. Es gilt
\begin{align*}
\E(\bar{X}) = \frac{1}{n}\sum_{j=1}^n \E(X_j) = \E(X_j) = \mu \stackrel{\eqref{EWWuerfel}}{=} 3.5
\end{align*}
sowie
\begin{align*}
\MSE(\bar{X}) = \var(\bar{X}) = \frac{\sigma^2}{n} \stackrel{\eqref{VarWuerfel}}{\approx} \frac{2.9167}{n}
\end{align*}
```{r}
n    <- 300 # 300 Würfe
Xbar <- cumsum(sample(1:6, size = n, replace = T)) / 1:n # Kumulierte Mittel
head(Xbar) 
```
\endxmpl




## Statistische Schätzverfahren
\framesubtitle{Konsistenz}

\xmpl[*]

```{r, echo = F, fig.width=0.49, fig.show="hold", fig.asp=0.8}
plot(x = 1:25, y = Xbar[1:25], type = "l", xlab = "$n$", col = "tomato3", ylim=c(1, 6), ylab="$\\bar{X}$", yaxt="n", lwd=2)
axis(side = 2, at = c(1:3, 3.5, 4:6), labels = c(1:3, "$\\mu$", 4:6))
abline(h = 3.5, lty = 2)
text(x = 12.5, y = 5.5, labels = paste0("$\\text{MSE}(\\bar{X}(25)) \\approx ", round(35/12 / 25, 4), "$"))
plot(x = 1:n, y = Xbar, type = "l", col = "tomato3", ylim = c(1, 6), xlab="$n$", ylab = "$\\bar{X}$", yaxt = "n", lwd=2)
axis(side = 2, at = c(1:3, 3.5, 4:6), labels = c(1:3, "$\\mu$", 4:6))
abline(h = 3.5, lty = 2)
text(x = 150, y = 5.5, labels = paste0("$\\text{MSE}(\\bar{X}(300)) \\approx", round(35/12 / n, 4), "$"))
```
\endxmpl


## Statistische Schätzverfahren
\framesubtitle{Konsistenz}

- Man spricht dann von \hil{Konvergenz im quadratischen Mittel}.
- Im quadratischen Mittel konvergente Schätzer sind immer auch mindestens asymptotisch erwartungstreu.
- Konsistenz ist eine Mindestanforderung für einen guten Schätzer.

\xmpl[Verzerrter, aber konsistenter Schätzer]
Das Mittel $\widetilde{X} = \frac{1}{n+3}\sum_{i=1}^n X_i$ ist ein verzerrter, jedoch konsistenter Schätzer für den Erwartungswert.
\begin{align*}
  \E(\widetilde{X}) = \frac{n}{n+3}\mu \neq \mu
\end{align*}
\endxmpl



## Statistische Schätzverfahren
\framesubtitle{Konsistenz}
\xmpl[*]
Überprüfung der Bedingung \blue{(a)}:
\begin{align*}
  \lim\limits_{n\rightarrow\infty}[\E(\widetilde{X}) - \mu]^2 & = \lim\limits_{n\rightarrow\infty}\left(\frac{n^2}{(n+3)^2} - 2\frac{n}{n+3} + 1\right)\cdot \mu^2 = 0\\
\end{align*}
Überprüfung der Bedingung \blue{(b)}:
\begin{align*}
   \lim\limits_{n\rightarrow\infty}\var(\widetilde{X}) & = \lim\limits_{n\rightarrow\infty}\frac{n}{(n+3)^2} \sigma^2 = 0
\end{align*}
\endxmpl



## Statistische Schätzverfahren
\framesubtitle{Konsistenz}
\xmpl[*]

- In der folgenden Abbildung werden die Verteilungen von $\widetilde{X}$ für verschiedene Stichprobengrößen aus $N(10, 4)$ dargestellt.
- Für die Stichprobenumfänge gilt $n_1<n_2<n_3$.
- Mit zunehmendem $n$ verringern sich Verzerrung und Varianz.
- Die Stichprobenverteilung bewegt sich in Richtung $\mu = 10$.

\endxmpl



## Statistische Schätzverfahren
\framesubtitle{Konsistenz}

\xmpl[*]
```{r KonsSchFun, echo=F, fig.width=0.8}
N <- c(10, 50, 100)

biased <- function(x) {
  sum(x) / (length(x) + 3)
}
cols <- c("tomato3", "dodgerblue3", "forestgreen")
plot(c(6, 11), c(0, 2.1), type = "n", xlab = "", ylab = "", xaxt = "n", main='Konsistente Sch\\"atzfunktion', lwd=2)
for(n in 1:length(N)) {
  smpls  <- replicate(rnorm(N[n], 10, 2), n = 100000)
  xtilde <- apply(smpls, 2, function(x) {
    biased(x)
  })
  lines(density(xtilde), col = cols[n], lwd=2)
}
axis(1, at = 6:11, labels = c(6:9, "$\\mu$", 11))
abline(v = 10, lty = 2)
legend(x = 7, y = 2, legend = paste0("$n_", 1:3," =",N,"$"), lty = rep(1, 3), col = cols, bty = "n")
```
\endxmpl



## Statistische Schätzverfahren
\framesubtitle{Konsistenz}

\xmpl[*]
```{r, eval = F}
# Definiere eigene verzerrte "mean" Funktion
mean_biased <- function(x) {
  sum(x) / (length(x) + 3)
}
# Ziehe 100000 Stichproben
smpls  <- replicate(100000, rnorm(n, 10, 2))
# Berechne für jede Stichprobe das verzerrte Mittel
xtilde <- apply(smpls, 2, function(x) mean_biased(x))
# Plot
hist(xtilde, freq = F, col = "lightblue", breaks = 40, ylab = "Dichte")
abline(v = 10, col = "red", lwd = 2) # Wahres Mittel
```
\endxmpl



## Statistische Schätzverfahren

\exe

Für den unbekannten Erwartungswert einer Grundgesamtheit liegen folgende Schätzfunktionen vor (Annahme: einfache Zufallsstichproben)
\[\hat\mu_1=\frac{3}{7}X_1+\frac{2}{7}X_2+\frac{1}{7}X_3+\frac{1}{7}X_4,\]
\[\hat\mu_2=\frac{1}{8}X_1+\frac{1}{4}X_2+\frac{1}{2}X_3+\frac{1}{8}X_4,\]
\[\hat\mu_3=\frac{1}{4}(X_1+X_2+X_3+X_4)+a, a\neq0.\]

(a) Welche der Schätzfunktionen ist erwartungstreu?
(b) Welche der erwartungstreuen Schätzfunktionen ist die effizienteste?

\endexe



## Konstruktion von Schätzfunktionen
\framesubtitle{Methoden}

- Es gibt verschiedene Vorgehensweisen für die Konstruktion von Schätzern, die jeweils eine Klasse von Schätzverfahren festlegen. 
- Die wichtigsten sind
  (1) die Momentenmethode (vgl. Anhang),
  (2) die Methode der kleinsten Quadrate und
  (3) die Maximum-Likelihood-Methode.


## Methode der kleinsten Quadrate

\xmpl[OLS-Schätzung für $\mu$]
\begin{small}
Der unbekannte Parameter $\mu$ einer Grundgesamtheit soll mit der Methode der kleinsten Quadrate geschätzt werden. Daher können wir schreiben \[ \hat{S}=\sum\limits_{j=1}^n (X_j-\hat{\mu})^2 \longrightarrow \underset{\hat{\mu}}{\text{Min!}}\]
Die erste und zweite Ableitung von $\hat{S}$ nach $\hat{\mu}$ lauten
\[ \dfrac{d\hat{S}}{d\hat{\mu}} = -2\sum_{j=1}^n(X_j-\hat{\mu})=0 \quad \text{und}\quad \dfrac{d^2\hat{S}}{d\hat{\mu}^2}=2n >0 . \]
Die Lösung der ersten Ableitung ergibt den OLS-Schätzer
\[ \hat{\mu}=\dfrac{1}{n} \sum\limits^n_{j=1}X_j=\bar{X}. \] Er minimiert aufgrund der positiven zweiten Ableitung $\hat{S}$.
\end{small}
\endxmpl


## Konstruktion von Schätzfunktionen
\framesubtitle{Maximum-Likelihood-Methode}

- Die \hil{Maximum-Likelihood-Methode} (ML-Methode) setzt voraus, dass die Verteilung von $X$ bekannt ist, bis auf die Parameter \[\boldsymbol\theta=(\theta_1,\ldots,\theta_K).\]
- Diese unbekannten Parameter werden so geschätzt, dass die Wahrscheinlichkeit für das Eintreten der bereits vorliegenden Stichprobenrealisationen $(x_1,\ldots,x_n)$ maximal wird.


## Konstruktion von Schätzfunktionen
\framesubtitle{Maximum-Likelihood-Methode}

- Da die Stichprobenvariablen unabhängig wie $X$
verteilt sind, besitzen sie alle dieselben Wahrscheinlichkeits- bzw. Dichtefunktionen
\[ f(X_1|\boldsymbol\theta)=f(X_2|\boldsymbol\theta)=\ldots =f(X_n|\boldsymbol\theta). \]
- Die gemeinsame Wahrscheinlichkeits- bzw. Dichtefunktion folgt nach Verallgemeinerung für den $n$-dimensionalen Fall als
\[ f(X_1,X_2,\ldots,X_n|\boldsymbol\theta)=f(X_1|\boldsymbol\theta)f(X_2|\boldsymbol\theta)\cdot \ldots \cdot f(X_n|\boldsymbol\theta)=\prod\limits_{j=1}^nf(X_j|\boldsymbol\theta). \]
(Verallgemeinerung von \blue{\eqref{gemWSUnabh}})


## Konstruktion von Schätzfunktionen
\framesubtitle{Maximum-Likelihood-Methode}

- In der gemeinsamen Wahrscheinlichkeits- bzw. Dichtefunktion sind jetzt die Elemente des Vektors $\boldsymbol\theta$ die Variablen.
- Man bezeichnet diese Funktion daher als \hil{Likelihood-Funktion} $L$, die bezüglich $\boldsymbol\theta$ zu maximieren ist.
\[ L=f(\boldsymbol\theta|x_1,\ldots,x_n) \longrightarrow \underset{\boldsymbol\theta}{\text{Max!}} \]
- Die Lösung dieses Problems liefert den \hil{ML-Schätzer} $\hat{\boldsymbol\theta}$.
- Unter geeigneten Annahmen sind ML-Schätzer konsistent, mindestens asymptotisch erwartungstreu und mindestens asymptotisch effizient.


## Konstruktion von Schätzfunktionen
\framesubtitle{Maximum-Likelihood-Methode}

\xmpl[Geometrische Verteilung (1/2)]
Die Likelihood-Funktion ist mit \eqref{DichteGeomVtlg}
\[L(p|x_1,\ldots,x_n)=\prod_{i=1}^n(1-p)^{x_i}p\]
Logarithmieren von $L$ liefert die zur Differentiation geeignetere Form
\[\ln L(p)=k\ln(1-p)+n\ln(p),\]
wobei $k=\sum_ix_i$.
Die erste Ableitung ist \[\frac{\partial\ln L}{\partial p}=-\frac{k}{1-p}+\frac{n}{p}.\]
\endxmpl


## Konstruktion von Schätzfunktionen
\framesubtitle{Maximum-Likelihood-Methode}
\xmpl[*]
Nullsetzen und Auflösen liefert
\[\frac{k}{1-p}=\frac{n}{p}\]
bzw. 
\[\frac{p}{n}+\frac{p}{k}=\frac{1}{k},\]
so dass
\[\hat{p}_{ML}=\frac{n}{k+n}.\]
\endxmpl


## Konstruktion von Schätzfunktionen
\framesubtitle{Maximum-Likelihood-Methode}
Äquivalent, aber praktischer ist es, die Schritte in einer anderen Reihenfolge durchzuführen.

1. Dichte für ein einzelnes $x_i$ logarithmieren
2. Logarithmierte Dichte ableiten
3. Von $i=1,\ldots,n$ summieren
4. Gleich Null setzen und auflösen


## Konstruktion von Schätzfunktionen
\framesubtitle{Maximum-Likelihood-Methode}
\exe
Gegeben sei eine Zufallsstichprobe vom Umfang $n$ aus einer Grundgesamtheit mit Dichte
\[f(x;\lambda)=\lambda e^{-\lambda x}\]
Ermitteln Sie den ML-Schätzer für $\lambda$!
\endexe


## Ausgewählte Schätzfunktionen

Hier ohne Beweis einige Beispiele für die oben diskutierten Konzepte.

- $\bar{X}$ ist erwartungstreu für den Erwartungswert $\mu$ und mit $\var(\bar{X})=\sigma^2/n$ darüber hinaus konsistent und effizient unter den unverzerrten Schätzern.
- Für $X\sim N(\mu,\sigma^2)$ ist \[S_{\text{ML}}^2=\frac{1}{n}\sum_{j=1}^n(X_j-\bar{X})^2\] der ML-Schätzer der Varianz $\sigma^2$.
- $S_{\text{ML}}^2$ ist verzerrt mit Erwartungswert $\sigma^2(n-1)/n$, asy.\ erwartungstreu und konsistent.
- Damit ist \[S^2=\frac{1}{n-1}\sum_{j=1}^n(X_j-\bar{X})^2\]  erwartungstreu und konsistent für die Varianz $\sigma^2$. Für $X\sim N(\mu,\sigma^2)$ ist $\var(S^2)=2\sigma^4/(n-1)$.


## Ausgewählte Schätzfunktionen
Siehe \texttt{AsyUnverzerrtheit.R}.

\exe
Zeigen Sie, dass $\bar{X}$ im quadratischen Mittel nach $\mu$ konvergiert, wenn $X_1,\ldots,X_n$ eine einfache Zufallsstichprobe ist.
\endexe
\exe
Ermitteln und vergleichen Sie den MSE von $S^2$ und $S^2_{\text{ML}}$, wenn $X_1,\ldots,X_n$ eine einfache Zufallsstichprobe aus einer normalverteilten Grundgesamtheit ist.
\endexe


## Ausgewählte Schätzfunktionen
\framesubtitle{}

\exe
Zeigen Sie, dass

(a) bei bekanntem Erwartungswert $\E(X)=\mu$ der Grundgesamtheit
\[S^{2}=\frac{1}{n}\sum\limits_{j}(X_{j}-\mu )^{2},\]
(b) bei unbekanntem Erwartungswert $\E(X)=\mu$ der Grundgesamtheit \[S^{2}=\dfrac{1}{n-1}\sum\limits_{j}(X_{j}-\overline{X})^{2}\] eine erwartungstreue Schätzfunktion für die unbekannte Varianz von $X$ in der Grundgesamtheit ist!

\endexe


## Ausgewählte Schätzfunktionen
\framesubtitle{}
\exe
Im Rahmen eines Projekts soll die Körpergröße von Studenten untersucht werden. Hierzu wird eine Zufallsstichprobe im Umfang von $n=10$ gezogen. Dies liefert folgende Körpergrößen in cm:
\[176, 180, 181, 168, 177, 186, 184, 173, 182, 177.\]

(a) Schätzen Sie den unbekannten Erwartungswert der Grundgesamtheit $\mu$!
(b) Schätzen Sie die unbekannte Varianz der Grundgesamtheit $\sigma^2$!

\endexe


## Intervallschätzung
\framesubtitle{Schwankungsintervall}
- Man bestimmt, ausgehend von den \hil{bekannten} Parametern der Grundgesamtheit ein Intervall, aus dem die Stichprobengröße mit hoher, vorgegebener Wahrscheinlichkeit Werte annimmt.
- Ein solches Intervall nennt man \hil{Schwankungsintervall}.
- Ein \hil{zentrales} Schwankungsintervall ist dabei symmetrisch um $\mu$ konstruiert.
- Die vorgegebene, hohe Wahrscheinlichkeit bezeichnet man mit $1-\alpha$.
- Hierzu benötigen wir die Verteilung der Stichprobengröße.
- So ist bspw.\ $\bar{X}$ bei einfachen Stichproben wegen \eqref{EWMWsrs} und \eqref{varMWsrs} für hinreichend großes $n$ dank des Zentralen Grenzwertsatzes \eqref{ZGWS} unter den dortigen Bedingungen annähernd normalverteilt mit
\[ \E(\bar{X})=\mu \quad \text{und} \quad\var(\bar{X})=\sigma_{\bar{X}}^2=\frac{1}{n}\sigma^2. \]


## Intervallschätzung
\framesubtitle{Schwankungsintervall}

```{r Stichprobenverteilung, echo = F}
grid <- seq(-3, 3, .03)
plot(x = grid, y = dnorm(grid), type = "l", ylim = c(0, .5), xaxt = "n", yaxt = "n", xlab = "", ylab = "", main="Stichprobenverteilung des Stichprobenmittels", bty="l")
axis(1, at = c(-1.5, 0, 1.5), labels = c("$\\mu-a$", "$\\mu$", "$\\mu+a$"))
polygon(x = c(-3, seq(-3, -1.5, .01), -1.5), y = c(0, dnorm(seq(-3, -1.5, .01)), 0),
        col = alpha("tomato3", 0.5), border = NA)
polygon(x = c(1.5, seq(1.5, 3, .01), 3),     y = c(0, dnorm(seq(1.5, 3, .01)), 0),
        col = alpha("tomato3", 0.5), border = NA)
text(x = 0, y = 0.2, labels = "$1-\\alpha$")
text(x = c(-2, 2), y = rep(0.17, 2), labels = rep("$\\frac{\\alpha}{2}$", 2))
segments(x0 = c(-2, 2), x1 = c(-1.6, 1.6), y0 = 0.125, y1 = 0.05)

title(xlab = "$\\bar{X}$", adj=1)
title(ylab = "$f(\\bar{x}|\\mu, \\sigma_{\\bar{X}}^2)$", adj=1, las=1)
```


## Intervallschätzung
\framesubtitle{Schwankungsintervall}

- Die nicht rötliche Fläche entspricht der Wahrscheinlichkeit $1-\alpha$, die rötlichen Flächen betragen jeweils $\alpha/2$.
- Wegen der Symmetrie der Normalverteilung wird die nicht schraffierte Fläche auf der Abszisse durch die Werte $\mu \pm a$ begrenzt.
- Damit gilt für die Wahrscheinlichkeit des Ereignisses $\mu-a\leq \bar{X}\leq \mu+a$
\[ P(\mu-a\leq \bar{X} \leq \mu+a)=1-\alpha. \]
- Nach Standardisieren mit $Z=\frac{\bar{X}-\mu}{\sigma_{\bar{X}}}$ folgt
\[ P\left(-\dfrac{a}{\sigma_{\bar{X}}} \leq \dfrac{\bar{X}-\mu}{\sigma_{\bar{X}}} \leq \dfrac{a}{\sigma_{\bar{X}}}\right) = P(-z\leq Z\leq z)=1-\alpha . \]


## Intervallschätzung
\framesubtitle{Schwankungsintervall}

- Die Wahrscheinlichkeit $1-\alpha$ legt die Werte $\pm z$ der standardnormalverteilten Zufallsvariablen $Z$ fest. Es gilt
\[ \pm z=\pm z_{1-\frac{\alpha}{2}}. \]
- Das Schwankungsintervall für $\bar{X}$ lautet daher
\[ P(\mu-\sigma_{\bar{X}}z_{1-\frac{\alpha}{2}} \leq \bar{X} \leq \mu + \sigma_{\bar{X}} z_{1-\frac{\alpha}{2}})=1-\alpha \]
mit \[ \sigma_{\bar{X}}\stackrel{\eqref{varMWsrs}}{=}\dfrac{\sigma}{\sqrt{n}}.\]





## Intervallschätzung
\framesubtitle{Schwankungsintervall}
\xmpl[Schwankungsintervall des Stichprobenmittels] \label{Bsp6_12}
Die Körpergröße deutscher Männer ist normalverteilt mit $\mu=178$ cm und $\sigma^2=64\text{ cm}^2$. Um das Schwankungsintervall für die Stichprobenmittelwerte $\bar{X}$ zu bilden, benötigt man $z_{1-\frac{\alpha}{2}}$.\mps
Für $1-\alpha=0.95$ erhalten wir $z_{1-\frac{\alpha}{2}}=z_{0.975}= 1.96$ aus `r runidue::s(qnorm(0.975))`.

Für einfache Stichproben beträgt die Varianz bei einem Stichprobenumfang von $n=100$ $\sigma^2_{\bar{X}}=64/100$ und die Standardabweichung $\sigma_{\bar{X}}=0.8$.
\endxmpl


## Intervallschätzung
\framesubtitle{Schwankungsintervall}
\xmpl[*]
Die Gleichung \[ P(\mu-\sigma_{\bar{X}}z_{1-\frac{\alpha}{2}} \leq \bar{X} \leq \mu + \sigma_{\bar{X}} z_{1-\frac{\alpha}{2}})=1-\alpha \]
geht daher über in
\begin{eqnarray*}
0.95=&P(178-1.96\cdot 0.8 \leq \bar{X}\leq 178+1.96\cdot 0.8)\quad \text{oder}\\
0.95=&P(176.432\leq \bar{X}\leq 179.568).
\end{eqnarray*}
Im berechneten Schwankungsintervall liegen 95% aller einfachen Stichprobenmittelwerte bei einem Stichprobenumfang von $n=100$.
\endxmpl

## Statistische Schätzverfahren
\framesubtitle{Eigenschaften von Schätzern - QuizAcademy}
\QuizAcademy{Schätzverfahren 1}\endQuizAcademy


## Intervallschätzung
\framesubtitle{Konfidenzintervalle}

- Ziel einer Intervallschätzung ist es, für einen unbekannten Parameter einer Grundgesamtheit ein Intervall so zu schätzen, dass es mit großer Wahrscheinlichkeit die unbekannte Größe einschließt.
- Ein solches Intervall nennt man \hil{Konfidenzintervall}, die Wahrscheinlichkeit $1-\alpha$ heißt \hil{Konfidenzniveau}. 
- Jedes berechnete Konfidenzintervall stellt eine \hil{Intervallschätzung} für den enthaltenen Grundgesamtheitsparameter dar.


## Intervallschätzung
\framesubtitle{Konfidenzintervalle}

- Konfidenzintervalle haben formal dieselbe Struktur wie Schwankungsintervalle.
- Aus \[ P\left(-z_{1-\frac{\alpha}{2}}\leq Z\leq z_{1-\frac{\alpha}{2}}\right)=1-\alpha \] folgt nach Substitution von $Z$ durch $(\bar{X}-\mu)/\sigma_{\bar{X}}$
\begin{eqnarray*}
&& P\left(-z_{1-\frac{\alpha}{2}}\leq \frac{\bar{X}-\mu}{\sigma_{\bar{X}}}\leq z_{1-\frac{\alpha}{2}}\right)=1-\alpha \\
&\Rightarrow& P\bigl(-z_{1-\frac{\alpha}{2}}\sigma_{\bar{X}}\leq \bar{X}-\mu\leq z_{1-\frac{\alpha}{2}}\sigma_{\bar{X}}\bigr)=1-\alpha\\
&\Rightarrow& P\bigl(-\bar{X}-z_{1-\frac{\alpha}{2}}\sigma_{\bar{X}}\leq -\mu\leq -\bar{X}+z_{1-\frac{\alpha}{2}}\sigma_{\bar{X}}\bigr)=1-\alpha \\
&\Rightarrow& P\bigl(\bar{X}+z_{1-\frac{\alpha}{2}}\sigma_{\bar{X}}\geq \mu\geq \bar{X}-z_{1-\frac{\alpha}{2}}\sigma_{\bar{X}}\bigr)=1-\alpha
\end{eqnarray*}
und schließlich \[ P\bigl(\bar{X}-z_{1-\frac{\alpha}{2}}\sigma_{\bar{X}}\leq \mu\leq \bar{X} + z_{1-\frac{\alpha}{2}}\sigma_{\bar{X}}\bigr)=1-\alpha. \]


## Intervallschätzung
\framesubtitle{Konfidenzintervalle}
\exe
In einer Brauerei ist eine Abfüllmenge für Bier auf eine Sollmenge von 100 l pro Faß eingestellt (Standardabweichung 0.3 l).
Es ist möglich, dass es nach einiger Zeit zu Abweichungen von dieser eingestellten Sollmenge kommt, so dass die Maschine neu justiert werden muss.
Bei einer zufälligen Stichprobe von 50 Fässern wurde eine durchschnittliche Abfüllmenge von 100.2 l gemessen.\mps
Erstellen Sie ein 95%-Konfidenzintervall für $\mu$, um zu überprüfen, ob die Maschine neu justiert werden muss!
\endexe


## Intervallschätzung
\framesubtitle{Konfidenzintervalle}

- Das Konfidenzintervall kann nur für bekanntes $\sigma^2$ erstellt werden.
- Ist dies nicht der Fall, wird $\sigma^2$ über die Stichprobenvarianz geschätzt.
- Wird bei normalverteilten $X_j$ in \[Z=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\] $\sigma^2$ durch die Varianz einer Stichprobe ersetzt, folgt \[ Z(t)=\frac{\bar{X}-\mu}{S/\sqrt{n}}\] bei einfachen Stichproben einer \hil{$t$-Verteilung} mit $n-1$ Freiheitsgraden, siehe \eqref{tVtlg}.


## Intervallschätzung
\framesubtitle{Konfidenzintervalle}
Das Konfidenzintervall folgt mit Symmetrie der $t$-Verteilung aus
\[ P\left(-t(1-\alpha/2,n-1) \leq \frac{\bar{X}-\mu}{S/\sqrt{n}}\leq t(1-\alpha/2,n-1)\right)=1-\alpha \] als
\[ P\left(\bar{X}-t(1-\alpha/2,n-1)\frac{S}{\sqrt{n}}\leq \mu\leq \bar{X}+t(1-\alpha/2,n-1)\frac{S}{\sqrt{n}}\right) =1-\alpha. \]
Bei Stichprobenumfängen von $n\geq 30$ ist folgende \hil{Approximation} möglich
\[ P\left(\bar{X}-z_{1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}\leq \mu\leq \bar{X} + z_{1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}\right)=1-\alpha. \]
Siehe \texttt{Konfidenzintervalle.R}.


## Intervallschätzung
\framesubtitle{Konfidenzintervalle}
\xmpl[Konfidenzintervall für die Körpergröße (Fortsetzung Beispiel \ref{Bsp6_12})]
Für die Körpergröße deutscher Männer soll ein Konfidenzintervall bei einem Konfidenzniveau von $1-\alpha=0.95$ mit einer einfachen Stichprobe des Umfangs $n=25$ erstellt werden. Das Stichprobenmittel lautet $\bar{x}=178$ cm, die Stichprobenvarianzschätzung beträgt $s^2=49\text{ cm}^2$.\mps
R liefert uns den kritischen Wert der $t$-Verteilung mit $1-\frac{\alpha}{2} = 0.975$ und $n-1=24$ Freiheitsgraden durch `r runidue::s(qt(0.975, 24))`.
\endxmpl


## Intervallschätzung
\framesubtitle{Konfidenzintervalle}
\xmpl[*]
Die Standardabweichung der Stichprobenverteilung ergibt sich als \[ \frac{s}{\sqrt{n}}=\frac{7}{5}=1.4.\] Dies führt zu dem Konfidenzintervall \[\mu\in[178-2.0639\cdot1.4 ; 178+2.0639\cdot1.4] \]
oder \[ \mu\in[175.1105 ;180.8895].\]
\endxmpl


## Intervallschätzung
\framesubtitle{Konfidenzintervalle}

\xmpl[*]

```{r KimitR}
library(MASS) # Mit Hilfe von mvrnorm werden normalverteilte Daten erzeugt,
# deren empirische Parameter denen aus dem Beispiel entsprechen.
n <- 25; x <- mvrnorm(n = n, empirical = TRUE, mu = 178, Sigma = 49)
head(as.vector(x))

# Alternative 1
confint(object = (lm(x~1)), level = 0.95)

# Alternative 2 (mit der t-Verteilung)
c(mean(x)-sd(x)/sqrt(n)*qt(0.975,n-1), mean(x)+sd(x)/sqrt(n)*qt(0.975,n-1))
# Mit der Normalverteilung
c(mean(x)-sd(x)/sqrt(n)*qnorm(0.975), mean(x)+sd(x)/sqrt(n)*qnorm(0.975))

```

\endxmpl


## Intervallschätzung
\framesubtitle{Konfidenzintervalle - QuizAcademy}
\QuizAcademy{Schätzverfahren 2}\endQuizAcademy


## Intervallschätzung
\framesubtitle{Konfidenzintervalle}
\exe
Es wurden 200 Versicherte einer großen KfZ-Haftpflichtversicherung zufällig ausgewählt und nach ihrer täglichen Fahrleistung $X$ befragt.
Die Auswertung der Befragung ergab eine durchschnittliche Fahrleistung von $\bar{x}=25$ km pro Tag und eine Varianz von $s^2=128$ km$^2$.\mps
Stellen Sie das 95%-Konfidenzintervall für die durchschnittliche Fahrleistung $\mu$ eines Versicherten auf!
\endexe


## Intervallschätzung
\framesubtitle{Konfidenzintervalle}

- Für die korrekte Interpretation von Konfidenzintervallen ist wichtig, dass nicht der Parameter, sondern die Grenzen des Intervalls die Zufallsvariablen sind, da sie von der Stichprobe abhängen.
- Würde man viele Stichproben ziehen und mit jeder ein Konfidenzintervall bestimmen, schließen ca.\ $(1-\alpha)100\%$ der Konfidenzintervalle den Parameter ein, $\alpha\cdot 100\%$ aber nicht.
- Deshalb ist es sinnvoll $1-\alpha$ groß zu wählen.


## Anhang
\framesubtitle{Momentenmethode}

- Lässt sich ein Grundgesamtheitsparameter $\theta$ als Funktion $T_{\theta}$ der Anfangsmomente der Grundgesamtheit $M_{\alpha}=\E(X^\alpha)$ angeben, erhält man
\[ \theta=T_{\theta}(M_1,\ldots,M_K). \]
- Die Schätzung erfolgt nun so, dass die Momente der Grundgesamtheit $M_{\alpha}$ durch die analogen, empirischen Momente $m_{\alpha}$ der Stichprobe ersetzt werden.
- Der Momentenschätzer lautet dann
\[ \hat{\theta}=T_{\theta} (m_1,\ldots,m_K)\quad \text{mit} \quad m_{\alpha} =\dfrac{1}{n}\sum\limits_{j=1}^n X_j^\alpha \quad \text{für } \alpha=1,\ldots,K. \]


## Anhang
\framesubtitle{Momentenmethode}

- Momentenschätzer sind unabhängig von der Verteilung der Grundgesamtheit immer konsistent.
- Dies liegt daran, dass die $m_\alpha$ mit wachsendem $n$ in die Grundgesamtheitsmomente $M_\alpha$, die auch \hil{theoretische Momente} genannt werden, übergehen.
- Einige dieser Schätzer sind erwartungstreu. Andere, verzerrte Schätzer lassen sich mit geringem Aufwand in erwartungstreue überführen.
- Jedoch sind bis auf wenige Ausnahmen Momentenschätzer nicht effizient.


## Anhang
\framesubtitle{Momentenmethode}
\xmpl[Momentenschätzer für $\mu$]
In einer Grundgesamtheit ist $\mu$ unbekannt. Dieser Lageparameter entspricht dem Anfangsmoment erster Ordnung. Somit gilt: $\mu=\E(X)=M_1$. Der Momentenschätzer lautet daher
\[ \hat{\mu}=T_\mu(m_1)=m_1\ \ \text{mit} \ \ m_1= \frac{1}{n}\sum\limits^n_{j=1} X_j \ \ \text{oder} \ \ \hat{\mu}=\frac{1}{n}\sum X_j=\bar{X}. \]
Liegt eine einfache Stichprobe vor, berechnet man mit ihren Realisationen das Stichprobenmittel und erhält eine Punktschätzung für $\mu$.
\endxmpl



<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%% Kapitel 8 -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- % Kürzungen gegenüber kompletter Fassung SoSe 2014: -->
<!-- % -->
<!-- % - Anteilswerte -->
<!-- % - Testentscheidung immer direkt über Z -->
<!-- % - weniger zu Bereichshyp usw. -->

# Statistische Testverfahren

## Signifikanztests
\framesubtitle{Testtheorie}

- Mit der \hil{Testtheorie} kann auf der Basis von Stichproben überprüft werden, ob Vorstellungen über eine Grundgesamtheit zu Recht bestehen, d.h. sich mit der Realität in Einklang befinden.
- Jede Annahme über die unbekannte Verteilung von $X$ in einer Grundgesamtheit heißt \hil{statistische Hypothese}.
- Statistische Hypothesen können sich auf die Parameter $\theta_k,$ $k=1,\ldots,K$, einer Verteilung oder auf die Verteilung selbst beziehen.
- Im ersten Fall heißen sie \hil{Parameterhypothesen}, im zweiten Fall \hil{Verteilungshypothesen}.
- Wir beschränken uns hier auf den ersten Fall.


## Signifikanztests
\framesubtitle{Statistische Hypothese}

- Parameterhypothesen legen für die unbekannten Verteilungsparameter numerische Werte fest.
- Geschieht dies für alle Parameter des Verteilungsgesetzes so, dass ihnen jeweils genau ein Wert zugeordnet wird, liegt eine \hil{einfache Hypothese} vor.
- Gibt man für einige oder alle Parameter hingegen Intervalle an oder lässt einige gänzlich unbestimmt, heißt die Hypothese \hil{zusammengesetzt}.


## Signifikanztests
\framesubtitle{Statistische Hypothese}
\xmpl[Hypothesen]
 $X$ sei normalverteilt mit unbekannten Parametern $\mu$ und $\sigma^2$. Da für $\mu$ keine a priori Restriktionen existieren, ist der Parameterraum $\mathbb{R}$. Der Parameterraum für $\sigma^2$ ist $[0,\infty)$.

- Einfache Hypothese: $\mu=2$ und $\sigma^2=4$.
- Gibt man den Wert für einen Parameter genau, für den anderen Parameter ein Intervall an, liegt eine zusammengesetzte Hypothese vor, z.B. $\mu=2$ und $0\leq\sigma^2\leq 4$.

\endxmpl



## Signifikanztests
\framesubtitle{Statistischer Test}

- Die Überprüfung einer statistischen Hypothese heißt \hil{statistischer Test}, die dabei überprüfte Hypothese nennt man \hil{Nullhypothese}, kurz $H_0$.
- In der Nullhypothese werden den Parametern $\theta_k, k=1,\ldots,K$ Werte zugeordnet, die eine Teilmenge des Parameterraumes bilden.
- Alle nicht in der Nullhypothese enthaltenen Werte können zur Formulierung einer sogenannten \hil{Alternativhypothese} genutzt werden.
- Null- und Alternativhypothese sind disjunkt.




## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}

- Man will z.B.\ oft  Hypothesen über den \hil{Erwartungswert} einer Grundgesamtheit testen.
- Bei einer \hil{einfachen Hypothese} wird für $\mu$ der Wert $\mu_0$ angenommen
\[ H_0: \, \mu=\mu_0. \]
- Zusammengesetzte \hil{Nullhypothesen}:
\[ H_0: \, \mu\leq\mu_0 \ \ \text{oder} \ \ \mu\geq\mu_0. \]




## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}

- Auch die \hil{Alternativhypothese} $H_1$ kann in einfacher oder zusammengesetzter Form vorliegen.
- Der allgemeinere Fall einer zusammengesetzten Alternativhypothese wird häufig angegeben als
\[ H_1: \, \mu>\mu_0 \ \ \text{oder} \ \ H_1: \, \mu<\mu_0 \ \ \text{oder} \ \ H_1: \, \mu\neq\mu_0. \]
- Die Vorgehensweisen beim \hil{zweiseitigen} und beim \hil{einseitigen Testen} ähneln sich sehr.
-  Null- und Alternativhypothese beim zweiseitigen Test lauten
\[ H_0: \, \mu=\mu_0 \ \ \text{und} \ \ H_1: \mu\neq\mu_0. \]


## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}
\xmpl[Anlagestrategie]
Sie wollen reich werden und haben sich dafür eine Handelsstrategie am Aktienmarkt überlegt. Der Erwartungswert der Rendite dieser Strategie sei $\mu$.\mps
Sie wollen testen, ob Ihnen die Strategie mehr einbringt als Null (oder aber eine sichere Anlage, oder ein Indexfonds, usw.).\mps
Dann testen Sie die zusammengesetzte Hypothese
\[H_0:\mu\leq0\]
gegen
\[H_1:\mu>0.\]
\endxmpl



## Signifikanztests
\framesubtitle{Teststatistik}

- Um $H_0$ zu überprüfen, benötigt man eine \hil{Teststatistik}.
- Diese ist eine Stichprobenfunktion und besitzt daher eine Verteilung.
- Einen konkreten \hil{Prüfwert} erhält man, wenn in die Teststatistik die Stichprobenrealisationen eingesetzt werden.
- Für den Erwartungswert $\mu$ eignet sich das arithmetische Mittel $\bar{X}=\frac{1}{n}\sum^n_{j=1} X_j$ als Teststatistik.
- Erinnerung: Für die Stichprobenverteilung von $\bar{X}$ sind weitere Informationen über die Verteilung von $X$ in der Grundgesamtheit notwendig.
- Ist $X$ normalverteilt mit bekannter Varianz $\sigma^2$, so ist auch $\bar{X}$ normalverteilt und \hil{unter der Nullhypothese} (zusammengesetzte Hypothesen s.u.) gilt mit \eqref{EWMWsrs} und \eqref{varMWsrs}
\[ \E(\bar{X})=\mu_0 \ \ \text{und} \ \ \var(\bar{X})=\frac{\sigma^2}{n}. \]


## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}
\xmpl[Anlagestrategie]
Sie testen Ihre Anlagestrategie, indem Sie sie an $n$ Handelstagen ausprobieren und die durchschnittliche Rendite ausrechnen.\mps
Dass Renditen normalverteilt sind, mag zutreffen oder auch nicht. Dass $\sigma^2$ bekannt ist, ist offenbar eine (vorläufige) vereinfachende Annahme.\mps
Wenn Sie die Nullhypothese testen, dass Ihre Strategie nichts bringt und dem auch so ist, dann hat Ihre Strategie einen Erwartungswert von $\mu\leq0$.
\endxmpl


## Signifikanztests
\framesubtitle{Signifikanzniveau}

- Da die Teststatistik selbst eine Zufallsvariable ist, können Entscheidungen bei einem Hypothesentest nicht mit Sicherheit getroffen werden.
- Daher benötigt man ein Kriterium, mit dem Abweichungen der geschätzten Parameter vom Parameterwert \hil{unter $H_0$} noch als zufällig oder bereits als \hil{signifikant} bewertet werden können.
- Gegeben ein $\alpha\in(0,1)$ werden für alle möglichen Realisationen der Teststatistik \[T(X_1,\ldots,X_n)\] zwei komplementäre Mengen $I_\alpha$ und $I_{1-\alpha}$ konstruiert.
- Wähle $I_\alpha$ so, dass die Wahrscheinlichkeit \hil{unter $H_0$} eine Realisation von $T$ in $I_\alpha$ zu erhalten gleich $\alpha$ ist.


## Signifikanztests
\framesubtitle{Signifikanzniveau}

- Darüber hinaus sollen in $I_\alpha$ alle Realisationen von $T$ liegen, die \hil{unter $H_0$} geringe Eintrittswahrscheinlichkeit haben.
- Damit lässt sich nun eine Entscheidungsregel für Hypothesentests aufstellen.
  - Liegt der Prüfwert in $I_\alpha$, wird $H_0$ \hil{abgelehnt}.
  - Liegt er in $I_{1-\alpha}$, wird $H_0$ \hil{nicht abgelehnt}.
- $\alpha$ ist also die Wahrscheinlichkeit $H_0$ abzulehnen, obwohl sie richtig ist (was der Tester jedoch nicht weiß).


## Signifikanztests
\framesubtitle{Signifikanzniveau}
\xmpl[Anlagestrategie]
Wenn Sie ihre Strategie an anderen Handelstagen ausprobiert hätten, hätten Sie auch eine andere durchschnittliche Rendite bekommen.\mps
Bloß weil Sie an ihr ein paar Euro verdient haben, würden Sie sich wohl kaum trauen, sich Millionen zu leihen und die Strategie in großem Stil zu implementieren. Die positive durchschnittliche Rendite könnte ja auch Zufall gewesen sein.\mps
Wir suchen also eine Prüfgröße, die nur mit sehr geringer Wahrscheinlichkeit \glqq{}extreme\grqq{} Werte (z.B.\ hohe Durchschnittsrenditen) annimmt, wenn Ihre Strategie nichts bringt.
\endxmpl


## Signifikanztests
\framesubtitle{Signifikanzniveau}

- Man wird daher bei einem Test $\alpha$ sehr klein wählen, in der Regel 0.1, 0.05 oder 0.01.
- Tests mit diesem Aufbau heißen \hil{Signifikanztests}.
- Man bezeichnet $\alpha$ daher als \hil{Irrtumswahrscheinlichkeit}.
- Die alternative Bezeichnung \glqq{}\hil{Signifikanzniveau}\grqq{} für $\alpha$ resultiert daraus, dass die Stichprobengröße (etwa $\bar{X}$) signifikant vom Wert des Parameters aus $H_0$ abweichen muss, um in $I_\alpha$ zu liegen.


## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert, zweiseitiger Test}


```{r, echo=F, fig.show="hold", fig.width=0.6, fig.asp=0.7}
grid <- seq(-3, 3, .01)
par(mar=c(3, 1, 2, 1))
plot(x = grid, y = dnorm(grid), type = "l", ylim = c(0, .5), xaxt = "n", yaxt = "n",
     xlab = "", ylab = "", main = "Null-Verteilung von $\\bar{X}$")
axis(1, at = c(-1.5, 0, 1.5), labels = c("$\\bar{x}_{r,1}$", "$\\mu_0$", "$\\bar{x}_{r,2}$"))
polygon(x = c(-3, seq(-3, -1.5, .01), -1.5), y = c(0, dnorm(seq(-3, -1.5, .01)), 0),
        col = "tomato3", border = NA)
polygon(x = c(1.5, seq(1.5, 3, .01), 3),     y = c(0, dnorm(seq(1.5, 3, .01)), 0),
        col = "tomato3", border = NA)
text(x = 0, y = 0.2, labels = "$1-\\alpha$")
text(x = c(-2.2, 2.2), y = rep(0.12, 2), labels = rep("$\\frac{\\alpha}{2}$", 2))
segments(x0 = c(-2, 2), x1 = c(-1.6, 1.6), y0 = 0.1, y1 = 0.05)

plot(x = grid, y = dnorm(grid), type = "l", ylim = c(0, .5), xaxt = "n", yaxt = "n",
     xlab = "", ylab = "", main = "Null-Verteilung von $Z$")
axis(1, at = c(-1.5, 0, 1.5), labels = c("$-z_r$", "0", "$z_r$"))
polygon(x = c(-3, seq(-3, -1.5, .01), -1.5), y = c(0, dnorm(seq(-3, -1.5, .01)), 0),
        col = "tomato3", border = NA)
polygon(x = c(1.5, seq(1.5, 3, .01), 3),     y = c(0, dnorm(seq(1.5, 3, .01)), 0),
        col = "tomato3", border = NA)
text(x = 0, y = 0.2, labels = "$1-\\alpha$")
text(x = c(-2.2, 2.2), y = rep(0.12, 2), labels = rep("$\\frac{\\alpha}{2}$", 2))
segments(x0 = c(-2, 2), x1 = c(-1.6, 1.6), y0 = 0.1, y1 = 0.05)
```



## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}

- Die linke Abbildung gibt die Normalverteilungsdichte der Teststatistik $\bar{X}$ unter $H_0$ wieder. $\bar{x}_{r,1}$ und $\bar{x}_{r,2}$ sind die beiden \hil{kritischen Werte}, die den \hil{Annahmebereich} $I_{1-\alpha}$ und \hil{Ablehnungsbereich} $I_\alpha$ trennen.
- Daneben ist dieselbe Testsituation dargestellt, jetzt jedoch mit der unter $H_0$ standardnormalverteilten Teststatistik
\[ Z=\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}. \]
- Man kann den kritischen Wert $\bar{x}_{r,2}$ durch Auflösen der Gleichung $P(\bar{X}\leq\bar{x}_{r,2})=1-\frac{\alpha}{2}$ erhalten (vgl.\ Buch S.\ 243).
 - Der Test lässt sich aber schneller durchführen, wenn $I_\alpha$ in Bezug auf $Z$ angegeben wird.


## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}

- Bei gegebener Wahrscheinlichkeit $1-\alpha/2$ erhält man das $1-\alpha/2$-Quantil $z_{1-\alpha/2}$ mit dem R-Befehl `qnorm(p)` wobei $p=1-\alpha/2$. 
- Der \hil{Ablehnungsbereich} lautet
\[ I_\alpha(z)=\{Z > z_{1-\alpha/2} \text{ oder } Z < z_{\alpha/2}\}. \]
- Liegt also die standardisierte Testgröße
\[z = \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}\]
in $I_\alpha(z)$, ist $H_0$ abzulehnen.
- In diesem Fall kann die Abweichung von $\bar{x}$ zu $\mu_0$ nicht als zufällig angesehen werden, sondern ist \hil{(statistisch) signifikant}.


## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert - QuizAcademy}
\QuizAcademy{Signifikanztests 1}
\endQuizAcademy


## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}

\xmpl[Parametertest bei normalverteilter Grundgesamtheit]
$X$ sei normalverteilt mit $\sigma^2=36$. Es galt $\E(X)=40$. Trifft dies noch zu? Da die mögliche Richtung der Veränderung von $\E(X)$ unklar ist, führen wir einen zweiseitigen Test durch.\mps

- $H_0:\mu=\mu_0=40$, $H_1:\mu\neq 40$.
- Das Signifikanzniveau sei $\alpha=0.05$.
- Eine Stichprobe im Umfang $n=100$ liefere $\bar{x}=42$.

Prüfe nun, ob die Abweichung $\bar{x}-\mu_0$ zufällig oder bei $\alpha=0.05$ signifikant ist.\mps
Die Statistik
\[ \bar{X}=\frac{1}{n}\sum\limits^n_{j=1}X_j \]
ist unter $H_0$ normalverteilt mit $\E(\bar{X})=40$ und
$\var(\bar{X})=36/100=0.36$.
\endxmpl


## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}
\xmpl[*]
Den kritischen Wert $z_{r,\alpha/2}$ ermittelt man mit der Gleichung
\[P(Z\leq z_{r,\alpha/2})=0.975.\]
Der R-Befehl `qnorm(0.975)` liefert $z_{0.975}=1.96$.
Der Annahmebereich für die Teststatistik
\[Z=\frac{\bar{X}-40}{0.6}\]
ist
\[ I_{1-\alpha}(z)=\{-1.96<Z<1.96\}. \]
Der Prüfwert $z=\frac{42-40}{0.6}=3.\bar{3}$ liegt nicht in $I_{1-\alpha}(z)$.
\endxmpl



## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}

\xmpl[*]
```{r}
library(MASS)
x <- mvrnorm(n = 100, empirical = TRUE, mu = 42, Sigma = 36)
head(as.vector(x))
t.test(x, mu = 40, alternative = "two.sided", conf.level = 0.95)
```
Auf die Bedeutung von `df=99` gehen wir später noch ein.

\endxmpl


## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}

\xmpl[Dow Jones Returns]
```{r}
library(quantmod)
getSymbols('DJIA', src = 'FRED')
DJIA    <- DJIA[!is.na(DJIA)]
returns <- diff(DJIA)/DJIA
t.test(x = as.vector(returns), mu = 0)
```
\endxmpl


## Signifikanztests
\framesubtitle{Einseitiger Test}

- Zusammengesetzte Nullhypothesen der Form $\mu\leq\mu_0$ oder $\mu\geq\mu_0$ lassen den für die Verteilung der Teststatistik unter $H_0$ benötigten Parameter $\mu$ unbestimmt.
- In beiden Fällen setzt man $\mu=\mu_0$, um den kritischen Bereich festzulegen.
- Zusammengesetzte Nullhypothesen führen zu \hil{einseitigen Tests}.
  - Für $H_0:\mu\leq\mu_0$ ist die Alternativhypothese $H_1:\mu>\mu_0$ und es wird \hil{rechtsseitig} getestet.
  - Mit $H_0:\mu\geq\mu_0$ korrespondiert die Alternativhypothese $H_1:\mu<\mu_0$, und ein \hil{linksseitiger} Test.


## Signifikanztests
\framesubtitle{Einseitiger Test}
\vspace{1cm}


```{r, echo=F}
par(xaxs="i",yaxs="i", bty="l")

library(shape)
myArrows <- function(...) {
  Arrows(..., arr.adj = 1, arr.length = 0.175, arr.width = 0.15, arr.type = "triangle")
}


grid <- seq(-3.5, 3.5, .03)
plot(x = grid, y = dnorm(grid), type = "l", ylim = c(0, .4), xaxt = "n", yaxt = "n", xlab = "", ylab = "", col = "tomato3", main="Rechtsseitiger Test")
# lines(x = grid + 4, y = dnorm(grid, 0, 1.6), col = "red2")
polygon(x = c(qnorm(0.9), seq(qnorm(0.9), 5, .01), 5), y = c(0, dnorm(seq(qnorm(0.9), 5, 0.01)), 0),
        col = alpha("tomato3", 0.5), border = NA)
# polygon(x = c(-1, seq(-1, 1.5, .01), 1.5), y = c(0, dnorm(seq(-5, -2.5, 0.01), 0 , 1.6), 0),
#        col = alpha("red3", 0.5), border = NA)
segments(x0 = 0, y0 = 0, y1 = dnorm(0), lty = 3)
abline(v = qnorm(0.9), lty = 5)

myArrows(x0 = -5,  x1 = qnorm(0.9), y0 = 0.3, y1 = 0.3, code = 2)
myArrows(x0 = qnorm(0.9), x1 = 8,   y0 = 0.3, y1 = 0.3, code = 1)
text(x = c(-2, 5), y = 0.35, labels = c('Annahmebereich\n f\\"ur $H_0: \\theta \\leq \\theta_0$', "kritischer Bereich"), cex = 1)

axis(1, at = 0, labels = "$\\theta_0$")
text(x = c(1.7, -.5), y = c(0.03, 0.08), labels = c("$\\alpha=0.1$", "$1-\\alpha$"), cex = 0.8)
text(x = c(2.3, 5), y = 0.35, labels = c('Ablehnungsbereich'), cex = 1)
```



## Signifikanztests
\framesubtitle{Einseitiger Test}

- Bei einem einseitigen Test werden die \hil{kritischen Werte} nicht mit $\alpha/2$, sondern mit $\alpha$ berechnet.
- Die hierfür relevante Gleichung lautet jetzt
\[ P(\bar{X}\leq\bar{x}_{r,2})=P(Z\leq z_{1-\alpha})=1-\alpha. \]
-  Der kritische Bereich für einen \hil{rechtsseitigen Test} ist also
\[ I_\alpha(z)=\{z_{1-\alpha}<Z\}. \]
- Bei symmetrischer Verteilung unter $H_0$ ist der kritische Bereich für einen \hil{linksseitigen Test} dann einfach
\[ I_\alpha(z)=\{Z<z_{\alpha}\} \]


## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}
\exe
Von einer Brotfirma werden Brötchen produziert. Die Firma verspricht, dass das durchschnittliche Gewicht der Brötchen 50 Gramm beträgt. Das Brötchengewicht $X$ sei normalverteilt mit einer Varianz von 1.44 g$^2$.\mps
Bei einer Stichprobe von 25 Brötchen ist ein mittleres Gewicht von $\bar{x}=49.5$g festgestellt worden.\mps
Stützt der Stichprobenmittelwert $\bar{x}$ bei einem Signifikanzniveau von 5% die Angaben der Brotfirma?
\endexe


## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}
\exe
Bei einem Test einer neu angeschafften Flaschenabfüllanlage wird der Brauerei eine mittlere Abfüllmenge von 0.5 l bei einer Standardabweichung von $\sigma=0.03$ l garantiert. Eine Stichprobe von $n=40$ Flaschen aus einer 10 minütigen Produktionsgesamtheit von 500 Flaschen ergab eine durchschnittliche Biermenge pro Flasche von 0.49 l.

(a) Kann man mit einer Irrtumswahrscheinlichkeit von 5% davon ausgehen, dass die mittlere Abfüllmenge geringer ist als vom Hersteller der Anlage angegeben?
(b) Bei welchem $\alpha$-Niveau wird die unter \blue{(a)} betrachtete Hypothese noch abgelehnt?

\endexe



## Signifikanztests
\framesubtitle{$\alpha$- und $\beta$-Fehler}

- Da beim statistischen Testen Entscheidungen nicht unter Sicherheit getroffen werden, besteht immer die Möglichkeit einer Fehlentscheidung.
- Der \hil{{\boldmath{$\alpha$}}-{Fehler}}, auch \hil{Fehler erster Art} genannt, entsteht, wenn $H_0$ abgelehnt wird, obwohl sie richtig ist.
- Dies ist mit der Wahrscheinlichkeit $\alpha$ der Fall, daher der Name.
- Ist $H_0$ falsch und fiele die Testgröße in den Annahmebereich, wird $H_0$ nicht verworfen.
- Diesen Fehler nennt man \hil{{\boldmath{$\beta$}}-{Fehler}} oder \hil{Fehler zweiter Art}.
- Die Wahrscheinlichkeit für den $\beta$-Fehler entspricht (bei einem rechtsseitigen Test) der blauen Fläche in der Abbildung auf der nächsten Folie.


## Signifikanztests
\framesubtitle{$\alpha$- und $\beta$-Fehler}
\label{alphabetaerr}

\vspace{0.2cm}

```{r, echo=F}
library(shape)
par(xaxs="i",yaxs="i", bty="l")

grid <- seq(-3.5, 8, .03)
plot(x = grid, y = dnorm(grid), type = "l", ylim = c(0, .4), xaxt = "n", yaxt = "n", xlab = "", ylab = "", col = "tomato3")
lines(x = grid, y = dnorm(grid, mean = 3), col = "dodgerblue3")
polygon(x = c(qnorm(0.9), seq(qnorm(0.9), 5, .01), 5), y = c(0, dnorm(seq(qnorm(0.9), 5, 0.01)), 0),
        col = alpha("tomato3", 0.5), border = NA)
polygon(x = c(-5, seq(-5, qnorm(0.9), .01), qnorm(0.9)), y = c(0, dnorm(seq(-5, qnorm(0.9), .01), mean = 3), 0), col = alpha("dodgerblue3", 0.5), border = NA)

segments(x0 = c(0, 3), y0 = 0, y1 = dnorm(0), lty = 3)
abline(v = qnorm(0.9), lty = 5)

myArrows(x0 = -5,  x1 = qnorm(0.9), y0 = 0.3, y1 = 0.3, code = 2)
myArrows(x0 = qnorm(0.9), x1 = 8,   y0 = 0.3, y1 = 0.3, code = 1)
text(x = c(-2, 5), y = 0.35, labels = c('Annahmebereich\n f\\"ur $H_0: \\theta \\leq \\theta_0$', "kritischer Bereich"))

axis(1, at = c(0, 3), labels = c("$\\theta_0$", "$\\theta_1$"))
text(x = c(1,1.8, -.5), y = c(0.02, 0.02, 0.08), labels = c("$\\beta$", "$\\alpha=0.1$", "$1-\\alpha$"), cex=0.8)
```



## Signifikanztests
\framesubtitle{$\alpha$- und $\beta$-Fehler}

\xmpl[Nullhypothese: nicht schwanger]
\begin{center}
\includegraphics[width=7cm,keepaspectratio=true]{Resources/Grafiken/errors.png}
\end{center}
\endxmpl



## Signifikanztests
\framesubtitle{Entscheidungssituationen und Fehlerarten}

<!-- \hil{Entscheidungssituationen und Fehlerarten} -->
\begin{table}
\begin{tabular}{|l|c|c|}\hline
     \diagbox{Entscheidung}{Realität} & $H_0$ richtig & $H_1$ richtig  \\ \hline
     $H_0$ nicht verwerfen & richtig & falsch \\
      & $1-\alpha$ &  $\beta$-Fehler \\  \hline
     $H_0$ ablehnen & falsch & richtig\\
      & $\alpha$-Fehler & $1-\beta$ \\ \hline
\end{tabular}
\end{table}

<!-- \begin{tabular}{|l|c|c|}\hline -->
<!--      \multicolumn{1}{|r|}{Reali-} & & \\ -->
<!--      Ent- \hspace*{1cm} tät & $H_0$ & $H_1$ \\ -->
<!--      scheidung & richtig & richtig \\ \hline -->
<!--      $H_0$ & richtig & falsch \\ -->
<!--      nicht verwerfen & $1-\alpha$ & $\beta$-Fehler  \\ \hline -->
<!--      $H_0$ & falsch & richtig \\ -->
<!--      ablehnen & $\alpha$-Fehler & $1-\beta$ \\ \hline -->
<!--     \end{tabular} -->
<!-- \end{center} -->






## Signifikanztests
\framesubtitle{$\alpha$- und $\beta$-Fehler}
\xmpl[Anlagestrategie]
Es kann also sein, dass ihr Test sagt, dass Ihre Strategie Sie reich macht, obwohl das gar nicht stimmt, weil das unwahrscheinliche, aber nicht unmögliche Ereignis eingetreten ist, dass Ihre Anlagestrategie an den $n$ Testtagen zufällig sehr gut klappte, obwohl sie langfristig nichts bringt.\mps
Umgekehrt kann es sein, dass Sie tatsächlich den Schlüssel zum Geld gefunden haben, das aber in den $n$ Testtagen noch nicht deutlich wird.\mps
Oder der Test liegt richtig: Ihre $n$ Testtage machen klar, dass die Idee doch nicht besonders toll war, oder aber bestätigen, dass sie reich werden.
\endxmpl



## Signifikanztests
\framesubtitle{$\alpha$- und $\beta$-Fehler}

- Ein \glqq{}guter\grqq{} Test sollte also kleine $\alpha$- und $\beta$-Fehler haben.
- Die Abbildung auf Folie \ref{alphabetaerr} zeigt, dass eine Verringerung des $\alpha$-Fehlers (Verschiebung des kritischen Wertes nach rechts) zu einer Erhöhung des $\beta$-Fehlers führt.
- Die Wahrscheinlichkeit eine falsche Nullhypothese abzulehnen ist $1-\beta$ und wird auch als \hil{Macht} bzw. \hil{Power} des Tests bezeichnet.



## Signifikanztests
\framesubtitle{$\alpha$- und $\beta$-Fehler}
\xmpl[Parametertest bei normalverteilter Grundgesamtheit]
Um bei obigem Test den $\beta$-Fehler zu quantifizieren, müssen wir uns aus der Alternativhypothese $H_1:\mu\neq\mu_0$ einen Punkt heraussuchen. Hier bietet sich bspw.\ die Punktschätzung an. Dann ist $\mu_1=42$.\mps
Es kann aber jeder Wert aus $H_1$ eingesetzt werden. Es ist wichtig zu verstehen, dass wir den wahren Wert $\mu$ nicht kennen und damit zwar verschiedene $\beta$-Fehler berechnen können, nicht jedoch wissen, welcher der \glqq{}richtige\grqq{} ist.
\endxmpl



## Signifikanztests
\framesubtitle{$\alpha$- und $\beta$-Fehler}
\xmpl[*]
Damit ist der $\beta$-Fehler quantifizierbar, der in der Abbildung auf der nächsten Folie als rote Fläche ausgewiesen ist. Er beträgt
\begin{eqnarray*}
 & & P(\mu_0 - 1.96\sigma \leq \bar{X} \leq \mu_0 + 1.96 \sigma|H_1=42) \\
& &  = P\left(\frac{38.824-42}{0.6}\leq Z\leq \frac{41.176-42}{0.6}\Bigl|H_1=42\right) \\
& &  = P(-5.293\leq Z\leq -1.373|H_1=42) \\
& & = F(5.293)-F(1.373)\approx1-0.9152=0.0848.
 \end{eqnarray*}
Der $\beta$-Fehler von 0.0848 gibt die Wahrscheinlichkeit an, dass die falsche Nullhypothese nicht verworfen wird, wenn das wahre $\mu$ tatsächlich gleich 42 ist.
\endxmpl



## Signifikanztests
\framesubtitle{$\alpha$- und $\beta$-Fehler}


```{r, echo=F}
grid <- seq(-5, 5, .03)
plot(x = grid, y = dnorm(grid, 0, 1.6), type = "l", ylim = c(0, .4), xaxt = "n", yaxt = "n", xlab = "", ylab = "", xlim = c(-5, 8), col = "dodgerblue3", main="$\\alpha$- und $\\beta$-Fehler beim zweiseitigen Test")
lines(x = grid + 4, y = dnorm(grid, 0, 1.6), col = "tomato3")
polygon(x = c(1.7, seq(1.7, 5, .01), 5), y = c(0, dnorm(seq(1.7, 5, 0.01), 0 , 1.6), 0),
        col = alpha("dodgerblue3", 0.66), border = NA)
polygon(x = c(-5, seq(-5, -1.7, .01), -1.7), y = c(0, dnorm(seq(-5, -1.7, 0.01), 0 , 1.6), 0),
        col = alpha("dodgerblue3", 0.66), border = NA)
polygon(x = c(-1, seq(-1, 1.7, .01), 1.7), y = c(0, dnorm(seq(-5, -2.3, 0.01), 0 , 1.6), 0),
        col = alpha("tomato3", 0.66), border = NA)

segments(x0 = c(0, 4), y0 = 0, y1 = dnorm(0, 0, 1.6), lty = 3)
axis(1, at = c(-1.7, 0, 1.7, 4, 8), labels = c("$\\mu_0-1.96\\sigma$", "$\\mu_0 = 40$", "$\\mu_0 + 1.96\\sigma$","$42$", "$\\bar{X}$"))
# text(text = c("$41,2$", "$\\bar{X}$"), side = 1, line = 0.6, x = c(1.7, 8), cex = 0.9)
text(x = -4, y = 0.1, labels = "$\\frac{\\alpha}{2}$")
text(x = 1.2, y = 0.03, labels = "$\\beta$")
myArrows(x0 = -3.8, x1 = -2.2, y0 = 0.08, y1 = 0.05)
abline(v = 1.7, lty = 5)
```




## Signifikanztests
\framesubtitle{Durchführungsschema für Signifikanztests}

- Für die Durchführung von Signifikanztests kann ein Standardschema angegeben werden, das vier Testschritte umfasst:

(1) Spezifikation der Null- und Alternativhypothese,
(2) Festlegung des Signifikanzniveaus,
(3) Auswahl der Teststatistik,
(4) Berechnung des Testwertes und Entscheidung.

- Die beiden ersten Schritte ergeben sich aus der inhaltlichen Problemstellung.

## Signifikanztests
\framesubtitle{QuizAcademy}
\QuizAcademy{Signifikanztests 2}\endQuizAcademy

## Signifikanztests
\framesubtitle{$\sigma^2$ unbekannt}

- Ist die Varianz von $X$ unbekannt, muss sie durch $S^2$ geschätzt werden.
- Bei normalverteiltem $X$ ist $Z(t)$ $t$-verteilt (vgl. \eqref{tVert}),
\[ Z(t)=\frac{\bar{X}-\mu}{S/\sqrt{n}}. \]
- D.h., bei geschätzter Varianz liefern die Quantile der $t$-Verteilung die kritischen Werte.
- Bei hinreichend großem $n$ können \hil{auch ohne Kenntnis der Verteilung} von $X$ Signifikanztests durchgeführt werden.
- In diesem Fall ist die Teststatistik wegen des zentralen Grenzwertsatzes approximativ normalverteilt.
- In der Praxis nimmt man oft $n\geq30$, auch wenn das streng genommen nur den Übergang von der $t$- in die $N(0,1)$-Verteilung beschreibt.


## Signifikanztests
\framesubtitle{$\sigma^2$ unbekannt}
\xmpl[Parametertest bei unbekannter Varianz]
In einer Grundgesamtheit ist $X$ normalverteilt, $\sigma^2$ unbekannt. Man vermutet, dass $\E(X)$ höchstens 20 ist. Mit einem rechtsseitigen Test soll $H_0:\mu\leq20$ bei einem Signifikanzniveau $\alpha=0.01$ überprüft werden.\mps
Eine Stichprobe mit $n=25$ liefert $\bar{x}=21.5$ und $s^2=9$. Bei $n-1=24$ Freiheitsgraden und $\alpha=0.01$ beträgt der kritische Wert $t(0.99,24)=2.4922$, welcher sich mit dem R-Befehl `qt(p = 0.99, df = 24)` errechnen lässt.
\endxmpl


## Signifikanztests
\framesubtitle{$\sigma^2$ unbekannt}
\xmpl[*]
Der Ablehnungsbereich ist \[ I_\alpha=\{Z(t)\geq 2.4922\}. \]
Die standardisierte Testgröße
\[\frac{\bar{x}-\mu_0}{s/\sqrt{n}}=\frac{21.5-20}{3/5}=2.5\]
liegt in $I_\alpha$. $H_0$ wird also abgelehnt.
\endxmpl


## Signifikanztests
\framesubtitle{$\sigma^2$ unbekannt}

```{r}
library(MASS)
x <- mvrnorm(n = 25, empirical = TRUE, mu = 21.5, Sigma = 9)
t.test(x, mu = 20, alternative = "greater", conf.level = 0.99)
```




## Signifikanztests
\framesubtitle{Parametertest für den Erwartungswert}

\exe
Auf Verpackungen eines Batterietyps wird eine durchschnittliche Lebensdauer von 280 Stunden angegeben.
Eine Qualitätskontrolle ($n=36$) führt zu folgenden Ergebnissen:
\begin{center}
\begin{tabular}{cccccc}
269 & 300 & 268 & 278 & 282 & 263\\
301 & 295 & 288 & 278 & 276 & 286\\
296 & 265 & 271 & 279 & 284 & 260\\
275 & 282 & 260 & 266 & 270 & 293\\
272 & 285 & 293 & 281 & 269 & 299\\
263 & 264 & 273 & 291 & 274 & 277
\end{tabular}
\end{center}

(a) Muss der Hersteller die Angaben korrigieren (Signifikanzniveau 1%)?
(b) Wie groß ist die Wahrscheinlichkeit, dass die Nullhypothese nicht verworfen wird, wenn sie falsch ist? (Arbeiten Sie mit $\bar{x}$.)

\endexe



## Signifikanztests
\framesubtitle{Parametertest für den Anteilswert}
Siehe Buch S.\ 249.



<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%% Kapitel 4 -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- % Kürzungen gegenüber kompletter Fassung SoSe 2014: -->
<!-- % -->
<!-- % - Eig F(x,y) -->
<!-- % - Var(X+Y) vereinfacht -->

# Zweidimensionale Zufallsvariablen


## Zweidimensionale Zufallsvariable
\framesubtitle{Mehrere interessierende Merkmale}

- Bis jetzt haben wir nur ein Merkmal betrachtet, das in eine eindimensionale Zufallsvariable überführt wurde.
- Besitzen die Ausgänge $\omega_j$ aber $K\geq 2$ interessierende Merkmale, kann für jedes eine eigene Zufallsvariable $X_1,\ldots,X_K$ eingeführt werden.
- Wir betrachten im Folgenden zwei interessierende Merkmale.
- Jedes der beiden Merkmale stellt eine Zufallsvariable dar.
- Zusammen ergeben sie die zweidimensionale Zufallsvariable \[[X(\omega),Y(\omega)],\] die jeden Ausgang $\omega$ in genau ein geordnetes Zahlenpaar abbildet.




## Zweidimensionale Zufallsvariable
\framesubtitle{Mehrere interessierende Merkmale}
\defn[Zweidimensionale Zufallsvariable]
Die Abbildung $(X,Y)$, die jeden Ausgang $\omega\in\Omega$ in ein geordnetes Zahlenpaar $(x,y)\in\mathbb{R}^2$ abbildet, also
\[ (X,Y):\quad \Omega\longrightarrow\mathbb{R}^2 \]
heißt \hil{zweidimensionale Zufallsvariable}, wenn
\[ \{\omega\in\Omega|X(\omega)\leq x \quad \text{und} \quad Y(\omega)\leq y\}\]
ein Ereignis für alle $(x,y)\in\mathbb{R}^2$ darstellt.
\enddefn



## Zweidimensionale Zufallsvariable

\xmpl[Haushaltsmerkmale]
Aus dem Einwohnermelderegister einer Stadt wird ein Haushalt zufällig ausgewählt. Der ausgewählte Haushalt ist Ausgang eines Zufallsvorgangs.\mps
Die beiden interessierenden Merkmale sind die Haushaltsgröße (Anzahl der Haushaltsmitglieder, $X$) und das Haushaltsjahreseinkommen, $Y$.\mps Die zweidimensionale Zufallsvariable $(X,Y)$ bildet jeden Ausgang $\omega$ (=ausgewählter Haushalt) in ein Zahlenpaar ab,
\[ [x=X(\omega),y=Y(\omega)].\]
\endxmpl



## Zweidimensionale Zufallsvariable
\framesubtitle{Verteilungsfunktion}

\xmpl[Haushaltsmerkmale]
Für die Wahrscheinlichkeit, dass ein zufällig ausgewählter Haushalt aus höchstens drei Mitgliedern besteht und sein Jahreseinkommen weniger als 50,000 € beträgt, benötigen wir die Verteilungsfunktion einer zweidimensionalen Zufallsvariablen.
\endxmpl

\defn[Zweidimensionale Verteilungsfunktion]
Eine Funktion $F:\; \mathbb{R}^2\longrightarrow\mathbb{R}$ mit
\[F(x,y)=P(X\leq x, Y\leq y)\]
heißt \hil{zweidimensionale Verteilungsfunktion}.
\enddefn

Für diese gelten ähnliche Eigenschaften wie für den eindimensionalen Fall, siehe Buch S.\ 152.


## Zweidimensionale Zufallsvariable
\framesubtitle{Verteilungsfunktion}

- Auch bei zweidimensionalen ZV unterscheiden wir, ob $X$ und $Y$ beide diskret, beide stetig oder ob sie  gemischt stetig-diskret sind.
- Wir beschränken uns im Folgenden auf stetige ZV. Die Ausführungen übertragen sich recht analog auf diskrete ZV.
- Diese sind formal wiederum sehr ähnlich zu gemeinsamen relativen Häufigkeiten aus der deskriptiven Statistik.


## Zweidimensionale Zufallsvariable
\framesubtitle{stetige zweidimensionale Zufallsvariable}
Bei \hil{stetigen}, zweidimensionalen Zufallsvariablen besteht der Wertebereich der $(X,Y)$ aus überabzählbar unendlich vielen Zahlenpaaren $(x,y)\in \mathbb{R}^2$.

\defn[gemeinsame Dichtefunktion]
Die Funktion $f:$ $(X,Y)\longrightarrow \mathbb{R}^+$ heißt \hil{gemeinsame Dichte(funktion)}, wenn für alle $(x,y)\in \mathbb{R}^2$ gilt
\[ F(x,y) = \int\limits_{-\infty}^y\int\limits_{-\infty}^x f(u,v) \, \text{d}u\, \text{d}v.\]
\enddefn
Für alle Punkte $(x,y)$, in denen $F(x,y)$ stetig ist, gilt
\[ \frac{\partial^2 F}{\partial x\partial y}=f(x,y).\]



## Zweidimensionale Zufallsvariable
\framesubtitle{Dichtefunktion/Verteilungsfunktion}
\xmpl[]

\begin{equation*}
 f(x,y)=
 \begin{cases}
  xy                   & 0<x<2 \text{ und } 0<y<1 \\
  0                   & \text{sonst}.
 \end{cases}
\end{equation*}
Über diesem Bereich beträgt das Volumen unter der Dichtefunktion eins
\enlargethispage{1cm}
\[ \int\limits_0^1\int\limits_0^2 xy\, \text{d}x\,\text{d}y = \int\limits_0^1\frac{y}{2}x^2\Bigg|_0^2 \,\text{d}y = \int\limits_0^1 2y \, \text{d}y = y^2\Bigg|_0^1 =1.\]
\endxmpl


## Zweidimensionale Zufallsvariable
\framesubtitle{Dichtefunktion/Verteilungsfunktion}


\xmpl[*]
$F(x,y)$ erhält man aus $f(x,y)$ durch Integration, wobei $u$ die Integrationsvariable für $x$ und $v$ die Integrationsvariable für $y$ ist,
\[ F(x,y)=\int\limits_0^y\int\limits_0^x uv \, \text{d}u\, \text{d}v = \int\limits_0^y \frac{v}{2}u^2\Bigg|_0^x\, \text{d}v= \int\limits_0^y \frac{v}{2}x^2 \, \text{d}v = \frac{1}{4}x^2v^2 \Bigg|^y_0 = \frac{1}{4} x^2y^2.\]
In ausführlicher Notation
\begin{equation*}
  F(x,y)=
\begin{cases}
  0                   & x\leq 0 \text{ und/oder } y\leq 0 \\
  \frac{1}{4}x^2y^2  & 0 < x < 2 \text{ und } 0<y<1 \\
  1                   & x\geq 2 \text{ und } y\geq 1.
 \end{cases}
\end{equation*}
\endxmpl


## Zweidimensionale Zufallsvariable
\framesubtitle{Dichtefunktion/Verteilungsfunktion}
\xmpl[*]
Die Kreuzableitung der Verteilungsfunktion $F(x,y)$ liefert $f(x,y)$
\[ \frac{\partial^2 F(x,y)}{\partial y \partial x} = \frac{\partial}{\partial y} \left(\frac{1}{2}xy^2\right) = xy.\]
Zur Berechnung von $P(0<X<1,\: 0<Y<1)$ verwendet man $F(x,y)$.
\[ P(0<X<1,\: 0<Y<1)=F(1,1)=\frac{1}{4}.\]
\endxmpl


## Zweidimensionale Zufallsvariable
\framesubtitle{Dichtefunktion/Verteilungsfunktion}

```{r, echo=F}
library(lattice)
library(gridExtra)
myPdf <- function(x, y) {
  ifelse(x > 0 & x <= 2 & y > 0 & y <= 1, x * y, 0)
}
myCdf <- function(x, y) {
  ifelse(x > 0 & x <= 2 & y > 0 & y <= 1, 0.25 * x^2 * y^2, 0)
}

mySettings <- list(axis.line = list(col = 0),
                   fontsize = list(text = 7),
                   clip = list(panel = "off", strip = "on"),
                   layout.heights = list(top.padding = -25,
                                         main.key.padding = 0,
                                         key.axis.padding = 0,
                                         axis.xlab.padding = 0,
                                         xlab.key.padding = 0,
                                         key.sub.padding = 0,
                                         bottom.padding = 0))

df <- expand.grid(x = seq(0, 2, .1), y = seq(0, 1, .1))
df$z <- myPdf(df$x, df$y)
p1 <- wireframe(z ~ x * y, data = df,
          scales = list(arrows = FALSE),
          drape = T, colorkey = F,
          screen = list(z = 25, x = -60),
          par.settings = mySettings,
          zlab = '$f(x, y)\\qquad$')

df$z2 <- myCdf(df$x, df$y)
p2 <- wireframe(z2 ~ x * y, data = df,
          scales = list(arrows = FALSE),
          drape = T, colorkey = F,
          screen = list(z = 25, x = -60),
          par.settings = mySettings,
          zlab = '$F(x,y)\\qquad$')
grid.arrange(p1, p2, ncol=2, clip=T)
```



## Zweidimensionale Zufallsvariable
\framesubtitle{Randdichte}
\defn[Randdichte einer stetigen Zufallsvariable]
Die Funktion
\[ f_X(x)\longrightarrow \mathbb{R}\quad\text{bzw.} \quad  f_Y(y)\longrightarrow \mathbb{R} \]
mit
\[ f_X(x) = \int\limits_{-\infty}^\infty f(x,y) \, \text{d}y \quad \text{bzw.}\quad f_Y(y)=\int\limits_{-\infty}^\infty f(x,y)\, \text{d}x \]
heißt \hil{Randdichtefunktion} von $X$ bzw. $Y$.
\enddefn



## Zweidimensionale Zufallsvariable
\framesubtitle{Randdichte}

- Im letzten \blue{Beispiel} erhält man die \hil{Randdichte} für $X$ als
\[ f_X(x)=\int\limits_0^1 xy \, \text{d}y = \frac{x}{2}y^2\bigg|_0^1=\frac{1}{2}x.\]
- Hierfür gilt, wie üblich,
\[ \int\limits _0^2 f_X(x)\, \text{d}x =\int\limits_0^2\frac{1}{2}x \, \text{d}x =1.\]
- Die \hil{Randverteilungsfunktion} ergibt sich, auch wie üblich, aus $f_X(x)$ als
\[ F_X(x)= \int\limits_0^x\frac{1}{2}u\, \text{d}u= \frac{1}{4}u^2\bigg|^x_0=\frac{1}{4}x^2.\]


## Zweidimensionale Zufallsvariable
\framesubtitle{Stochastische Unabhängigkeit}
\defn[Stochastische Unabhängigkeit bei stetigen, zweidimensionalen Zufallsvariablen]
Die stetigen Zufallsvariablen $X$ und $Y$ heißen \hil{stochastisch unabhängig}, wenn für alle Paare $(x,y)\in\mathbb{R}^2$ gilt
\[ f(x,y) = f_X(x)f_Y(y) \quad \text{bzw.} \quad F(x,y)=F_X(x)F_Y(y).\]
\enddefn


## Zweidimensionale Zufallsvariable
\framesubtitle{Stochastische Unabhängigkeit}
\xmpl[]
Die Randdichtefunktion für $Y$ ist
\[ f_Y(y) = \int\limits_0^2 xy \, \text{d}x = \frac{y}{2}x^2\bigg|_0^2 =2y.\]
Also ist $f(x,y)=xy=2y\frac{x}{2}=f_Y(y)f_X(x)$.
\endxmpl


## Zweidimensionale Zufallsvariable
\framesubtitle{Erwartungswert und Varianz für die Randverteilungen}
Für $X$ gilt exemplarisch
\begin{eqnarray*}
  \E(X)&=&\int\limits_x xf_X(x)\, \text{d}x = \int\limits_x x\left(\int\limits_y f(x,y)\, \text{d}y\right)\,\text{d}x \\
  &=& \int\limits_x\int\limits_y xf(x,y)\, \text{d}y\, \text{d}x. \\
\end{eqnarray*}
sowie $\var(X)=\E(X^2)-[\E(X)]^2$ mit
\[\E(X^2) = \int\limits_x x^2 f_X(x)\, \text{d}x = \int\limits_x\int\limits_y x^2 f(x,y)\, \text{d}x\, \text{d}y \]

## Zweidimensionale Zufallsvariable
\framesubtitle{Kovarianz - QuizAcademy}
\QuizAcademy{Zweidimensionale Zufallsvariable 1}\endQuizAcademy


## Zweidimensionale Zufallsvariable

\exe
Gegeben ist folgende Funktion:
\[f(x,y)=\begin{cases}
\dfrac{x+3y}{2} & 0<x<1,\;\; 0<y<1\\[2mm]
0 & \text{sonst}.
\end{cases}\]

(a) Überprüfen Sie, ob $f(x,y)$ eine Dichtefunktion ist!
(b) Leiten Sie die gemeinsame Verteilungsfunktion $F(x,y)$ her!
(c) Bestimmen Sie die Randdichten von $X$ und $Y$!
(d) Bestimmen Sie den Erwartungswert und die Varianz von $X$ und $Y$!
(e) Sind $X$ und $Y$ stochastisch unabhängig?
(f) Bestimmen Sie für $Z=X+Y$ den Erwartungswert $\E(Z)$ und die Varianz $\var(Z)$!

\endexe


## Zweidimensionale Zufallsvariable
\framesubtitle{Kovarianz}

\defn[Kovarianz]
Das \hil{gemischte Zentralmoment}
\[ \sigma_{X,Y} = \cov(X,Y)= \E\{[X-\E(X)][Y-\E(Y)]\} = \E[(X-\mu_X)(Y-\mu_Y)] \]
heißt \hil{Kovarianz} $(\cov)$ von $X$ und $Y$.
\enddefn


- Die Berechnung der Kovarianz vereinfacht sich bei Anwendung von
\[ \cov(X,Y)=\E(XY)-\E(X)\E(Y).\]
- Diese Vereinfachung lässt sich zeigen mit
\begin{align*}
 \cov(X,Y) & =\E[XY-X\E(Y)-\E(X)Y+\E(X)\E(Y)] \\
 & =\E(XY)-\E(X)\E(Y).
\end{align*}


## Zweidimensionale Zufallsvariable
\framesubtitle{Kovarianz}


- Den Erwartungswert $\E(XY)$ ermittelt man als
\begin{eqnarray*}
 \E(XY) &=& \int\limits_y\int\limits_x xyf(x,y) \, \text{d}x\,\text{d}y.
\end{eqnarray*}
- Sind $X$ und $Y$ unabhängig, vereinfacht sich die Gleichung wegen
\[f(x,y)=f_X(x)f_Y(y).\]
- Damit können wir schreiben

\begin{eqnarray*}
\E(XY) &=& \int\limits_x \int\limits_y xy f(x,y) \, \text{d}x\, \text{d}y = \int\limits_x xf_X(x)\, \text{d}x\int\limits_y y f_Y(y) \, \text{d}y \\
  & =& \E(X)\E(Y).
\end{eqnarray*}


## Zweidimensionale Zufallsvariable
\framesubtitle{Kovarianz}

- Stochastische Unabhängigkeit impliziert also eine Kovarianz von null.
Die Umkehrung gilt \underline{nicht} allgemein!
- Eine Kovarianz von null bedeutet lediglich, dass für beide Variablen kein linearer Zusammenhang vorliegt.

\xmpl[stochastische Abhängigkeit]
Sei $X\sim N(0,1)$. Dann sind $X$ und $X^2$ \hil{unkorreliert}, da
\[\cov(X,X^2)=\E(X^3)-\E(X)\E(X^2)=0-0=0\]
Auch ohne Rechnung ist aber klar, dass $X$ und $X^2$ nicht unabhängig sind.
\endxmpl


## Zweidimensionale Zufallsvariable
\framesubtitle{Kovarianz - QuizAcademy}
\QuizAcademy{Zweidimensionale Zufallsvariable 2}\endQuizAcademy


## Zweidimensionale Zufallsvariable
\framesubtitle{Korrelationskoeffizient}

- Standardisieren von $X$ und $Y$ liefert
\[ Z_X = \frac{X-\E(X)}{\sigma_X} \quad \text{und} \quad Z_Y=\frac{Y-\E(Y)}{\sigma_Y}.\]
- Die Kovarianz der standardisierten Variablen folgt wegen
\[ \E(Z_X)=\E(Z_Y)=0 \] als
\begin{align*}
 \cov(Z_X,Z_Y) & = \E(Z_XZ_Y)=\E\left[\frac{X-\E(X)}{\sigma_X}\frac{Y-\E(Y)}{\sigma_Y}\right] \\
 \\
 & = \frac{\E\{[X-\E(X)][Y-\E(Y)]\}}{\sigma_X\sigma_Y} = \frac{\cov(X,Y)}{\sigma_X\sigma_Y} = r,
\end{align*}
bekannt als Korrelationskoeffizient.


## Zweidimensionale Zufallsvariable
\framesubtitle{Korrelationskoeffizient}

- Bei exakt linearer, positiver Abhängigkeit zwischen $X$ und $Y$ gilt $r=1$, bei exakt linearer, negativer Abhängigkeit $r=-1$.
- Eine Kovarianz von null impliziert $r=0$.\mps
Auch hier gilt: Unkorreliertheit bedeutet nicht Unabhängigkeit.


## Zweidimensionale Zufallsvariable

\xmpl[] 
Es gelte
\[f(x,y)=x+y \qquad\text{wenn }\;0<x<1\,\text{ und }\,0<y<1,\]
0 sonst. Auf $[0,1]$ sind die Randdichten dann $f_X(x)=x+0.5$ bzw.\ $f_Y(y)=y+0.5$. $X$ und $Y$ sind nicht unabhängig, da $f(x,y)\neq f_X(x)f_Y(y)$.\mps
Damit folgen $\E(X)=\E(Y)=7/12$ und $\E(X^2)=\E(Y^2)=5/12$, so dass $\var(X)=\var(Y)=11/144$.\mps
Ferner ist
\[\E(XY)=\int_0^1\int_0^1xy(x+y)\text{d}x\text{d}y=1/3,\]
so dass $\cov(X,Y)=-1/144$ und
\[r=\frac{-\frac{1}{144}}{\sqrt{\frac{11}{144}\frac{11}{144}}}=-\frac{1}{11}\]
\endxmpl


## Zweidimensionale Zufallsvariable
\framesubtitle{abhängige stetige Zufallsvariablen}
\exe
Gegeben ist folgende Funktion:
\[f(x,y)=\begin{cases}
\dfrac{x+3y}{2} & 0<x<1,\;\; 0<y<1\\[2mm]
0 & \text{sonst}.
\end{cases}\]
Beurteilen Sie die Richtung der Abhängigkeit mit der Kovarianz und die Stärke mit Hilfe des Korrelationskoeffizienten!
\endexe


## Zweidimensionale Zufallsvariable
\framesubtitle{Linearkombination bei Abhängigkeit}

- Wir behandeln jetzt Erwartungswert und Varianz für Linearkombinationen $\lambda_1X+\lambda_2Y$ abhängiger Zufallsvariablen.
- Erwartungswert
\[ \E(\lambda_1X+\lambda_2Y)=\lambda_1\E(X)+\lambda_2\E(Y)= \lambda_1 \mu_X + \lambda_2\mu_Y\]
- Die \hil{Varianz} der Summe $X+Y$ erhält man als
\begin{eqnarray*}
 \var(X+Y) & = & \E[(X+Y)-\E(X+Y)]^2 \\
 & = & \E[(X-\mu_X)+(Y-\mu_Y)]^2 \\
 & = & \E(X-\mu_X)^2+\E(Y-\mu_Y)^2 +2\E[(X-\mu_X)(Y- \mu_Y)]\\
 & = & \var(X) + \var(Y) + 2\cov(X,Y).
\end{eqnarray*}


## Zweidimensionale Zufallsvariable
\framesubtitle{Linearkombination bei Abhängigkeit}

- Bei Abhängigkeit ist in $\var(S)$ die Kovarianz ungleich null.
- Verallgemeinerung für die Linearkombination $\tilde{S}_\lambda=\lambda_1X_1 + \ldots + \lambda_nX_n$:
\begin{eqnarray*}
 \var(\tilde{S}_\lambda) & =& \sum_{j=1}^n \lambda_j^2\var(X_j) + 2 [\lambda_1\lambda_2\cov(X_1,X_2)+\ldots+\lambda_1\lambda_n\cov(X_1,X_n) \\ \notag
 & & \quad\, +\ldots+\lambda_{n-1}\lambda_n\cov(X_{n-1},X_n)] \\
 & = &\sum_{j=1}^n\lambda_j^2\var(X_j) +  \sum_{i=1}^n\sum_{\genfrac{.}{.}{0pt}{1}{j=1}{j\neq i}}^n \lambda_i\lambda_j \cov(X_i,X_j),
\end{eqnarray*}
der \hil{Additionssatz für Varianzen abhängiger Zufallsvariablen}.
- Hieraus folgt unmittelbar der Additionssatz für Varianzen unabhängiger Zufallsvariablen, weil für diese gilt
\[ \cov(X_i,X_j)=0 \quad \text{für alle} \quad i,j \quad\text{mit} \quad i\neq j.\]


## Zweidimensionale Zufallsvariable
\framesubtitle{Linearkombination bei Abhängigkeit}

\xmpl
Ein Investor legt einen Viertel seines Vermögens in einem Immobilienfonds (IF) und den Rest in einem Aktienfonds (AF) an. Für diese gilt
\begin{center}
\begin{tabular}{ll}
$\E(\text{IF})=0.28$ & $\sigma_{\text{IF}}=0.2$\\[1ex]
$\E(\text{AF})=0.12$ & $\sigma_{\text{AF}}=0.06$.
\end{tabular}
\end{center}
Bei Unabhängigkeit der beiden Anlageformen beträgt die Standardabweichung des gesamten Portfolios (PF)
\[\sigma_{\text{PF}}=\sqrt{0.25^2\cdot 0.2^2+0.75^2\cdot 0.06^2}=0.0673.\]
Bei Abhängigkeit (mit $\sigma_{\text{IF},\text{AF}}=0.8$) beträgt diese
\[\sigma_{\text{PF}}=\sqrt{0.25^2\cdot 0.2^2+0.75^2\cdot 0.06^2+2\cdot 0.25\cdot 0.75\cdot 0.8}=0.5518.\]
\endxmpl
